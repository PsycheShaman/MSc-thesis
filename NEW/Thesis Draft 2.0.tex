% Generated by GrindEQ Word-to-LaTeX 
\documentclass{article} %%% use \documentstyle for old LaTeX compilers

\usepackage[english]{babel} %%% 'french', 'german', 'spanish', 'danish', etc.
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{mathdots}
\usepackage[classicReIm]{kpfonts}
\usepackage[dvips]{graphicx} %%% use 'pdftex' instead of 'dvips' for PDF output

% You can include more LaTeX packages here 


\begin{document}

%\selectlanguage{english} %%% remove comment delimiter ('%') and select language if required


\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent \textit{THE APPLICATION OF DEEP LEARNING TECHNIQUES TOWARDS PARTICLE IDENTIFICATION AND HIGH ENERGY PHYSICS EVENT SIMULATIONS RELATING TO THE ALICE TRD AT CERN}

\noindent \includegraphics*[width=5.73in, height=5.39in, keepaspectratio=false]{image1}

\noindent \textbf{Christiaan Gerhardus Viljoen}

\noindent Department of Statistics \textbf{{\textbar}{\textbar}} Department of Physics

\noindent Faculty of Science

\noindent University of Cape Town

\noindent 

\noindent This dissertation is submitted in partial fulfilment of the Degree of Master of Science

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent \textit{}

\noindent \textit{}

\noindent \textit{}

\noindent \textit{Dedicated to my mother, Elizabeth Suzanna Bloem Viljoen, who has always inspired me to follow my higher passions, despite the myriad difficulties that life makes us face; and to search fearlessly and incessantly for the deeper truths underlying our everyday world.}

\noindent 

\noindent \eject 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent 

\noindent \includegraphics*[width=5.87in, height=3.01in, keepaspectratio=false]{image2}

\noindent \eject 

\noindent THE APPLICATION OF DEEP LEARNING TECHNIQUES TOWARDS PARTICLE IDENTIFICATION AND HIGH ENERGY PHYSICS EVENT SIMULATIONS RELATING TO THE ALICE TRD AT CERN

\noindent Abstract

\noindent This Masters Dissertation outlines the application of deep learning methods on raw data from the Transition Radiation Detector at CERN as well as on simulated data from the Monte Carlo Event Generator Geant4, in order to achieve the following goals:

\noindent 

\begin{enumerate}
\item  Classification Part I: Particle identification; distinguishing between electrons and pions
\end{enumerate}

\noindent 

\noindent To this end, various feedforward neural networks, convolutional neural networks, as well as recurrent neural networks were built using Keras with a TensorFlow back-end, resulting in an ultimate pion efficiency of ${\varepsilon }_{\pi }=$ $2.2\%$ at electron efficiency ${\varepsilon }_e=$ 90\%.

\noindent 

\noindent Raw data was extracted from the Worldwide LHC Computing grid using the ROOT data analysis framework, a C++ based platform maintained by physicists at CERN. R and Python were used interchangeably during various stages of data exploration, processing, analysis and model-building.

\noindent 

\begin{enumerate}
\item  Classification Part II: Distinguishing real data from data generated by Geant4
\end{enumerate}

\noindent 

\noindent This stage of the project focused on employing convolutional neural networks towards distinguishing real data from simulated data. Data was simulated using Geant4, a Monte Carlo toolkit which simulates the passage of particles through matter. ROOT was used to reconstruct the simulated data to deliver it in a similar format to that given by raw data after processing. A balanced accuracy score of 91.5\% (with Sensitivity = 0.8575 and Specificity = 0.9725) was achieved, using a 2D Convolutional Neural Network.

\noindent 

\begin{enumerate}
\item  Deep Generative Modeling: Prototyping Variational Autoencoders and various Generative Adversarial Networks towards data generation
\end{enumerate}

\noindent 

\noindent 

\noindent Various deep generative models were built to take as input raw TRD data and produce simulated observations which are likely under the training data distribution. While results were not as accurate as simulations from Geant4, it is nonetheless worthwhile to see how they performed on this problem.

\noindent 

\noindent Acknowledgements

\noindent 

\noindent 

\noindent Firstly, I would like to thank my father, Christiaan Gerhardus Viljoen, for all the support -- material, emotional and financial -- he has selflessly provided to me throughout my life, and particularly towards my higher education journey. You have no idea how much appreciation I have for all the sacrifices you have made for me, and all the advice you have given me.

\noindent 

\noindent Secondly, I want to thank my aunt, Professor Emma Ruttkamp-Bloem, for all the mentoring she has provided to me in navigating the world of academia, and for the inspiration that her own academic career instils in me.

\noindent 

\noindent Thirdly, I want to thank Dr Thomas Dietel for providing me with this immense opportunity to be part of the largest scientific experiment in human history, and for the rigorous scientific guidance that he has, and continues to provide to me.

\noindent 

\noindent Lastly, I would like to thank my larger family, on both my father's and mother's side, for providing the loving and stable environment that makes any place we assemble Home.

\noindent 

\noindent 

\noindent \textit{Computations were performed using facilities provided by the University of Cape Town's ICTS High Performance Computing team:~}hpc.uct.ac.za\textit{}

\noindent \textit{}

\noindent 

\noindent Travel to CERN was paid for by iThemba Labs via the SA-CERN agreement

\noindent 

\noindent 

\noindent \eject 

\noindent Table of Contents

\noindent \textbf{\textit{1 Introduction 141.1 Background 151.2 Summary of Work Done \& Major Findings 161.3 The Structure \& Organization of this Dissertation 172 High Energy Physics \& The CERN Experiment 182.1 A Brief History of Atomic Theory 192.2 The Standard Model of Particle Physics 192.2.1 Introduction 202.2.2 The Fundamental Particles 202.2.3 The Fundamental Forces 212.2.4 The Higgs Boson 222.3 Standard Model Vertices 232.4 Interactions of Particles with Matter 242.4.1 The Bethe-Bloch Curve 242.4.2 Transition Radiation 252.5 The Quark Gluon Plasma (QGP) 252.5.1 Introduction to QGP 252.5.2 QGP, the Big Bang and the Micro Bang 272.6 The CERN Experiment 282.6.1 Hardware 292.6.2 HEP Software 333 The ALICE Detector \& the Transition Radiation Detector 363.1 The ALICE Detector System 373.2 The Transition Radiation Detector 433.2.1 A Note on Geometry 433.2.2 TRD Design Synopsis 443.2.3 TRD Measurement Mechanism 453.2.4 Particle Identification in the TRD 454 Deep Learning 524.1 Deep Learning within the Context of Artificial Intelligence and Machine Learning 534.2 Mathematical Background for Deep Learning 534.2.1 Rosenblatt's Perceptron 534.2.2 Deep Feedforward Neural Networks 544.2.3 Regularization and Optimization for Deep Learning 574.3 Convolutional Neural Networks 644.3.1 The Kernel Concept and Motivation for CNNs 644.3.2 Pooling 654.3.3 The Convolution Function 664.4 Recurrent Neural Networks 684.4.1 Computational Graphs 684.4.2 Long Short-Term Memory 704.5 Generative Models 724.5.1 Background: Latent Variable Models 724.5.2 Variational Autoencoders 734.5.3 Generative Adversarial Networks 784.5.4 Variations on the GAN concept used towards Event Simulation in this Dissertation 795 Statistical Tests 815.1 Hypotheses 825.2 Significance Level and Power 835.3 Statistical Tests for Particle Selection 836 Data 856.1 LHC Runs Used 866.2 Data Structure 866.3 Graphical Overview of Data 876.3.1 Example Images of Tracklet Signals 876.3.2 Electron and Pion Counts per Run 886.3.3 Bethe Bloch Curve per Run for Electrons and Pions 896.3.4 n$\sigmaup$ Electron per Run for Electrons and Pions 907 Methods 927.1 Data Extraction 937.2 Deep Learning for Particle Identification 937.3 Deep Learning for Distinguishing Geant4 data from real data 947.4 Deep Generative Models Towards Event Simulation 968 Results 988.1 Deep Learning for Particle Identification 998.1.1 Most useful model 998.2 Distinguishing Geant Simulations from Real Data 1038.3 Deep Generative Models Towards Event Simulation 1058.3.1 Variational Autoencoders 1058.3.2 Generative Adversarial Networks 1079 Discussion and Conclusions 1149.1 Discussion 1159.1.1 Particle Identification Using Deep Learning 1159.1.2 Distinguishing Simulated from Real Data 1199.1.3 Deep Generative Models towards High Energy Physics Event Simulations 1219.2 Conclusions 1259.2.1 Particle Identification 1259.2.2 Simulations 1269.3 Outlook and Future Work 1269.3.1 Particle Identification 1269.3.2 Simulations 1269.3.3 Outlook 12610 Bibliography 127}}

\noindent \eject 

\noindent List of Tables

\noindent Table 1: The twelve fundamental fermions. 21Table 2: Confusion Matrix for Particle Identification 101Table 3: Confusion Matrix for distinguishing between Geant vs Real Data 105\textbf{\textit{\eject }}

\noindent List of Figures

\noindent Figure 1: Standard model interaction vertices \eqref{GrindEQ__2_} 23Figure 2: Bethe-Bloch curve for a pion moving at relativistic speeds through silicon medium 24Figure 3: Bethe-Bloch curve for an electron moving at relativistic speeds through a silicon medium 25Figure 4: Simplified diagram of classical states of matter and transitions between them, with the Vacuum added as a fifth element, providing the space in which matter exists \eqref{GrindEQ__7_}, reproduced and modified from \eqref{GrindEQ__6_} 26Figure 5: Phase diagram of hadronic matter \eqref{GrindEQ__8_} 27Figure 6: The evolution of the Universe, from the Big Bang to Modern Day \eqref{GrindEQ__10_} 28Figure 7: CERN facilities in geographical context \eqref{GrindEQ__15_}. 29Figure 8: The LHC Proton Source, connected to the Duoplasmatron device, which strips electrons off Hydrogen molecules, to produce the beams of protons which eventually collide within the LHC \eqref{GrindEQ__21_} 30Figure 9: The CERN accelerator complex \eqref{GrindEQ__24_}. 31Figure 10: The ALICE detector system \eqref{GrindEQ__39_} 37Figure 11: ALICE Inner Tracking System (SPD, SDD, SSD) \eqref{GrindEQ__39_}. 38Figure 12: ALICE TPC \eqref{GrindEQ__39_}. 39Figure 13: ALICE TOF \eqref{GrindEQ__39_}. 39Figure 14: ALICE HMPID \eqref{GrindEQ__39_}. 40Figure 15: ALICE TRD \eqref{GrindEQ__39_} 40Figure 16: ALICE PHOS \eqref{GrindEQ__39_}. 41Figure 17: ALICE EMCal \eqref{GrindEQ__39_}. 42Figure 18: ALICE forward Muon arm \eqref{GrindEQ__39_}. 43Figure 19: Cylindrical coordinates as used in geometric coordinate specifications for measurements made in experiments conducted at the LHC  \eqref{GrindEQ__42_}. 44Figure 20: A schematic representation of the components in an MWPC module 45Figure 21: Time evolution of the TRD signal, measured as pulse height vs drift time for electrons and pions (both at p = 2GeV) \eqref{GrindEQ__44_}. 46Figure 22: Reference distributions for most probable signal dependence on \beta\gamma in the TRD, i.e. from measurements taken in pp-runs, test beams and measurements from cosmic rays \eqref{GrindEQ__44_}. 47Figure 23: Truncated mean signal (dE/dx + TR) for various charged particles as measured for p-Pb collisions at \boldsymbol{\mathrm{5}}.\boldsymbol{\mathrm{02}} \boldsymbol{\mathrm{TeV}}. This method allows for particle identification of light particles and hadrons \eqref{GrindEQ__44_}. 48Figure 24: Normalised distribution of charge deposition for electrons and pions in a single TRD chamber \eqref{GrindEQ__44_}. 49Figure 25: Pion efficiency as a function of electron efficiency for the various particle identification methods discussed \eqref{GrindEQ__44_}. 50Figure 26: Momentum dependence of pion efficiency for various methods (where electron efficiency is at 90\%) 51Figure 27: Illustration of the descent towards zero, of the Binary Cross Entropy Loss Function as \^{y}, or \boldsymbol{pmodel}(\boldsymbol{y}|\boldsymbol{x}), approaches the true y. 56Figure 28: \boldsymbol{L}\boldsymbol{\mathrm{1}} and \boldsymbol{L}\boldsymbol{\mathrm{2}} norm penalties  59Figure 29: An illustration of the concept of max pooling, using pool-width of 3 with a stride of one (top panel) vs a stride of two (bottom panel) \eqref{GrindEQ__48_}. 66Figure 30: Illustration of mathematical equivalence of implementing a convolution with unit stride followed by downsampling to implementing a convolution with stride = 2. 67Figure 31: ReLU activated hidden unit in a Neural Network depicted as a computational graph 69Figure 32: Acyclic computational graph of a dynamical system 69Figure 33: Graph-based representation of special LSTM units 70Figure 34: Training-time VAE 76Figure 35: Training-time VAE with reparameterization trick to enable backpropagation 76Figure 36: Testing time VAE 76Figure 37: Gan Densities during training, close to convergence, P(x) is shown in black, G(z) in blue and D(G(z)) in red 78Figure 38: Gan Densities during training, once the Algorithm has converged, G(z) matches P(x) perfectly and D(G(z)) outputs 0.5 everywhere 79Figure 39: An illustration of rejection or acceptance of the null hypothesis, under the assumed distributions of \boldsymbol{H}\boldsymbol{0} and \boldsymbol{H}\boldsymbol{1}, when t falls in the critical region \boldsymbol{t}>\boldsymbol{tcut} 82Figure 40: Eta distributions for real and Geant simulated data 95Figure 41: nsigma pion estimate (TPC) distributions for real and Geant simulated data, before cut 95Figure 42: nsigma pion distributions after applying cut 95Figure 43: Momentum distributions for both real and Geant simulated data, before cut 96Figure 44: Momentum distributions after cut 96Figure 45: Particle Identification Model Architecture 99Figure 46: Training vs Validation Accuracy 100Figure 47: Training vs Validation Loss 100Figure 48: t-statistic selection allowing for 90\% electron efficiency 101Figure 49: Combined output probabilities for true electrons (1, blue) and true pions (0, red) 101Figure 50: Weights of first convolutional layer 102Figure 51: Weights of second convolutional layer 102Figure 52: Weights of third convolutional layer 103Figure 53: Weights of fourth convolutional layer 103Figure 54: Training loss and accuracy curves for training and validation data 104Figure 55: Model architecture for distinguishing real from Geant simulated data 105Figure 56: Encoder 106Figure 57: Decoder 106Figure 58: Four examples of simulated data created using a Variational Autoencoder 107Figure 59: Training accuracy and loss curves for training vs validation data 107Figure 60: Example of a 2D-Convolutional Network training to high validation accuracy 115Figure 61: Example of an LSTM Network training to high validation accuracy 115Figure 62: Example of a 1D-Convolutional Neural Network training to high validation accuracy 116Figure 63: Running a successful model for twice the number of epochs results in minimal gains and eventually, results in overfitting 116Figure 64: No Dropout vs Same Model with too much Dropout 118Figure 65: Gaussian Noise with $\sigmaup$=0.2 118Figure 66: Distribution of ADC values for simulated data 119Figure 67: Distribution of ADC values for real data 119Figure 68: Distribution of the number of pads with no data per image for simulated data 120Figure 69: Distribution of the number of pads with no data per image for real data 120Figure 70: Distribution of mean ADC value per image for simulated data 120Figure 71: Distribution of mean ADC value per image for real data 121Figure 72: Real images 121Figure 73: Autoencoder outputs 122Figure 74: An illustration of how the Generative network of a GAN can begin producing images with similar features that, even though they are not correct, result in low loss, because of a Discriminator which is not conducive to Generator training. 122Figure 75: Illustrating the effect of Batch normalization to prevent output images from looking highly similar 123Figure 76: Illustrating how convolutional architectures in a GAN setup results in features that might exist in the training distribution, but do not appear in the right place 124Figure 77: Least squares GAN after 94600 epochs 124Figure 78: Least squares GAN after 387600 epochs 125Figure 79: Least squares GAN after 449000 epochs 125

\noindent 

\noindent \eject 

\noindent List of Abbreviations and Acronyms

\begin{tabular}{|p{2.0in}|p{2.0in}|} \hline 
\textbf{\newline } & \textbf{} \\ \hline 
ALICE & A Large Ion Collider Experiment \\ \hline 
TRD & Transition Radiation Detector \\ \hline 
CERN & European Organization for Nuclear Research \\ \hline 
QGP & Quark Gluon Plasma \\ \hline 
LHC & Large Hadron Collider \\ \hline 
WLCG & Worldwide LHC Computing Grid \\ \hline 
QCD & Quantum Chromodynamics \\ \hline 
ML & Machine Learning \\ \hline 
PbPb & Lead-Lead Collisions \\ \hline 
$e^-$ & Electron \\ \hline 
$\pi $ & Pion \\ \hline 
QED & Quantum Electrodynamics \\ \hline 
p & Proton \\ \hline 
n & Neutron \\ \hline 
$v_e$ & Electron Neutrino \\ \hline 
$v_{\mu }$ & Muon Neutrino \\ \hline 
$v_{\tau }$ & Tau Neutrino \\ \hline 
LSTM & Long Short-Term Memory \\ \hline 
VAEs & Variational Autoencoders \\ \hline 
GANs & Generative Adversarial Networks \\ \hline 
n$\sigmaup$-electron & The TPC's estimate for how many standard deviations away from the expected signal for an electron a particle is \\ \hline 
eV & Electron Volt \\ \hline 
u & Up quark \\ \hline 
d & Down quark \\ \hline 
s & Strange quark \\ \hline 
c & Charm quark \\ \hline 
b & Bottom quark \\ \hline 
t & Top Quark \\ \hline 
${\mu }^-$ & Muon \\ \hline 
${\tau }^-$ & Tau lepton \\ \hline 
EWT & Electroweak Theory \\ \hline 
c & The speed of light \\ \hline 
$<O>$ & Vacuum expectation value \\ \hline 
QFT & Quantum Field Theory \\ \hline 
g & Coupling strength of standard model interaction vertices \\ \hline 
$\tau $ & Characteristic mean lifetime of subatomic particles \\ \hline 
HEP & High Energy Physics \\ \hline 
fm & Femtometer \\ \hline 
s & Seconds \\ \hline 
T & Temperature \\ \hline 
UNESCO &  United Nations Educational, Scientific and Cultural Organization \\ \hline 
TB & Terabytes \\ \hline 
RAM & Random Access Memory \\ \hline 
GiB/s & Gigabytes per second \\ \hline 
RHIC & Relativistic Heavy Ion Collider \\ \hline 
Z & Atomic number \\ \hline 
$H_2$ & Hydrogen \\ \hline 
$\sqrt{s}$ & Centre of Mass Energy \\ \hline 
p & Momentum \\ \hline 
pPb & Proton-Lead Collisions \\ \hline 
Pb & Lead \\ \hline 
PS & Proton Synchrotron \\ \hline 
SPS & Super Proton Synchrotron \\ \hline 
NbTi & Niobium-titanium \\ \hline 
K & Degrees Kelvin \\ \hline 
$\circ$C & Degrees Celcius \\ \hline 
T & Tesla \\ \hline 
$P_{vac}$ & Vacuum Pressure \\ \hline 
$atm$ & Atmosphere \\ \hline 
ATLAS & A Toroidal LHC Apparatus \\ \hline 
CMS & Compact Muon Solenoid \\ \hline 
LHCb & Large Hadron Collider beauty \\ \hline 
TOTEM & The TOTal cross section, Elastic scattering and diffraction dissociation Measurement at the Large Hadron Collider \\ \hline 
MoEDAL & The Monopole \& Exotics Detector at the LHC \\ \hline 
OOP & Object Oriented Programming \\ \hline 
GNU & Gnu's Not Unix \\ \hline 
OS & Operating System \\ \hline 
$T_c$ & Critical Temperature \\ \hline 
m & Meter \\ \hline 
ITS & Inner Tracking System \\ \hline 
SPD & Silicon Pixel Detectors \\ \hline 
SDD & Silicon Drift Detectors \\ \hline 
SSD & Silicon Strip Detectors \\ \hline 
dE/dx & Energy loss per unit pathlength \\ \hline 
$P_T$ & Transverse Momentum \\ \hline 
TPC & Time Projection Chamber \\ \hline 
TOF & Time of Flight \\ \hline 
m² & Square meters \\ \hline 
ps & Picoseconds \\ \hline 
HMPID & Ring Imaging Cherenkov Detectors \\ \hline 
${Xe\mathrm{-}CO}_{\mathrm{2}}$ & Xenon - Carbon Dioxide Gas \\ \hline 
PHOS & Photon Spectrometer \\ \hline 
EmCal & Electromagnetic Calorimeter \\ \hline 
$PbWO_{\mathrm{4}}$ & Lead Tungstate \\ \hline 
V0 & ALICE V0 Detector \\ \hline 
T0 & ALICE Fast timing and trigger detector \\ \hline 
PMD & Photomultiplicity detector \\ \hline 
FMD & Forward multiplicity detector \\ \hline 
ZDC & Zero Degree Calorimeters \\ \hline 
cm & Centimetre \\ \hline 
$\eta $ & Pseudorapidity \\ \hline 
MWPC & Multi-wire Proportional Chamber \\ \hline 
ns & Nanoseconds \\ \hline 
ADC & Analog to digital converter \\ \hline 
$\gamma$ & Relativistic Factor \\ \hline 
LQ1D & One-dimensional likelihood \\ \hline 
LQ2D & Two-dimensional likelihood \\ \hline 
AI & Artificial Intelligence \\ \hline 
MLPs & Multilayer Perceptrons \\ \hline 
ReLU & Rectified Linear Unit \\ \hline 
J & Objective function \\ \hline 
SGD & Stochastic Gradient Descent \\ \hline 
${\mathrm{\epsilonup }}_{\mathrm{i}}$ & Learning rate at iteration i \\ \hline 
$\boldsymbol{\alpha }$ & Momentum decay parameter \\ \hline 
v & Velocity \\ \hline 
$\boldsymbol{\mathrm{\thetaup }}$ & Parameter set \\ \hline 
$\mathrm{g}$ & gradient \\ \hline 
$\widetilde{\mathrm{\thetaup }\ }$ & Interim parameter update \\ \hline 
$\mathrm{\rhoup }$ & Length scale \\ \hline 
Adam & Adaptive Moments \\ \hline 
CNN & Convolutional Neural Network \\ \hline 
RNN & Recurrent Neural Network \\ \hline 
ANN & Artificial Neural Network \\ \hline 
${\mathrm{s}}^{\mathrm{(t)}}$ & State of a dynamical system at timestep t \\ \hline 
Z & Latent space \\ \hline 
I & Identity matrix \\ \hline 
$\mathcal{D}$ & Kullback-Leibler divergence \\ \hline 
$\mathrm{\muup }$ & Mean \\ \hline 
$\mathit{\Sigma}$ & Multidimensional standard deviation \\ \hline 
$\sigmaup$ & Standard deviation \\ \hline 
D & Discriminative Neural Network \\ \hline 
G & Generative Neural Network \\ \hline 
BiGANs & Bidirectional Generative Adversarial Networks \\ \hline 
LSGANs & Least Squares Generative Adversarial Networks \\ \hline 
${\mathrm{H}}_0$ & Null Hypothesis \\ \hline 
t & Test statistic \\ \hline 
${\mathrm{t}}_{\mathrm{cut}}$ & Threshold value for test statistic \\ \hline 
$\mathrm{\alphaup }$ & Significance level \\ \hline 
$\mathrm{1-}\mathrm{\betaup }$\textbf{} & Power \\ \hline 
${\mathrm{\varepsilonup }}_e$ & Electron Efficiency \\ \hline 
${\mathrm{\varepsilonup }}_{\pi }$ & Pion Efficiency \\ \hline 
PDG & Particle Data Group \\ \hline 
$V_0$ & Primary vertex \\ \hline 
${\mathrm{\gammaup }}_1$ & Skewness \\ \hline 
 &  \\ \hline 
\end{tabular}



\noindent 

\noindent 

\noindent \eject 

\noindent THE APPLICATION OF DEEP LEARNING TECHNIQUES TOWARDS PARTICLE IDENTIFICATION AND HIGH ENERGY PHYSICS EVENT SIMULATIONS RELATING TO THE ALICE TRD AT CERN

\noindent Chapter 2: High Energy Physics \& The CERN Experiment

\noindent 4  Christiaan Gerhardus Viljoen - September 2019

\noindent Christiaan Gerhardus Viljoen - September 2019   3


\section{ Introduction}

\noindent \eject 


\subsection{ Background}

\noindent 

\noindent This Masters Dissertation seeks to apply cutting edge techniques in Machine Learning (ML) towards:

\noindent 

\begin{enumerate}
\item  Particle identification of electrons and pions, from raw signal data produced by these particles as they traverse the Transition Radiation Detector (TRD), using deep feedforward neural networks, convolutional neural networks and recurrent neural networks, specifically Long-Short-Term Memory (LSTM) networks
\end{enumerate}

\noindent 

\begin{enumerate}
\item  Distinguishing between real data and data simulated by the Geant4 Monte Carlo simulation environment
\end{enumerate}

\noindent 

\noindent 

\begin{enumerate}
\item  Prototyping of variational autoencoders and Generative Adversarial Networks towards the simulation of High Energy Physics (HEP) collision events
\end{enumerate}

\noindent 

\noindent The motivation for each of these elements is as follows:

\noindent 

\begin{enumerate}
\item  Accurate particle identification (in particular, electron samples that are as pure as possible) allows physicists at the ALICE (A Large Ion Collider Experiment) experiment to study the properties of the Quark Gluon Plasma (QGP), the primordial state of matter in the early universe. Since this deconfined state of matter rehadronizes quite soon after forming, it cannot be studied directly, but only via its decay products, of which the electron is one. To this end, having an electron sample which is as pure as possible is desirable and being able to accurately reject pions from the electron sample, whilst keeping as many as possible actual electrons in the sample under investigation, is a major concern
\end{enumerate}

\noindent 

\begin{enumerate}
\item  Being able to distinguish Monte Carlo simulations from real data, could be indicative that Monte Carlo simulations, used for calibration and calculations of detector response functions, etc. are not accurate enough and that they could potentially be tuned via various parameter settings in future studies to increase their accuracy
\end{enumerate}

\noindent 

\noindent 

\begin{enumerate}
\item  Using deep generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), instead of Monte Carlo simulations, could be a desirable future course of action, since these simulations are extremely fast compared to Geant4 simulations; but their use is contingent on whether they provide comparable accuracy to Geant4 simulations, as well as their customizability, e.g. is it possible to specify which particle, and at what momentum you want to simulate?
\end{enumerate}

\noindent 

\noindent A variety of different software packages were utilized during the course of this project, including ROOT for data extraction, Geant4 for event simulation, Python and R for statistical analysis and Keras with a Tensorflow back-end for deep learning implementations.

\noindent \eject 


\subsection{ Summary of Work Done \& Major Findings}

\noindent The lowest pion efficiency at 90\% electron efficiency obtained with chamber-gain corrected data was 2.2\%. This result was obtained using a convolutional neural network as described in 8.1.

\noindent 

\noindent The highest balanced accuracy in distinguishing Geant4 simulated data from true raw data was 91.5\%. This was also achieved using a convolutional network, discussed in 8.2.

\noindent 

\noindent In terms of Deep Generative Models, Variational Autoencoders gave results that look quite realistic to the human eye, but which could be easily distinguished from real data using a convolutional neural network.

\noindent 

\noindent Several variations on the Generative Adversarial Network concept was tested out, each of which performed in vastly different ways, but none of these were found to generate samples that looked as realistic as those obtained from Variational Autoencoders or which could compete with Geant4 simulations.


\subsection{ The Structure \& Organization of this Dissertation}

\noindent Chapter 2: High Energy Physics \& The CERN Experiment begins with a history of atomic theory from Democritus to the Standard Model of Particle Physics, outlines the fundamental particles and forces and the standard model vertices, which explains their interactions; then touches upon two ways in which particles interact with matter relevant to this thesis and a quick overview of the Quark Gluon Plasma. Next, the European Organization for Nuclear Research (CERN) experiment is discussed, in terms of its establishment, particle acceleration hardware and the various experiments conducted at the LHC today, as well as a discussion of its currently used software packages for data analysis (ROOT) and simulation (Geant4).

\noindent 

\noindent Chapter 3: The ALICE Detector \& the Transition Radiation Detector goes into detail about the ALICE detector, focussing on the Transition Radiation Detector (TRD) and methods currently used for particle identification in the TRD, with a brief overview of their performance.

\noindent 

\noindent Chapter 4: Deep Learning introduces Deep Learning within the larger context of Machine Learning and Artificial Intelligence, then discusses the original theory upon which modern deep learning is based: Rosenblatt's perceptron. After this, the mathematical background for deep learning used in this dissertation is discussed, including feedforward neural networks, backpropagation, methods for regularization, convolutional- and recurrent neural networks and two types of generative models, namely variational autoencoders and generative adversarial networks.

\noindent 

\noindent Chapter 5: Statistical Tests is a short chapter explaining the statistical tests used for particle selection in this thesis at the hand of: hypotheses, significance level and power. It also introduces the concepts of electron- and pion efficiency, which is important in understanding the Results section of this dissertation.

\noindent 

\noindent Chapter 6: Data outlines the format of the data used in this thesis, along with some example plots of TRD tracklet signals, electron and pion counts per run, Bethe Bloch curves for electrons and pions per run as well as n$\sigmaup$-electron plots for electrons and pions per run.

\noindent 

\noindent Chapter 7 is the Methods section of this dissertation.

\noindent 

\noindent Chapter 8 is the Results section of this dissertation, where only the most successful results are discussed.

\noindent 

\noindent Chapter 9 contains the Discussion and Conclusions, including a deeper analysis of results, as well as an outline of future work which can be done in this area.

\noindent 

\noindent 

\noindent 

\noindent \eject 


\section{ High Energy Physics \& The CERN Experiment}

\noindent \eject 


\subsection{ A Brief History of Atomic Theory}

\noindent 

\noindent The earliest correct model for the atom can be traced back to 400 BCE, when Democritus proposed that the entire universe consisted of fundamental particles, or ``Atoms'', which cannot be divided any further.

\noindent 

\noindent In 1803, Dalton refined this model to state that these indivisible atoms can have distinguishing chemical and physical traits and that they combine to form chemical compounds.

\noindent 

\noindent Then, in 1987, JJ Thompson discovered the electron and proposed a -- subsequently proven to be incorrect -- theory for subatomic structure, in which negatively charged electrons were embedded between positive charges within an atom.

\noindent 

\noindent Rutherford, Marsder and Geiger disproved this model in 1911, with their seminal alpha-particle scattering experiment and put forth a more accurate model for the atom, in which most of the atom consists of empty space, with a dense core of positively charged protons.

\noindent 

\noindent In 1913, Bohr refined this model further, indicating that electrons orbit the positively charged atomic core at distinct energy levels. While this model \textit{did} explain the emission spectrum of Hydrogen, it could not explain the emission spectra of any of the other elements.

\noindent 

\noindent Between 1924 -- 1928, De Broglie, Heisenberg and Schr\"{o}dinger each separately developed a similar quantum paradigm, where electrons have wave-like properties and appear in much more complex orbitals. This is still the accepted theory of atomic structure today.

\noindent 

\noindent There have been some refinements made to the quantum theory, as new information has come to light: a neutral subatomic particle, the neutron, was discovered in 1932, which solved the puzzle of why atoms were found to be nearly twice as heavy as expected based on proton number; this discovery also disproved Dalton's second law, which stated that all atoms of a specific element were identical; which resulted in the concept of isotopes (atoms with the same number of protons, but differing numbers of neutrons). In the same year, Cockroft and Walton split the atom for the first time, by bombarding Lithium atoms with electrons, splitting them into two Helium particles.

\noindent 

\noindent The 1950s brought about a new era in nuclear physics, in which particle accelerators with collision energies of a few hundreds of MeVs became affordable, along with cosmic ray and inelastic proton-scattering experiments; since this time, a whole host of subatomic elements have been discovered, many of which are unstable. The discovery of these new particles has led, over time, to the development and refinement of the modern Standard Model of Particle Physics. 


\subsection{ The Standard Model of Particle Physics}

\noindent 


\paragraph{ Introduction}

\noindent 

\noindent The Standard Model of Particle Physics is a framework which allows us to understand the fundamental structure and dynamics of our universe in terms of elementary particles, where all interactions between elementary particles are similarly facilitated by an exchange of particles. In summary, based on our current understanding, our entire universe consists of a very sparse array of fundamental particles once we delve into the subatomic realm \eqref{GrindEQ__1_}.

\noindent 

\noindent At an energy scale of ${10}^0$ electron Volts (an electron Volt, eV, is a unit of energy, equivalent to the amount of work required to accelerate a single electron through a potential difference of 1 Volt), the low energy manifestation of Quantum Electrodynamics (QED) allows atoms to exist in bound states with negatively charged electrons ($e^-$) orbiting a positively charged nucleus consisting of positively charged protons ($p$) and electrically neutral neutrons ($n$), based on the electrostatic attraction of these opposing electrical charges \eqref{GrindEQ__1_}.

\noindent 

\noindent Quantum mechanics explains the emergence of unique physical properties in different elements, which arise from their exact electronic structures. Quantum Chromodynamics (QCD) is the fundamental theory of the strong interaction, which binds protons and neutrons together within the nucleus of the atom. Similarly, at this energy scale, the weak force causes nuclear $\beta$-decays of radioactive isotopes and is involved in the nuclear fusion processes that occur within stars; the nearly massless electron neutrino $v_e$) is produced during both of the abovementioned processes \eqref{GrindEQ__1_}.

\noindent 

\noindent Therefore, almost all physical phenomena that occur under normal circumstances can be explained by the Electromagnetic-, Strong- and Weak Forces, Gravity (which is very weak, but explain the large-scale structure of the universe), and just four fundamental particles: the electron, proton, neutron and electron neutrino \eqref{GrindEQ__1_}.


\paragraph{ The Fundamental Particles}

\noindent 

\noindent At higher energy scales, of the order of ${10}^9$ electron Volt (or ${10}^0$ giga-electron Volt, 1 GeV), protons and neutrons are understood to be bound states of truly fundamental particles called quarks, in the following manner: protons consist of two up-quarks and a down-quark p(uud), whereas neutrons consist of two down-quarks and an up-quark n(ddu) \eqref{GrindEQ__1_}.

\noindent 

\noindent At the lowest energy level of the standard model, the first generation of particles are then the electron, electron neutrino, the up-quark and the down-quark; these are currently considered to be truly elementary, in that they cannot be subdivided \eqref{GrindEQ__1_}.

\noindent 

\noindent Higher energy scales, such as those achieved at modern particle accelerators, result in the second and third generation of the four elementary particles; these are heavier versions of the first generation: for example, the muon (${\mu }^-$) is essentially a version of an electron which is 200 $\times$ heavier than a low energy electron, i.e. $m\ \ \ \ \ 200\ m_e$. The tau-lepton (${\tau }^-$) is the third generation of the electron, and is much heavier, i.e. $m_{\tau \ }\approx 3500{\ m}_e$. These mass differences do have physical consequences, but the fundamental properties and interactions of the various generations remain identical \eqref{GrindEQ__1_}.

\noindent 

\noindent Current experimental evidence indicates that there are no further generations than these three, and so all matter in the universe seems to be circumscribed by the following twelve fundamental fermions, reproduced from \eqref{GrindEQ__1_}:

\noindent 

\noindent \textbf{Table 1: The twelve fundamental fermions.}

\begin{tabular}{|p{0.7in}|p{0.5in}|p{0.3in}|p{0.6in}|p{0.7in}|p{0.5in}|p{0.7in}|} \hline 
\multicolumn{4}{|p{1in}|}{\textbf{Leptons}} & \multicolumn{3}{|p{1.9in}|}{\textbf{Quarks}} \\ \hline 
 & Particle & Q & Mass/GeV & Particle & Q & Mass/GeV \\ \hline 
First Generation & Electron ($e^-$) & -1 & 0.005 & Down (d) & -1/3 & 0.003 \\ \hline 
 & Neutrino ($v_e$) & 0 & $\mathrm{<}$ ${\mathrm{10}}^{\mathrm{-9}}$ & Up (u) & +2/3 & 0.005 \\ \hline 
Second Generation & Muon (${\mu }^-$) & -1 & 0.106 & Strange (s) & -1/3 & 0.1 \\ \hline 
 & Neutrino ($v_{\mu }$) & 0 & $\mathrm{<}$ ${\mathrm{10}}^{\mathrm{-9}}$ & Charm (c) & +2/3 & 1.3 \\ \hline 
Third Generation & Tau (${\tau }^-$) & -1 & 1.78 & Bottom (b) & -1/3 & 4.5 \\ \hline 
 & Neutrino ($v_{\tau }$) & 0 & $\mathrm{<}$ ${\mathrm{10}}^{\mathrm{-9}}$ & Top (t) & +2/3 & 174 \\ \hline 
\end{tabular}



\noindent While it is accepted that neutrinos are not massless, their masses are so small that they have not been precisely determined, however, the upper bounds for the estimated masses for neutrinos are around 9 orders of magnitude smaller than the other fermions \eqref{GrindEQ__1_}.

\noindent 

\noindent The Dirac equation describes the state of each of the twelve fundamental fermions and indicates that for each fermion there is an antiparticle which has the same mass but opposite charge, which is indicated by a horizontal bar over the particle's symbol, or a charge symbol of the opposite sign, e.g. the anti-down quark is indicated by d¯, whereas the antimuon is indicated by ${\mu }^+$$\includegraphics*[width=0.06in, height=0.16in, keepaspectratio=false]{image3}\ CITATION\ Tho13\ \backslash l\ 1033\ \ \eqref{GrindEQ__1_}$.

\noindent 

\noindent Interactions between particles are facilitated by the four fundamental forces, but the effect of gravity at this scale is sufficiently negligible that it can be ignored without loss of accuracy. All particles take part in weak interactions and are therefore subject to the weak force. The neutrinos are all electrically neutral and therefore are not involved in electromagnetic interactions and are, so to speak, invisible to this force. Quarks carry what is termed as ``colour charge'' by QCD and are therefore the only particles that feel the strong force \eqref{GrindEQ__1_}. 

\noindent 

\noindent The strong force confines quarks to confined states within hadrons and quarks are therefore not freely observed under normal circumstances \eqref{GrindEQ__1_}.


\paragraph{ The Fundamental Forces}

\noindent 

\noindent Classical electromagnetism explained the electrostatic interaction between particles using a scalar potential, Newton stated that matter could interact with any other matter without the mediation of direct contact \eqref{GrindEQ__1_}.

\noindent 

\noindent Quantum Field Theory circumvents this non-material explanation and encompasses the description of each of the fundamental forces. Electromagnetism is explained by Quantum Electrodynamics (QED), the Strong Force by Quantum Chromodynamics (QCD), the weak force by the Electroweak Theory (EWT), Gravity has not been explained by the Standard Model yet; therefore, Einstein's General Theory of Relativity is still the best explanation of this force, but it falls within the bounds of Classical Physics. As such, the search to incorporate gravity into the Standard Model is an ongoing area of research and has resulted in exciting new theoretical research avenues such as string theory and loop quantum gravity arising \eqref{GrindEQ__1_}.

\noindent 

\noindent Looking at electromagnetism, the interaction between charged particles occurs via the exchange of massless virtual photons, which explains momentum transfer via a particle exchange and circumvents the issue of a non-physical potential as the medium of interaction \eqref{GrindEQ__1_}.

\noindent 

\noindent Similarly, there are virtual particles (gauge bosons) for both the Strong Force (i.e. the massless gluon) and Weak Force (i.e. $W^+$ and $W^-$ bosons, which are around 80 times heavier than the proton; and the Z boson, which facilitates a weak neutral-current interaction). The gauge bosons all have spin 1, compared to the fermions whom all have spin ½ \eqref{GrindEQ__1_}.

\noindent 


\paragraph{ The Higgs Boson}

\noindent 

\noindent The Higgs Boson, whose existence was confirmed by the CMS and ATLAS collaborations at CERN in 2012, but proposed in 1964 by three separate theoretical papers, breaks rank with the other particles outlined by the standard model in that it is many orders of magnitude heavier and is a scalar particle which endows other standard model particles with mass, a property without which all particles would constantly move at the speed of light, $c$$\includegraphics*[width=0.06in, height=0.16in, keepaspectratio=false]{image4}\ CITATION\ Tho13\ \backslash l\ 1033\ \ \eqref{GrindEQ__1_}$. 

\noindent 

\noindent The Higgs boson manifests as a disturbance of the Higgs field, which is non-zero in a vacuum, in contrast to the other fundamental particles which all have a vacuum expectation value of zero, i.e. $<\mathrm{O}>\mathrm{=}0$$\includegraphics*[width=0.06in, height=0.16in, keepaspectratio=false]{image5}\ CITATION\ Tho13\ \backslash l\ 1033\ \ \eqref{GrindEQ__1_}$.

\noindent 

\noindent In QFT, an expectation value is a real number calculated as the average over the expected values of an observable, weighted according to their respective likelihood.

\noindent 

\noindent On their own, all particles are massless, but by interacting with the Higgs Field, which is always non-zero, the Higgs mechanism gives them their distinguishing masses \eqref{GrindEQ__1_}.

\noindent 


\subsection{ Standard Model Vertices}

\noindent 

\noindent The properties of the bosons in the associated quantum field theory for the various forces of the Standard Model (i.e. QCD for the strong force, QED for the electromagnetic force and EWT for the weak force), along with their coupling with the spin-half fermions, are illustrated by three-point interaction vertices of a gauge boson with an incoming and outgoing fermion. Each of these interactions also has an associated coupling strength $g$ \eqref{GrindEQ__1_}.

\noindent 

\noindent A particle will only couple with the force-carrying boson if it carries the interaction's charge, for instance quarks are the only particles that carry colour charge and are therefore the only particles that can participate in the strong interaction with a gluon; similarly, only charged particles can interact with photons; but since all 12 of the fundamental fermions listed in Table 1 carry the weak isospin charge involved in the weak interaction, they all participate in this interaction \eqref{GrindEQ__1_}.

\noindent 

\noindent The weak charged-current interaction differs from the other forces in that it is involved in the coupling of different flavour fermions. The ${\mathrm{W}}^{\mathrm{+}}\ \mathrm{and\ }W^-$ bosons carry charges +e and $-$e respectively, so in order for electric charge to be conserved, this interaction can only occur between pairs of fermions that differ by one unit of electric charge \eqref{GrindEQ__1_}.

\noindent 

\noindent Figure 1 shows the main Standard model interaction vertices in the form of Feynman diagrams.

\noindent 

\noindent \includegraphics*[width=4.36in, height=4.47in, keepaspectratio=false]{image6}

\noindent \textbf{Figure 1: Standard model interaction vertices \eqref{GrindEQ__2_}}

\noindent 


\subsection{ Interactions of Particles with Matter}

\noindent 

\noindent In order to study subatomic particles, they need to be detected. Most particles produced during High Energy Physics Experiments are unstable and therefore decay within a specific characteristic mean lifetime $\tau $. Those particles with $\tau >{10}^{-10}s$ will traverse several meters before decaying and are therefore directly detectable by particle detectors such as those installed at the Large Hadron Collider (LHC) at CERN. Particles with shorter lifespans are usually detected indirectly, by the interaction of their decay products with detector material \eqref{GrindEQ__1_}.


\paragraph{ The Bethe-Bloch Curve}

\noindent 

\noindent The Bethe-Bloch equation describes the energy lost by a charged particle moving at relativistic speed through a medium, as a result of electromagnetic interactions with atomic electrons. A single charged particle with velocity $v=\ \beta c$, passing through a medium with atomic number $Z$ and density $n$, will lose energy as a result of ionisation of the medium, as a function the distance travelled in the medium, according to the Bethe-Bloch formula \eqref{GrindEQ__1_}:

\noindent 
\[\frac{dE}{dx}\mathrm{\ }\mathrm{\approx }\mathrm{\ -4}\pi {\mathrm{\hslash }}^{\mathrm{2}}c^{\mathrm{2}}{\alpha }^{\mathrm{2}}\frac{nZ}{m_ev^{\mathrm{2}}}\left\{ln\left[\frac{\mathrm{2}{\beta }^{\mathrm{2}}{\gamma }^{\mathrm{2}}c^{\mathrm{2}}m_e}{I_e}\right]\mathrm{-}{\beta }^{\mathrm{2}}\right\}\] 


\noindent Figure 2 illustrates the characteristic energy loss curves for various subatomic particles as measured by the TPC, including the two subatomic particles studied in this project, the pion $\pi $ and the electron $e^-$.

\noindent 

\noindent \includegraphics*[width=4.55in, height=3.09in, keepaspectratio=false]{image7}

\noindent \textbf{Figure 2: Bethe-Bloch curve for various subatomic particles as measured by the ALICE TPC at $\sqrt{\boldsymbol{s}}\boldsymbol{=}\boldsymbol{7}\boldsymbol{TeV}$}


\paragraph{ Transition Radiation}

\noindent Transition radiation is radiation emitted by a charged particle as it traverses the boundary between two mediums with different optical properties, no significant energy loss occurs in this process, but the resultant radiation is an important aid in detecting charged particles in HEP experiments \eqref{GrindEQ__3_}.

\noindent 

\noindent For relativistic particles, the photons emitted in this process extends into the X-ray domain and is highly forward-peaked compared to the direction the particle is moving in; transition radiation yield is increased by stacking multiple radiative boundaries in gas detectors, such as the Transition Radiation Detector (TRD) at ALICE, and placing high atomic number (high-Z) gases within subsequent chambers to absorb the emitted X-ray photons \eqref{GrindEQ__4_}.

\noindent 

\noindent One of the main aims of this thesis is distinguishing electrons from pions. This is facilitated by the fact that electrons and pions have different characteristic energy loss curves, and particularly at low momenta, electrons have a higher relative energy loss, as well as the fact that electrons emit transition radiation and pions don't.


\subsection{ The Quark Gluon Plasma (QGP)}


\paragraph{ Introduction to QGP}

\noindent 

\noindent As mentioned above in 2.2.2, quarks and gluons are confined by the Strong Force to remain within the bound states of colour-neutral hadrons (e.g. protons and neutrons) and are therefore never found freely in nature. However, the currently held view of the early universe, predicted by the standard model and supported by over three decades of High Energy Physics experiments and lattice QCD simulations, is that directly subsequent to the Big Bang, the universe was composed of a deconfined state of matter, known as the Quark-Gluon Plasma (QGP) \eqref{GrindEQ__5_}.

\noindent 

\noindent Statistical mechanics understands matter as a system in thermal equilibrium. Global observables, such as net charge, temperature and energy density define the average properties of such a system. As these global observables take on different values, radically different average properties can be held by the system, manifesting as different states of matter bounded by phase boundaries, which matter traverses via phase transitions \eqref{GrindEQ__6_}, see Figure 4 for an illustration of this process.

\noindent 

\noindent \includegraphics*[width=3.05in, height=2.93in, keepaspectratio=false]{image8}

\noindent \textbf{Figure 3: Simplified diagram of classical states of matter and transitions between them, with the Vacuum added as a fifth element, providing the space in which matter exists \eqref{GrindEQ__7_}, reproduced and modified from \eqref{GrindEQ__6_}}

\noindent 

\noindent If nucleons (protons and neutrons) were truly fundamental, i.e. if they were not bound states of smaller composite elements (quarks and gluons), a density limit of matter would be reached, when compressing it under ever higher pressure conditions. If, however, nucleons were truly composite states, increasing density would eventually cause their boundaries to overlap and nuclear matter would transition from a stable state of colour-neutral three-quark or quark-antiquark hadronic matter to a state of deconfinement, consisting mainly of unbound quarks \eqref{GrindEQ__6_}.

\noindent 

\noindent Hadrons all have the same characteristic radius of around 1 fm; it has been found experimentally that increasing density (through compression or heating), results in the formation of clusters where there are more quarks within such a hadronic volume than logical partitioning into colour neutral hadrons allows for, thus leading to colour-deconfinement \eqref{GrindEQ__6_}.

\noindent 

\noindent In Figure 5, a simplified phase diagram of hadronic matter is depicted. Within the hadronic phase, there is a baryonic density/temperature boundary where transitions between mesons (colour-neutral quark-antiquark systems) and nucleons (colour-neutral three-quark systems) occur, (not shown in this diagram). The existence of diquarks as localised bound states within the QGP medium allows for yet another state of matter, the colour superconductor, discussion of which is outside of the scope of this dissertation.

\noindent 

\noindent \includegraphics*[width=4.48in, height=3.19in, keepaspectratio=false]{image9}

\noindent \textbf{Figure 4: Phase diagram of hadronic matter \eqref{GrindEQ__8_}}

\noindent 


\paragraph{ QGP, the Big Bang and the Micro Bang}

\noindent 

\noindent It is estimated that at t ${10}^{-43}s$ after the initial expansion of the Universe (affectionately termed the `big bang', but which is more accurately described as a `big inflation'), the prevailing temperature was T $\mathrm{\simeq }$ ${10}^{19}$ GeV, a temperature so high that the principles of general relativity do not apply, and which cannot be understood with present-day physical theory \eqref{GrindEQ__9_}. 

\noindent 

\noindent Quarks and gluons propagated freely in this early deconfined space-time QGP expansion phase of the Universe, down to a temperature of T $\mathrm{\simeq }$ 150 MeV, a phenomenon thought to be caused by a change in the vacuum properties of this extremely hot early Universe \eqref{GrindEQ__5_}.

\noindent 

\noindent To understand how matter was formed in the early Universe, heavy ion collisions, such as the Pb-Pb collisions performed at ALICE, result in a miniscule space-time domain of QGP (which one can refer to as a `micro bang'), in which local quark-gluon deconfinement occurs. The subsequent hadronization process, where protons, neutrons and other subatomic particles are formed, leaves traces in the ALICE detector material, giving physicists an indication of how matter arose as the early Universe rapidly cooled down \eqref{GrindEQ__5_}.

\noindent 

\noindent Since the QGP cannot be detected directly, it is studied via the interactions of its decay products with detector material. Accurately distinguishing between electrons and pions is an important step in this process and as such is the motivation for the particle identification phase of this Masters project.

\noindent 

\noindent \includegraphics*[width=4.51in, height=6.41in, keepaspectratio=false]{image10}

\noindent \textbf{Figure 5: The evolution of the Universe, from the Big Bang to Modern Day \eqref{GrindEQ__10_}}


\subsection{ The CERN Experiment}

\noindent At the end of 1951, a resolution was agreed upon to establish a European Council for Nuclear Research (CERN: \textit{Conseil Europ\'{e}en pour la Recherche Nucl\'{e}aire)}~at an intergovernmental UNESCO meeting in Paris. The final draft of the CERN commission was signed by twelve nations in 1953 \eqref{GrindEQ__11_}.

\noindent 

\noindent Today, CERN is a truly international organization, with 22 member states, who contribute to operating costs and are involved in major decision making, many countries with observer status, and even more non-member countries with co-operation agreements, including South Africa \eqref{GrindEQ__12_}.

\noindent 

\noindent CERN's research mandate revolves around finding answers to fundamental questions about the structure and evolution of our universe, as well as its origins; it aims to achieve these goals by providing access to its particle accelerator facilities and compute resources to international researchers, who perform research that advances the forefront of human knowledge, for the benefit of humanity as a whole. As such, CERN is politically neutral and advocates for evidence-based reasoning, knowledge transfer from fundamental research to industry and grass-roots development of future generations of scientists and engineers \eqref{GrindEQ__13_}.


\paragraph{ Hardware}

\noindent In order to fulfil its ambitious goals, CERN's facilities, located under the Franco-Swiss border (see Figure 7 for geographical context), boasts an intricate system of particle accelerators and -detectors and a data centre with over 174,000 processor cores, 150,000 Terabytes (TB) of Disk space and over 1,000 TB of random access memory (RAM) \eqref{GrindEQ__14_}; this main datacentre is connected both to its extension in Budapest, Hungary and the multi-tier Worldwide LHC Computing Grid (WLCG), all of which operates at a data transfer rate of around 10 Gigabytes/second (GiB/s).

\noindent 

\noindent \includegraphics*[width=5.78in, height=3.37in, keepaspectratio=false]{image11}

\noindent \textbf{Figure 6: CERN facilities in geographical context \eqref{GrindEQ__15_}.}

\noindent 
\subparagraph{High Energy Particle Accelerators}

\noindent 
{\bf The LHC Compared to Accelerators from Other Experiments World-wide}

\noindent 

\noindent At a circumference of 27 km, the Large Hadron Collider (LHC) is currently the largest particle accelerator in the world \eqref{GrindEQ__16_}. To put this into perspective, the Relativistic Heavy Ion Collider (RHIC), located at the Brookhaven National Laboratory in New York, has a circumference of 3.8 km \eqref{GrindEQ__17_}, Fermilab's Tevatron, which is no longer in operation, was 6.3 km in circumference \eqref{GrindEQ__18_} and the KEKB accelerator in Tsukuba, Japan also has a circumference of around 3 km \eqref{GrindEQ__19_}.

\noindent 

\noindent It is also the most powerful particle accelerator in the world, with a centre of mass energy of 13 Tera-electron-Volts ($E_{CM}=13\ TeV$ \eqref{GrindEQ__16_}), compared to RHIC, which operates at $E_{CM}\approx 200\ GeV$ \eqref{GrindEQ__17_}, the Tevatron, which reached $E_{CM}\approx 1.8\ TeV$ \eqref{GrindEQ__18_} and KEKB at $E_{CM}\approx 10.58\ GeV$ \eqref{GrindEQ__20_}.

\noindent 

\noindent 
{\bf The LHC}

\noindent 

\noindent The LHC, located 50-175 m underground, is the final step in a chain of successive accelerators feeding beams of accelerated particles into each other at increasing energies, as can be seen in Figure 9. 

\noindent 

\noindent The LHC's proton source  is a bottle of compressed Hydrogen, which releases its contents into a Duoplasmatron device, which subsequently surrounds the $H_2$ molecules with an electrical field and separates it into its constituent protons and electrons \eqref{GrindEQ__21_}. A simplified diagram depicting this process can be seen in Figure 8.

\noindent 

\noindent 

\noindent \includegraphics*[width=5.10in, height=2.08in, keepaspectratio=false]{image12}

\noindent \textbf{Figure 7: The LHC Proton Source, connected to the Duoplasmatron device, which strips electrons off Hydrogen molecules, to produce the beams of protons which eventually collide within the LHC \eqref{GrindEQ__21_}}

\noindent 

\noindent 

\noindent A linear accelerator (LinAc2) injects these protons into a booster ring (PS booster) at an energy of 50 MeV, where proton beams are accelerated up to 1.4 GeV, before being injected into the Proton Synchrotron (PS), which accelerates them up to 25 GeV, the Super Proton Synchrotron (SPS) is the final intermediate step before proton beams enter the LHC and proton beams reach an energy of 450 GeV around this accelerator beam before they begin their 20 minute acceleration around the LHC before reaching an energy of 6.5 TeV each \eqref{GrindEQ__22_}.

\noindent 

\noindent To calculate the centre-of-mass energy at collision-time, we do:

\noindent 

\noindent $E_{MC}\mathrm{=\ }\sqrt{s}\mathrm{=\ }\sqrt{{\left(\sum^{\mathrm{2}}_{i\mathrm{=1}}{E_i}\right)}^{\mathrm{2}}\mathrm{-\ }{\left(\sum^{\mathrm{2}}_{i\mathrm{=1}}{{\boldsymbol{p}}_i}\right)}^{\mathrm{2}}}\mathrm{=\ }\sqrt{{\mathrm{2}}^{\mathrm{2}}\mathrm{-}{\mathrm{6.5}}^{\mathrm{2}}}$ = 13 TeV \eqref{GrindEQ__1_}

\noindent 

\noindent This equation is derived from the relativistic relationship between energy and momentum, where the rest energy (invariant mass of a particle) is the familiar $E_0=mc^2$ and the kinetic energy from acceleration is $p_{tot}=\ p^2+c^2$. To simplify the equations, the speed of light, $c$ is set at a constant $c=1$ \eqref{GrindEQ__23_}.

\noindent 

\noindent 

\noindent An entirely different protocol is employed to generate the lead ions used in heavy-ion collisions (pPb, PbPb) studied at ALICE. A highly pure Lead (Pb) sample is heated up to a temperature of 800$\circ$C and the resulting Pb vapour is ionized by an electron current, which manages to strip a maximum of 29 electrons from a single Pb atom. Those atoms with higher resulting charge are preferentially selected and accelerated through a carbon foil, which strips most ions to ${Pb}^{54+}$. These ions are accelerated through the Low Energy Ion Ring (LEIR) and subsequently through the PS and SPS, where it is passed through a second foil, which strips the remaining electrons and passes the fully ionized ${Pb}^{82+}$ ions to the LHC, where beams of Pb-ions are accelerated up to 2.56 TeV \eqref{GrindEQ__22_}; because there are many protons in a single lead ion, the collision energies reached in PbPb collisions reach a maximum of 1150 TeV \eqref{GrindEQ__22_}.

\noindent 

\noindent \includegraphics*[width=6.00in, height=5.33in, keepaspectratio=false]{image13}

\noindent \textbf{Figure 8: The CERN accelerator complex \eqref{GrindEQ__24_}.}

\noindent 

\noindent In order to achieve these high collision energies, a precise system of 1232 dipole magnets is required to keep particles in their circular orbits, with 392 quadrupole magnets employed to focus the two collision beams. The dipole magnets use niobium-titanium (NbTi) cables at a temperature of 1.9 K (-271.3$\circ$C). At these temperatures the cables conduct electricity with no resistance (i.e. they become superconducting) and allow the magnetic field to reach 8.3 Tesla (8.3 T) required to bend the beams around the circular LHC ring \eqref{GrindEQ__22_}.

\noindent 

\noindent The beams themselves are contained within a vacuum tube emptier than outer space ($P_{vac}=\ {10}^{-13}atm$) and are accelerated by electromagnetic resonators and accelerating cavities to 99.9999991\% of the speed of light, which means that a beam goes around the 26.659 km LHC ring around 11,000 revolutions/second, resulting in around a billion collisions per second \eqref{GrindEQ__22_}.

\noindent 

\noindent 
\subparagraph{The Seven CERN Experiments}

\noindent Collisions at the LHC result in a multitude of particles being produced. Observing the produced particles from different perspectives produces evidence relevant to different research streams; as such, there are several collaborations at CERN which use detectors with differing attributes to study specific areas within the broad area of fundamental subatomic Physics \eqref{GrindEQ__25_}.

\noindent 

\noindent ATLAS and CMS investigate a very broad range of particle physics. Their independent design specifications allow any new discoveries at one of these detectors, such as the discovery of the Higgs' Boson in 2012, to be corroborated  by the other \eqref{GrindEQ__25_}. Other research avenues pursued at these experiments include the search for additional dimensions as well as the constituent elements of dark matter. The ATLAS detector is the largest particle detector ever built, weighing 7000 tonnes with dimensions 46m $\times$ 25m $\times$ 25m \eqref{GrindEQ__26_}.

\noindent 

\noindent ALICE and LHCb are the other two main experiments at CERN and are tasked with the discovery of specific physical phenomena \eqref{GrindEQ__25_}. ALICE focuses on the extreme energy densities present during heavy ion collisions, which leads to the production of the Quark Gluon Plasma, a newly discovered phase of matter thought to have been dominant in the early universe, directly subsequent to the big bang \eqref{GrindEQ__27_}. LHCb investigates subtle distinguishing nuances in the matter-antimatter dichotomy, as evidenced by attributes of the beauty quark \eqref{GrindEQ__28_}.

\noindent 

\noindent TOTEM and LHCf are smaller experiments focused on forwardly thrown particles produced during non-central collisions, TOTEM investigates particles produced during non-central collisions on either side of the CMS experiment, while LHCf does the same for non-central collisions at the  ATLAS experiment \eqref{GrindEQ__25_}. LHCf uses some of these forwardly thrown particles produced at the LHC as a simulated source of cosmic rays to complement the calibration and interpretation of large-scale cosmic ray experiments \eqref{GrindEQ__29_}.

\noindent 

\noindent MoEDAL is the most recent experiment at CERN and searches for a hypothetical magnetic monopole particle; theoretically envisioned, the magnetic monopole would be a subatomic particle with its own magnetic charge, whose evidence of existence would manifest as extensive damage to the MoEDAL detector \eqref{GrindEQ__30_}.


\paragraph{ HEP Software}

\noindent 
\subparagraph{ROOT}

\noindent 

\noindent ROOT is an object oriented data analysis platform developed in C++ for High Energy Physics implementations; in addition to its data analysis capabilities, ROOT is also used to transform the petabytes of raw data from collision events at the LHC into more compact and useful representations \eqref{GrindEQ__31_}.

\noindent 

\noindent The basic ROOT framework provides default classes for most common use-cases and as the HEP community pushes research into new frontiers, they can use the object-oriented programming (OOP) approach followed by ROOT to make use of sub-classing and inheritance to extend existing classes. Similarly, the concept of encapsulation keeps the number of global variables to a minimum and increases the opportunity for structural reuse of code. The ROOT forums allow users of the platform to report bugs and suggest fixes and in this way contribute to the platform without being part of the official development team \eqref{GrindEQ__31_}.

\noindent 

\noindent ROOT is freely available for download from \eqref{GrindEQ__32_} and can be installed using precompiled binaries or built from source using the GNU g++ complier on Unix platforms, such as Linux or MacOSX; Windows 10 64-bit users can make use of the Ubuntu subsystem or locally hosted Linux Virtual Machines to install and use ROOT, but native Microsoft Windows is not supported \eqref{GrindEQ__31_}. 

\noindent 

\noindent Upon installation, running the following line in a Unix terminal

\noindent 

\noindent \textbf{$\boldsymbol{\mathrm{>}}$ echo \$ROOTSYS}

\noindent 

\noindent will print the symbolic path to the top of the ROOT directory, e.g.

\noindent 

\noindent /Users/gerhard/root

\noindent 

\noindent Looking at the contents of this directory, \textbf{\$ROOTSYS/bin} contains executables such as the main ROOT executable, daemons for remote ROOT file access and authentication of parallel processing capabilities, etc.

\noindent 

\noindent \textbf{\$ROOTSYS/lib} contains the libraries for the C++ interpreter, image manipulation, ROOT base classes, as well as interfaces with event generators.\textbf{}

\noindent 

\noindent Additional directories exist, i.e. \textbf{\$ROOTSYS/tutorials} which contains example .C macro files, \textbf{\$ROOTSYS/test} which contains .cxx files and \textbf{\$ROOTSYS/include} which contains the .h header files.

\noindent 

\noindent ROOT libraries are designed with minimal dependencies and as such are loaded as needed. At runtime, libCore.so (the core library) is always invoked; it is composed of the base-, container-, metadata-, OS specification- and ROOT file compression classes. Additionally, the interactive C++ interpreter library libCling.so is used by all ROOT 6 applications, it features a command line prompt with just-in-time interactive compilation to facilitate rapid application development and testing.

\noindent 

\noindent When building executables, libraries containing the needed classes are linked to. Extensive documentation is available online at the ROOT reference guides for ROOT 5 \eqref{GrindEQ__33_}, the version of ROOT developed and used for LHC run 1 and run 2; and ROOT 6 \eqref{GrindEQ__34_}, the version of ROOT developed for LHC run 3, scheduled to start in 2021 after the second long shut down period (LS2).

\noindent 

\noindent 
\subparagraph{AliROOT }

\noindent 

\noindent AliROOT and AliPhysics are built on top of the base ROOT architecture to provide functionality specific to the ALICE collaboration.

\noindent 

\noindent C++ classes define all the code in ROOT, AliPhysics and AliROOT and enables the user to create variables (data) and functions (methods) specific to each class, as its members. A class's variables are usually accessed via the class's methods \eqref{GrindEQ__35_}.

\noindent 

\noindent C++ code is split into header (.h) and implementation (.cxx) files, both having the same name as the class being defined. Header files list all the constants, functions and methods contained in a class. Implementation files use a class's methods to set and get variables' values in that class.

\noindent 

\noindent The concept of inheritance is frequently utilized to prevent unnecessary repetition of code. Child classes inherit common behaviours and attributes from base/ parent classes and define additional methods and variables that are not common to other classes deriving from the base class.

\noindent 
\subparagraph{Geant4}

\noindent 

\noindent Geant4 is a C++ toolkit for simulating how particles traverse through matter. Comprehensive and accurate simulations of particle detectors, using platforms like Geant4, is extremely important, since it provides a theoretical reference against which data can be compared. Should there be any statistically significant discrepancies between simulations and data, it could indicate that phenomena occurred which are not explicable by the Standard Model of Particle Physics and could in rare circumstances lead to the discovery of new fundamental principles of nature \eqref{GrindEQ__36_}.

\noindent 

\noindent Simulation software typically rests on four key components:

\begin{enumerate}
\item  Event Generation

\item  Detector Simulation

\item  Reconstruction

\item  Analysis
\end{enumerate}

\noindent 

\noindent In a typical High energy Physics Simulation set-up, Geant4 is often used as the Detector Simulation component, tied to an event generator such at Pythia or HIJING, with ROOT used for Reconstruction and Analysis. As such, Geant4 has well-defined interfaces to the other components in the simulation set-up \eqref{GrindEQ__37_}.

\noindent 

\noindent The following aspects of simulation are implemented in Geant4: materials and geometry of the detector system, fundamental particles and their transition through the detector and external electromagnetic fields, how the detector responds to these processes to generate data and storing said data for downstream analysis \eqref{GrindEQ__37_}.\eject 


\section{ The ALICE Detector \& the Transition Radiation Detector }

\noindent \eject 


\subsection{ The ALICE Detector System}

\noindent 

\noindent Colliding heavy ions, such as the Pb-Pb collisions conducted at the LHC and studied at ALICE, offers the most ideal experimental conditions currently achievable for the reproduction of the primordial QGP matter \eqref{GrindEQ__38_}. A transition from ordinary matter to a state of deconfinement occurs at a critical temperature $T_c\approx 2\ \times \ {10}^{12\ }K$, which is around100,000 times hotter than the core temperature of our sun \eqref{GrindEQ__38_}.

\noindent 

\noindent The QGP cannot be probed directly, but is studied via particles produced during the hadronization process that occurs as the QGP cools down and quarks and gluons recombine in various ways; the ordinary-matter particles produced in this process interact with various detector elements and leave traces in the detector material that are generally recorded via electronic signals \eqref{GrindEQ__38_}.

\noindent 

\noindent The scale of the ALICE detector system is illustrated in Figure 10. The detector weighs 10,000 tonnes and has spatial dimensions 26m $\times$ 16m $\times$ 16m \eqref{GrindEQ__27_}.

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.33in, keepaspectratio=false]{image14}

\noindent \textbf{Figure 9: The ALICE detector system \eqref{GrindEQ__39_}}

\noindent 

\noindent A uniform magnetic field is applied over the detector, to allow particles to propagate in curved paths through the detector geometry, with the extent of curvature of the particle's track through the detector being inversely correlated to the particle's momentum; additionally, the sign of charge of a particle can also be deduced from its track curvature \eqref{GrindEQ__38_}.

\noindent 

\noindent The ALICE detector has a total of 18 stacked subdetectors involved in specific particle tracking tasks, these are broadly divided into: Tracking Systems, situated closest to the collision area, which make use of digital track-reconstruction of particle-detector interaction traces to indicate the path of a particle; these are followed by Electromagnetic and Hadronic Calorimeters, through which particle cascades are generated as particles enter and are absorbed by the calirometric material, with the magnitude of a particle's energy deposition acting as the signal in these subdetectors; all of which is surrounded by the Muon System in the outermost layer, which detects muons, which interact very weakly with matter and therefore generally travel much further through the detector system \eqref{GrindEQ__38_}.

\noindent 

\noindent High momentum resolution is obtained in all the detector elements over the high multiplicity densities (number of particles produced per unit volume) present in heavy ion collisions \eqref{GrindEQ__40_}. In addition to heavy ion collisions, lighter ion- as well as proton-nucleus and proton-proton collisions are also performed at ALICE, and this entire momentum range can be accurately measured by the ALICE detector \eqref{GrindEQ__40_}.

\noindent 

\noindent Looking at the detector geometry in more detail, we first find, closest to the collision area, a central barrel part for measuring photons, electrons, hadrons, as well as a forward muon spectrometer, all of which is embedded in a large solenoid magnet and which covers polar angles between 45$\circ$-135$\circ$. Moving outward from the first layer of the central barrel, we find an inner tracking system (ITS, Figure 11), consisting of 6 planes of silicon pixel detectors (SPD), silicon drift detectors (SDD) and silicon strip detectors (SSD), which provide for high resolution particle detection \eqref{GrindEQ__40_}.

\noindent 

\noindent \includegraphics*[width=4.99in, height=3.33in, keepaspectratio=false]{image15}

\noindent \textbf{Figure 10: ALICE Inner Tracking System (SPD, SDD, SSD) \eqref{GrindEQ__39_}.}

\noindent 

\noindent The main functions of the ITS are: 1) the reconstruction of secondary vertices in the decay of strange- and heavy flavour particles, 2) particle identification and tracking of particles with low momentum, and 3) improving the resolution of impact parameters and momentum. The outer SSD detectors have analog readout for particle identification via d\textit{E}/d\textit{x} (see section 2.4.1 The Bethe-Bloch Curve), in the non-relativistic (i.e. low $P_T$) region.

\noindent 

\noindent Next, as we move outwards, we find the Time-Projection Chamber (TPC, Figure 12).

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.33in, keepaspectratio=false]{image16}

\noindent \textbf{Figure 11: ALICE TPC \eqref{GrindEQ__39_}.}

\noindent 

\noindent As the main tracking detector, the TPC is a conservative system, sacrificing data volume and speed for redundant tracking mechanisms, which guarantee reliable performance, by ensuring good double-track resolution and by minimising space charge distortions \eqref{GrindEQ__40_}.

\noindent 

\noindent After the TPC, we find three Time of Flight (TOF) particle identification arrays (Figure 13).

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.33in, keepaspectratio=false]{image17}

\noindent \textbf{Figure 12: ALICE TOF \eqref{GrindEQ__39_}.}

\noindent 

\noindent Optimized for large acceptance and particle identification in the average momentum range, the TOF covers an area of 140 m² with 160~000 individual cells, the TOF offers time resolution of 100 ps.

\noindent 

\noindent Next, we find Ring Imaging Cherenkov Detectors (HMPID, Figure 14).

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.33in, keepaspectratio=false]{image18}

\noindent \textbf{Figure 13: ALICE HMPID \eqref{GrindEQ__39_}.}

\noindent 

\noindent A single-arm detector consisting of an array of proximity focusing ring imaging Cherenkov counters, the HMPID extends particle identification (especially the identification of hadrons) towards a higher spectrum of momentum \eqref{GrindEQ__40_}.

\noindent 

\noindent After the HMPID detectors, we get to the Transition Radiation Detector (TRD, Figure 15), part of the overarching topics of this dissertation. 

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.33in, keepaspectratio=false]{image19}

\noindent \textbf{Figure 14: ALICE TRD \eqref{GrindEQ__39_}}

\noindent 

\noindent The TRD identifies electrons of high momentum, above 1 GeV/c, to quantify production rates of quarkonia and heavy quarks in the mid rapidity (relativistic velocity) range \eqref{GrindEQ__40_}. Six time expansion wire chambers filled with ${\mathrm{Xe-CO}}_{\mathrm{2}}$ are used in conjunction with attendant composite polystyrene radiators to distinguish electrons from other particles by comparing their actual energy deposition in the detector to their characteristic d\textit{E}/d\textit{x }curves \eqref{GrindEQ__40_}.

\noindent 

\noindent The outer layers of the central barrel are occupied by two electromagnetic calorimeters (PHOS, Figure 16, and EMCal, Figure 17).

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.33in, keepaspectratio=false]{image20}

\noindent \textbf{Figure 15: ALICE PHOS \eqref{GrindEQ__39_}.}

\noindent 

\noindent Another single-arm detector, PHOS is an electromagnetic calorimeter which gives a high-granularity and -resolution view of photons, to distinguish their production mechanisms (i.e. whether they arise from thermal emission or hard QCD processes). Scintillating $\mathrm{PbW}{\mathrm{O}}_{\mathrm{4}}$ crystals amplify the signal to give good resolution of lower energy photons. Charged particles are vetoed by a set of multiwire chambers, inwardly adjacent to PHOS \eqref{GrindEQ__40_}.

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.33in, keepaspectratio=false]{image21}

\noindent \textbf{Figure 16: ALICE EMCal \eqref{GrindEQ__39_}.}

\noindent 

\noindent EMCal is a lead-scintillator sampling calorimeter, larger than PHOS, it is used in the measurement of jet production rates and fragmentation functions (functions used to calculate the probability that specific observed final states arise from a given quark or gluon) \eqref{GrindEQ__40_}.

\noindent 

\noindent All of the detectors in the central barrel, except for HMPID, EMcal and PHOS, cover the full azimuth, i.e. they can detect particles at all angles around the central collision area \eqref{GrindEQ__40_}.

\noindent 

\noindent Outside of the central barrel, a variety of smaller detector elements are found (V0, T0, PMD, FMD, ZDC) that are involved in the triggering of data collection for a specific event, as well as global event characterization \eqref{GrindEQ__40_}.

\noindent 

\noindent The forward muon arm (covering angles between 2$\circ$-9$\circ$ relative to the collision centre) completes the picture of the ALICE detector (Figure 18). It consists of 14 planes of triggering and tracking chambers, as well as various muon absorbers and its own dipole magnet \eqref{GrindEQ__40_}.

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.33in, keepaspectratio=false]{image22}

\noindent \textbf{Figure 17: ALICE forward Muon arm \eqref{GrindEQ__39_}.}

\noindent 

\noindent The measurement of heavy-quark resonance production is fulfilled by the Muon spectrometer, its small angle relative to the beam-line allows acceptance down to zero transverse momentum. It is made up of a composite absorber and ten thin cathode strip planes acting as high granularity tracking stations. An additional muon filter and four Resistive Plate Chambers are employed in the processes of triggering and muon identification. The muon spectrometer is protected from secondary particles produced in the beam pipe, by a 60 cm-thick absorber tube \eqref{GrindEQ__40_}.


\subsection{ The Transition Radiation Detector}

\noindent At particle momenta above 1 GeV/c, the pion rejection strategy for electron identification employed in the TPC is no longer sufficient. The TRD's main goal is to expand the range of the ALICE Collaboration's Physics objectives by providing accurate electron identification capabilities at these high momenta, by supplementing its own data with data obtained from the ITS and TPC; as well as the operation of event triggers that determine whether data from a specific collision should be kept, based on measurements such as collision centrality, amongst others. As an added benefit, the TRD informs the ALICE central barrel's calibration, and the data it produces is used extensively during track reconstruction and particle identification \eqref{GrindEQ__41_}.

\noindent 


\paragraph{ A Note on Geometry}

\noindent 

\noindent Figure 19 serves as a guide to understanding the coordinate system used at the LHC and in this thesis.

\noindent 

\noindent The point of beam intersection (the collision centre) acts as the zero-point in geometric coordinate expressions (x = 0, y = 0, z = 0). Cylindrical coordinates are specified from this origin, with the z-axis pointing along the beam line (with positive z coordinates indicated along this plane in the direction of the muon arm) \eqref{GrindEQ__40_}.

\noindent 

\noindent 

\noindent \includegraphics*[width=3.19in, height=2.76in, keepaspectratio=false]{image23}

\noindent \textbf{Figure 18: Cylindrical coordinates as used in geometric coordinate specifications for measurements made in experiments conducted at the LHC  \eqref{GrindEQ__42_}.}

\noindent 

\noindent Where appropriate, traditional Cartesian coordinates are used, for instance when talking about the location of a detector element. In these cases, the y-axis proceeds from the origin in the direction of the wires in the Multi-Wire Proportional Chambers (MWPC, discussed in section 3.2.3) and also indicates the direction of deflection in the magnetic field, the x-axis proceeds from the origin in the direction of electron drift \eqref{GrindEQ__40_}.

\noindent 

\noindent 

\noindent In order to specify the cylindrical coordinates ($\rhoup$, $\theta$, $\mathrm{\phi}$) of a point P, one can 538695026firstly 538695026GVGerhard Viljoen538695026-948782187I don't think I got this right. Phi and Eta seem to be the same value (angle from x-axis)obtain $\rhoup$, by measuring the distance from the origin to point P. Next, one would project a line from P onto a point Q on the \textit{xy}-plane, to obtain $\theta$, as the angle between the positive x-axis and the line segment from the origin to point Q. Finally, one would calculate $\mathrm{\phi}$ as the angle between the positive z-axis and the line segment from the origin to point P \eqref{GrindEQ__43_}.

\noindent 

\noindent An additional geometric term used in HEP literature is pseudorapidity, $\eta $, which is a specification of a particle's angle relative to the beam (z-) axis.

\noindent 


\paragraph{ TRD Design Synopsis}

\noindent 

\noindent Pseudorapidity coverage in the TRD is similar to the other detector elements in the central barrel, i.e. $\mathrm{|}\eta |\ \le 0.9$. The space between the TOF and TPC detectors is filled by the six layers of the TRD, which are subdivided in azimuthal angle into 18 sectors, with an additional segmentation into 5 sectors occurring along the z-axis. So, in total, we have 18 $\times \ $5$\ \times \ $6 = 540 individual detector elements in the TRD \eqref{GrindEQ__41_} at a radial distance of 2.9 -- 3.7 m from the beam axis \eqref{GrindEQ__44_}.

\noindent 

\noindent Each individual detector element consists of the following broad components: 1) a radiator (4.8 cm thick), 2) a 0.7 cm multiwire proportional 539212174readout chamber, 539212174GVGerhard Viljoen539212174-1485632640here is also a 3 cm drift region in here according to Pachmayer,

\noindent Need to figure out where and if part of 4.8cm mentionedand 3) front-end electronics to convert from an amplified particle energy-deposition signal to a digital signal, which is eventually stored if deemed interesting by the multi-tiered TRD trigger system \eqref{GrindEQ__41_}.

\noindent 

\noindent 


\paragraph{ TRD Measurement Mechanism}

\noindent 

\noindent As the name suggests, transition radiation occurs when a particle transits across a dielectric boundary, this radiation is often measured in particle detectors to inform track reconstruction. Multiple boundaries are typically required to increase radiation yield, and since highly relativistic particles emit transition radiation that extends into the X-ray domain, the TRD utilizes gases with high proton-number (Z) to absorb this radiation, resulting in a high yield of energy deposition relative to the energy lost via ionization \eqref{GrindEQ__41_}.

\noindent 

\noindent The drift time of gas particles within the MWPC provides fine-grained positional information about where the particle tracklet passed through the radiator. The detected signal takes the form of charged gas molecules (ionized via interaction with transition radiation photons and amplified through a chain of interactions between gas molecules), finally being absorbed by a negatively charged wire (anode), this process is shown in Figure 20.

\noindent 

\noindent \includegraphics*[width=5.92in, height=3.01in, keepaspectratio=false]{image24}

\noindent \textbf{Figure 19: A schematic representation of the components in an MWPC module}

\noindent 


\paragraph{ Particle Identification in the TRD}

\noindent 

\noindent At momenta p  $\mathrm{>}$ 1 GeV/c, the TRD provides electron identification via the measurement of transition radiation. At these momenta, pion rejection (achieved in the TPC via specific energy loss as per characteristic Bethe-Bloch dE/dx curves for pions vs. electrons) is no longer possible. The time evolution of signals generated in the TRD is an important factor in distinguishing between electrons and pions. The electron identification capability is also used to trigger at level 1 \eqref{GrindEQ__44_}.

\noindent 

\noindent Electron identification and triggering as mentioned above enables the in-depth study of physical phenomena such as jets, the semi-leptonic decay of heavy-flavour hadrons and the di-electron mass spectra of heavy quarkonia; in turn, these phenomena act as probes to study the Quark Gluon Plasma \eqref{GrindEQ__44_}.

\noindent 

\noindent The TRD signal originally induced on the segmented cathode plane is captured and processed by a preamplifier-shaper circuit, this processed signal is then digitized by a 10 MHz ADC to take samples of the time-evolution of the signal at defined 100 ns intervals \eqref{GrindEQ__44_}.

\noindent 

\noindent Figure 21 shows the time evolution of the abovementioned signal at p = 2 GeV, for both electrons and pions. The initial peak seen in earlier time-bins on the graph originates from the amplification region of the detector and the plateau that follows is caused by particles moving through the 3 cm drift region in the detector \eqref{GrindEQ__44_}.

\noindent 

\noindent \includegraphics*[width=5.88in, height=4.86in, keepaspectratio=false]{image25}

\noindent \textbf{Figure 20: Time evolution of the TRD signal, measured as pulse height vs drift time for electrons and pions (both at p = 2GeV) \eqref{GrindEQ__44_}.}

\noindent 

\noindent Also evident from Figure 21 is that, in this momentum region, the pulse height of electrons is much higher than that for pions, because electrons have higher characteristic energy loss (dE/dx) in this region \eqref{GrindEQ__44_}.

\noindent 

\noindent An average of one transition radiation photon in the X-ray domain will be emitted by an electron traveling at a highly relativistic speed (above $\gamma$ $\mathrm{\sim}$ 800), since it will cross many dielectric boundaries in the radiator portion of a detector element, the absorption of this type of photon is evidenced by an additional peak at later times in Figure 21, since it will be absorbed preferentially close to the radiator, adding its signal to the ionization energy of the track \eqref{GrindEQ__44_}.

\noindent 

\noindent In order to distinguish muons originating from particle-particle collisions from muons originating from cosmic rays, cosmic runs were performed at ALICE; this data, along with energy loss and transition radiation measurements from test-beams at the proton synchrotron (CERN PS) in 2004, and proton-proton (pp-) collisions performed at $\sqrt{s}$ = 7 TeV at ALICE, provides reference distribution data used for particle identification in the ALICE TRD (see Figure 22 for a plot showing the most probable signal dependence on $\beta$$\gamma$ \eqref{GrindEQ__44_}).

\noindent 

\noindent \includegraphics*[width=5.87in, height=4.16in, keepaspectratio=false]{image26}

\noindent \textbf{Figure 21: Reference distributions for most probable signal dependence on $\beta$$\gamma$ in the TRD, i.e. from measurements taken in pp-runs, test beams and measurements from cosmic rays \eqref{GrindEQ__44_}.}

\noindent 

\noindent 
\subparagraph{Methods used in Particle Identification}

\noindent 

\noindent Currently, the following methods are employed for particle identification based on TRD data:

\noindent 

\begin{enumerate}
\item  Truncated mean of the signal

\item  One- and two-dimensional likelihood estimations

\item  Neural Networks
\end{enumerate}

\noindent 

\noindent 
{\bf Truncated Mean}

\noindent 

\noindent The truncated mean signal is the combined signal of Transition Radiation + Specific Ionization Energy; this method focusses on classifying electrons vs pions based on their expected energy loss as per the Bethe Bloch curve shown in Figure 23.

\noindent 

\noindent \includegraphics*[width=4.51in, height=3.23in, keepaspectratio=false]{image27}

\noindent \textbf{Figure 22: Truncated mean signal (dE/dx + TR) for various charged particles as measured for p-Pb collisions at $\sqrt{\boldsymbol{\mathrm{5}}.\boldsymbol{\mathrm{02}}}$ $\boldsymbol{\mathrm{TeV}}$. This method allows for particle identification of light particles and hadrons \eqref{GrindEQ__44_}.}

\noindent 

\noindent 
{\bf One-dimensional Likelihood (LQ1D)}

\noindent 

\noindent One dimensional likelihood estimation is performed on the total integrated charge left by a particle in a single chamber in the TRD (i.e. a single tracklet). Figure 24 shows that electrons have on average a higher charge deposit, because they experience higher characteristic energy loss in this momentum range, as well as the fact that they emit Transition Radiation and pions don't \eqref{GrindEQ__44_}.

\noindent 

\noindent \includegraphics*[width=3.42in, height=2.49in, keepaspectratio=false]{image28}

\noindent \textbf{Figure 23: Normalised distribution of charge deposition for electrons and pions in a single TRD chamber \eqref{GrindEQ__44_}.}

\noindent 

\noindent The reference distributions allow maximum likelihood estimations to be carried out on each particle traversing the TRD, i.e. the likelihood of it being a muon, pion, kaon or an electron. Pions are rejected based on momentum-dependent cuts based on the likelihood for electrons, taking into account an electron efficiency score calculated using a clean reference sample of electrons arising from photon conversion \eqref{GrindEQ__44_}.

\noindent 

\noindent 
{\bf Two-dimensional Likelihood (LQ2D)}

\noindent 

\noindent Two-dimensional likelihood takes the temporal evolution of the signal (Figure 21) into account by splitting the signal into two time-bins and summing the charge in each bin and calculating the likelihood based on pure pion- and electron samples from collision data \eqref{GrindEQ__44_}.

\noindent 

\noindent 
{\bf Neural Networks}

\noindent 

\noindent A neural network was trained using a similar approach as LQ2D, but instead of splitting and summing over two time-bins, the input feature-set to the neural network was obtained by splitting into seven time-bins and summing the charge over each bin, respectively \eqref{GrindEQ__44_}.

\noindent 

\noindent 
\subparagraph{Particle Identification Accuracy}

\noindent 

\noindent To calculate the accuracy of the abovementioned methods, clean reference samples were used. The separating power of these approaches are often expressed as pion efficiency (the fraction of pions incorrectly classified as electrons, i.e. the false positive rate or fallout rate) at a specific electron efficiency (the fraction of electrons correctly identified, i.e. the true positive rate or sensitivity) \eqref{GrindEQ__44_}. 

\noindent 

\noindent Figure 25 shows the obtained pion efficiency for the methods discussed above, as a function of electron efficiency, it is clear from this plot that the misidentification of pions as electrons (False Positive Rate) is reduced substantially by the LQ2D and Neural Network techniques, compared to truncated mean- and LQ1D methods, and that the temporal evolution of the signal is therefore a highly informative feature for particle identification \eqref{GrindEQ__44_}.

\noindent 

\noindent It is important to note that pion suppression (the inverse of pion efficiency) is hampered when a particle passes through fewer than the available six layers of the TRD, and that electron efficiency is sometimes sacrificed during analysis to obtain a more pure sample \eqref{GrindEQ__44_}.

\noindent 

\noindent Figure 26 shows how pion efficiency depends on momentum for the four methods under discussion, data is plotted for samples where electron efficiency of 90\% is obtained. LQ1D and LQ2D are quite accurate at low momenta where the emission of transition radiation commences, but their separating power decreases at higher momenta as transition radiation production saturates and pions deposit more energy, making it harder to tell them apart. The truncated mean method performs poorly at high momenta, since transition radiation with its attendant high charge deposition is more likely to be removed during the truncation procedure \eqref{GrindEQ__44_}.

\noindent 

\noindent \includegraphics*[width=3.84in, height=3.53in, keepaspectratio=false]{image29}

\noindent \textbf{Figure 24: Pion efficiency as a function of electron efficiency for the various particle identification methods discussed \eqref{GrindEQ__44_}.}

\noindent \includegraphics*[width=3.45in, height=3.24in, keepaspectratio=false]{image30}

\noindent \textbf{Figure 25: Momentum dependence of pion efficiency for various methods (where electron efficiency is at 90\%)}

\noindent \eject 


\section{ Deep Learning}


\subsection{ \eject Deep Learning within the Context of Artificial Intelligence and Machine Learning}

\noindent 

\noindent Artificial Intelligence (AI) is a branch of Computer Science concerned with getting computers to perform tasks that are characteristic of those performed by the human mind. The field of AI encompasses both hard-coded rule-based programs (known as the knowledge base approach to AI, which has largely remained ineffective), as well as Machine Learning, which is an approach to AI which aims to get computers to perform these tasks without explicitly coding the solutions for them \eqref{GrindEQ__45_}.

\noindent 

\noindent The success of Machine Learning algorithms is largely determined by the representation of the data fed through them. Often, a large amount of an AI practitioner's time is dedicated to engineering the right feature-set to hand to a simple machine learning algorithm \eqref{GrindEQ__45_}.

\noindent 

\noindent In the case of machine learning for image classification, which loosely ties back to some of the aims in this project, it is not always immediately obvious as to which features will be informative to an ML algorithm. For example, feeding raw pixel values into a linear regression model should not be very effective, since images vary in terms of positional information, lighting, sharpness, rotation, etc. \eqref{GrindEQ__45_}

\noindent 

\noindent Representation learning is a solution to feature generation in which ML is applied, not only to map from a feature set to an output, but also towards automatically learning the most useful representation of the data; usually this representation will encompass identifying the major factors of variation which effectively explain the observed data and discarding those which are not useful to the algorithm \eqref{GrindEQ__45_}.

\noindent 

\noindent Deep Learning is an approach to representation learning which constructs useful representations based on a combination of simpler representations. In fact, the basic unit of a neural network is the perceptron, which in itself is a very simple function, but once compiled into a Multi-layer Perceptron, the rich texture of the input data distribution can be very accurately captured, since useful features discovered in the first layers of such a neural network can be combined in various ways to create additional useful features \eqref{GrindEQ__45_}. Continuing with the image classification example, an early layer of a convolutional neural network may detect edges in an image, the next layer may detect corners and shadows, and layers further down will ideally detect actual visual elements (faces, car lights, arms, etc.) \eqref{GrindEQ__45_}.


\subsection{ Mathematical Background for Deep Learning}


\paragraph{ Rosenblatt's Perceptron}

\noindent 

\noindent The original Rosenblatt paper \eqref{GrindEQ__46_} outlining the concept of the ``perceptron'' aimed to develop a theory to explain: 1. How sensory information is detected by biological organisms, 2. how that information is subsequently processed and stored and 3. how mental comprehension or organismal behaviour (which he termed ``\textit{preference for a particular response}'') was driven by the first two processes.

\noindent 

\noindent He outlined a mathematical framework for these mechanisms, at the hand of the following constructs:

\noindent 1. \textbf{S-points:} sensory units which can possess any of a number of response curves based on the signal strength of incoming information

\noindent 2. \textbf{A-units: }association cells located in an ``association area'' $A_{II}$, which in some of his models was preceded by a ``projection area'' $A_I$

\noindent 3. S-points are connected in specific ways to A-units and forward their stimulus response to them, in the form of an inhibitory or an excitatory impulse 

\noindent 4. $\boldsymbol{\theta }$: A threshold value assigned to each A-unit dictates whether it will fire, based on the algebraic sum of excitatory and inhibitory signals received, from either S-points or preceding A-units

\noindent 5. The connections between S-points and A-units, and between A-units themselves is random, and not all elements of such a network are connected to each other

\noindent 6. Response units, $R_1,R_2,\dots ,R_n$, receive a large number of inputs from the $A_{II}$ set, called its source-set, and have feedback mechanisms to A-units in its source set. \eqref{GrindEQ__46_}

\noindent 

\noindent He put forth various models for response curve summation and how these networks would learn \eqref{GrindEQ__46_}, but while the mathematical constructs he proposed were oversimplifications of the complexity of biological brains, they were found to be extremely useful in training computers to emulate their capabilities.


\paragraph{ Deep Feedforward Neural Networks}

\noindent 

\noindent At its most basic level, an artificial neural network (ANN) is an approximation of a mapping function $f_a$, which maps from a set of input features $x_i\ ;\ i=\{1,2,\dots ,n\}\ $to a response, $y$. Feedforward neural networks have one-way information flow from input features to output, whereas recurrent neural networks have feedback connections \eqref{GrindEQ__45_}.

\noindent 

\noindent Also called multilayer perceptrons (MLPs), deep feedforward networks are composed of an arbitrary number of nested approximating mapping functions, of the form:

\noindent 
\[f\left(x_{i,..,n}\right)\mathrm{=\ }f^m_a\mathrm{(}f^{\mathrm{\dots }}_a\mathrm{(}f^{\mathrm{2}}_a\mathrm{(}f^{\mathrm{1}}_a\mathrm{(}x_{i\mathrm{,\dots ,}n}\mathrm{))))\ }\] 


\noindent The superscript of these functions, $f^.$, indicates the layer index of the function in an ANN, with $m$ indicating the depth of such a neural network. It is this concept of chained functions of arbitrary depth from which the term Deep Learning is derived \eqref{GrindEQ__46_}.

\noindent 

\noindent The process of training such a network, $f$, to give the closest approximation to the desired output, $y$, is an iterative process, involving passing many observations, each having the same feature set $x_{i,\dots ,n}$ through the MLP, assessing the output, $y\textrm{^}$, according to an error metric, $E$, and individually adjusting each of the mapping functions $f^{j,\dots ,m}_a$ according to their contribution to the differential of the magnitude of error at the conclusion of each training step $k$. In other words, a parameter set $\theta $, pertaining to each $f^j_a$ is iteratively adjusted according to ${{\frac{\partial E_k}{\partial f^j_a}}}$. \eqref{GrindEQ__45_}.

\noindent 

\noindent The set of nested approximation functions outlined above are commonly referred to as hidden layers, the dimensionality of the outputs of each layer is known as its width, or as the number of neurons in that particular hidden layer \eqref{GrindEQ__45_}.

\noindent 

\noindent In order to produce subtle derived features from the input feature set, nonlinear transformations are applied to the output of each layer in the network, which in itself is a simple linear function of the form $w^Tx\mathrm{+}b\mathrm{\ }$, where $w^T$ is a vector of weights of the same length as the set of input features, which are essentially a set of coefficients for each $f_a$ in the chain of functions, and $b$ is a real-valued bias term, which is essentially an intercept term for each $f_a$$\includegraphics*[width=0.06in, height=0.16in, keepaspectratio=false]{image31}\ CITATION\ Goo16\ \backslash l\ 1033\ \ \eqref{GrindEQ__45_}$.

\noindent 

\noindent It is easy to see that chaining such a set of linear models without applying nonlinear transformations (denoted as $\phi (f_a(x))$) to what are essentially an arbitrary number of linear regression functions ($y=\ {\beta }_1x_1+\ {\beta }_1x_1+\ {\beta }_1x_1+c$), one would simply arrive at another linear model \eqref{GrindEQ__45_}.

\noindent 

\noindent Non-linear transformations applied over $w^Tx\mathrm{+}b$ allow deep learning models to more accurately model the multidimensional feature space of the data distribution.

\noindent 

\noindent A commonly used nonlinear transformation $\phi $, or activation function, in modern deep learning algorithms is the rectified linear unit (the ReLU function), which is simply an affine transformation, reminiscent of the response curves envisioned in Rosenblatt's paper, of the form $\phi \left(f_a(x)\right)=\mathrm{max}\mathrm{}\{0,f_a(x)\}$ \eqref{GrindEQ__45_}.

\noindent 

\noindent Various other nonlinear transformations (more commonly known as activation functions) can be viewed on the Keras website at \eqref{GrindEQ__47_}.

\noindent 

\noindent Combining the concepts explained above, then gives us a representation for a single hidden layer in an ANN as follows:

\noindent 
\[h\mathrm{=}\phi \mathrm{(}W^Tx\mathrm{+}b\mathrm{)}\] 


\noindent And, by extension, for a neural network with three hidden layers:

\noindent 
\[h^{\mathrm{(1)}}\mathrm{=}{\phi }^{\left(\mathrm{1}\right)}\mathrm{(}W^{\left(\mathrm{1}\right)T}x\mathrm{+}b^{\left(\mathrm{1}\right)}\mathrm{)}\] 

\[h^{\mathrm{(2)}}\mathrm{=}{\phi }^{\left(\mathrm{2}\right)}\mathrm{(}W^{\left(\mathrm{2}\right)T}h^{\mathrm{(1)}}\mathrm{+}b^{\left(\mathrm{2}\right)}\mathrm{)}\] 

\[h^{\mathrm{(3)}}\mathrm{=}{\phi }^{\left(\mathrm{3}\right)}\mathrm{(}W^{\left(\mathrm{3}\right)T}h^{\mathrm{(2)}}\mathrm{+}b^{\left(\mathrm{3}\right)}\mathrm{)}\] 


\noindent We now have a vector of weights multiplied by a vector of input features, which can be the original features fed to $h_1$, or the weighted outputs of previous hidden units in $h_{2,\dots ,n}$. Since we essentially have a vector of hidden units, we also have a vector of bias terms, and all of these hyperparameters, collectively referred to as $\theta $, need to be optimized to arrive at a reasonable approximation of a theoretically optimal mapping function $f^*\left(x\right)=y$$\includegraphics*[width=0.06in, height=0.16in, keepaspectratio=false]{image32}\ CITATION\ Goo16\ \backslash l\ 1033\ \ \eqref{GrindEQ__45_}$.

\noindent 

\noindent To achieve the optimization of $\theta $, most deep learning models utilize the concept of maximum likelihood, to minimize a loss function $J(\theta )$, for example, binary cross entropy:

\noindent 
\[J\left(\theta \right)\mathrm{=\ -}\left(y{\mathrm{log} \left(p\right)\ }\right)\mathrm{-(1-}{\mathrm{log} \left(p\right)\ }\mathrm{)},\] 


\noindent where $p$ is the model's estimate for the probability of an observation of being of a particular class $y$$\includegraphics*[width=0.06in, height=0.16in, keepaspectratio=false]{image33}\ CITATION\ Goo16\ \backslash l\ 1033\ \ \eqref{GrindEQ__45_}$.

\noindent 

\noindent Figure 27 shows how, as $p_{model}(y|x)$ approaches the true $y$ (in this binary classification example, $y=1$), the binary cross entropy loss function approaches 0.

\noindent 

\noindent \includegraphics*[width=4.10in, height=3.16in, keepaspectratio=false]{image34}

\noindent \textbf{Figure 26: Illustration of the descent towards zero, of the Binary Cross Entropy Loss Function as \^{y}, or ${\boldsymbol{p}}_{\boldsymbol{model}}\boldsymbol{(}\boldsymbol{y}\boldsymbol{|}\boldsymbol{x}\boldsymbol{)}$, approaches the true y.}

\noindent 

\noindent The chain rule of calculus is employed by backpropagation to enable the derivative of the loss function to be redistributed through the network, based on the partial derivative of each hyperparameter with respect to the derivative of the loss function \eqref{GrindEQ__45_}:

\noindent 
\[g\mathrm{\ }\mathrm{\leftarrow }\mathrm{\ }{\mathrm{\nabla }}_{\mathrm{\textrm{\^{y}}}}\mathrm{\ }J\mathrm{=\ }{\mathrm{\nabla }}_{\mathrm{\textrm{\^{y}}}}\mathrm{\ }L\mathrm{(}\mathrm{\textrm{\^{y}}}\mathrm{,y}\mathrm{)}\] 


\noindent For k = $l,\ \ l-1,\ \dots ,\ 1$ hidden layers, $h^{(l)},h^{(l-1)},\dots ,\ h^1$, we compute the element-wise gradient on the layer's output (before the non-linear activation function is applied):

\noindent 
\[g\mathrm{\ }\mathrm{\leftarrow }\mathrm{\ }{\mathrm{\nabla }}_{a^{\left(k\right)}}J\mathrm{=}g\mathrm{\odot }f^{\mathrm{'}}\left(a^{\left(k\right)}\right)\] 


\noindent And the gradients on the weights and the bias term:

\noindent 
\[{\mathrm{\nabla }}_{W^{\mathrm{(}k\mathrm{)}}}J\mathrm{\ =\ }g\mathrm{\ }h^{\mathrm{(}k\mathrm{-}\mathrm{1)}T}\mathrm{+}\lambda {\mathrm{\nabla }}_{W^{\left(k\right)}}\mathrm{\Omega }\mathrm{(}\theta \mathrm{)}\] 

\[{\mathrm{\nabla }}_{b^{\mathrm{(}k\mathrm{)}}}J\mathrm{=}g\mathrm{+}\lambda {\mathrm{\nabla }}_{b^{\left(k\right)}}\mathrm{\Omega }\mathrm{(}\theta \mathrm{)}\] 


\noindent Here, $\lambda $ represents the weight decay penalty, where the size of the weights are constrained, in a manner inversely proportional to $\lambda $ . A regularizer $\mathrm{\Omega }(\theta )$ is added to the loss, where $\theta $ contains all the weight and bias parameters.

\noindent 

\noindent This gradient is then propagated to the activations of the preceding layer:

\noindent 
\[g\mathrm{\leftarrow }\mathrm{\ }{\mathrm{\nabla }}_{h^{\mathrm{(}k\mathrm{-1)}}}J\mathrm{=}W^{\mathrm{(}k\mathrm{)}T}g\] 


\noindent In this manner all weights and biases in the ANN are repeatedly adjusted, proportionately to their contribution to the loss function at that iteration, until a (hopefully global) minimum is achieved \eqref{GrindEQ__45_}.

\noindent 


\paragraph{ Regularization and Optimization for Deep Learning}

\noindent 

\noindent 
\subparagraph{Regularization}

\noindent 

\noindent Regularization strategies are often employed in Deep Learning to reduce test error; by potentially sacrificing accuracy on training set predictions; effective regularization reduces overfitting of the model to features only present in the training data, and therefore increases accuracy on unseen data \eqref{GrindEQ__45_}.

\noindent 

\noindent Regularization strategies can be achieved by, for example, constraining parameter values by adding penalty terms to an objective function or by explicitly constraining parameters. Carefully designed regularization processes can improve performance on test data by encoding prior domain knowledge, making an undetermined problem determined, or by simplifying the model so that it generalizes better \eqref{GrindEQ__45_}.

\noindent 

\noindent Regularization in deep learning models often involves limiting the capacity (the hypothesis space) of an ANN by introducing a parameter norm penalty $\mathrm{\Omega }(\theta )$ to the loss function J. The loss function regularized in this fashion is denoted by J~, as follows:

\noindent 
\[\tilde{J}\left(\theta ,X,y\right)\mathrm{=}J\mathrm{(}\left(\theta ,X,y\right)\mathrm{+}\alpha \mathrm{\Omega }\mathrm{(}\theta \mathrm{)}\] 


\noindent Where $\alpha$ is a weighting hyperparameter, determining the extent of contribution of the parameter norm penalty to the magnitude of the regularized loss function J~, i.e. setting $\alpha$ = 0 eliminates regularization and increasing its value results in more regularization \eqref{GrindEQ__45_}.

\noindent 

\noindent Various norms $\Omega$ can be used in such a setup and can be applied to the entire set of network parameters $\theta$ or a specific subset, e.g. all the weights can be regularized, but all the bias terms can be set to escape regularization, because weights encode the interaction between two variables under a variety of circumstances, whereas bias terms only affect the output of one variable \eqref{GrindEQ__45_}.

\noindent 

\noindent Ideally, each ANN layer should have its own $\alpha$ coefficient, but doing so increases the search space for the optimal value, so a global $\alpha$ is sometimes used in practice \eqref{GrindEQ__45_}.

\noindent 

\noindent 
{\bf A Note on Norms}

\noindent 

\noindent Norms are a means of measuring the size of a vector, by mapping them to non-negative values, by satisfying the following properties:

\noindent 

\begin{enumerate}
\item  $f\left(x\right)\mathrm{=0\ }\mathrm{\Longrightarrow }x\mathrm{=0}$

\item  $f\left(x\mathrm{+}y\right)\mathrm{\le }f\left(x\right)\mathrm{+}f\mathrm{(}y\mathrm{)}$

\item  $\mathrm{\forall }\alpha \mathrm{\in }\mathbb{R}\mathrm{,}\mathrm{\ }\mathrm{\ }f\left(\alpha x\right)\mathrm{=|}\alpha \mathrm{|}f\mathrm{(}x\mathrm{)}$
\end{enumerate}

\noindent 

\noindent In general, the $L^{\mathrm{p}}$ norm is specified by:

\noindent 
\[{\left|\left|x\right|\right|}_p\mathrm{=}{\left(\sum_i{{\mathrm{|}x_i\mathrm{|}}^p}\right)}^{\frac{\mathrm{1}}{p}}\] 


\noindent 
{\bf ${\boldsymbol{L}}^{\boldsymbol{2}}\boldsymbol{:}$ Weight Decay Regularization}

\noindent 

\noindent The $L^{\mathrm{2}}$ parameter norm is a simple regularization strategy which shrinks the weights of an ANN closer to the origin by adding the squared and weighted parameter norm penalty 
\[\mathrm{\Omega }\left(\theta \right)\mathrm{=}{\mathrm{|}\left|w\right|\mathrm{|}}^{\mathrm{2}}_{\mathrm{2}}\mathrm{=\ }\frac{}{\mathrm{2}}w^Tw\] 


\noindent to the objective function \eqref{GrindEQ__45_}.

\noindent 

\noindent The $L^{\mathrm{2}}$ norm is known as the Euclidean norm, because it gives the magnitude of the Euclidean distance from the origin to the point defined by $x$. It is squared in this regularization technique for computational efficiency, because calculating the derivative with respect to each component of the unsquared $L^{\mathrm{2}}$ norm involves all its elements, whereas the derivative for each component of the squared $L^{\mathrm{2}}$ norm depends only on the corresponding element of $x$ \eqref{GrindEQ__45_}.

\noindent 

\noindent Figure 28 (RHS) illustrates the manner in which introducing an $L^{\mathrm{2}}$ norm penalty introduces an additional constraint on the objective function, i.e. having to minimize the magnitude of the $L^{\mathrm{2}}$ norm in addition to minimizing the loss function causes the weights to be shrunk, since this larger regularized loss function is interpreted as having higher variance \eqref{GrindEQ__45_}.

\noindent 

\noindent 

\noindent \includegraphics*[width=4.97in, height=2.25in, keepaspectratio=false]{image35}

\noindent \textbf{Figure 27: ${\boldsymbol{L}}^{\boldsymbol{\mathrm{1}}}$ and ${\boldsymbol{L}}^{\boldsymbol{\mathrm{2}}}$ norm 539826434penalties }539826434GVGerhard Viljoen539826434-1485618138Need to find a better image and explanation\textbf{}

\noindent 

\noindent 
{\bf ${\boldsymbol{L}}^{\boldsymbol{1}}$ Regularization}

\noindent 

\noindent 

\noindent $L^{\mathrm{1}}$ regularization adds a slightly different weighted parameter norm penalty 

\noindent 
\[\mathrm{\Omega }\left(\theta \right)\mathrm{=\ }{\mathrm{|}\left|w\right|\mathrm{|}}_{\mathrm{1}}\mathrm{=\ }\mathrm{\ }\sum_i{\mathrm{|}w_i\mathrm{|}}\mathrm{\ }\] 


\noindent to the objective function.

\noindent 

\noindent When $L^{\mathrm{1}}$ regularization is used, as the sum of the absolute values of the weights of the ANN increases, the loss function will also increase, as it does for $L^{\mathrm{2}}$ regularization; but in contrast to $L^{\mathrm{2}}$ regularization, $L^{\mathrm{1}}$ allows weights to be shrunk down to zero, resulting in a more sparse neural network, depending on the magnitude of the weighting parameter . This phenomenon allows for better feature selection, by reducing the amount of connections in the network and therefore removing the influence of some features on its output \eqref{GrindEQ__45_}.

\noindent 

\noindent When using either $L^{\mathrm{1}}$ or $L^{\mathrm{2}}\ $regularization, care has to be taken to select the right level for , since a large  could result in the backpropagation algorithm getting trapped in a local minimum or where the weights are shrunk by so much that they can't impart any useful information to the next layer \eqref{GrindEQ__45_}.

\noindent 

\noindent 
{\bf Early Stopping}

\noindent 

\noindent By saving the parameter setting at the conclusion of each epoch during training, one can return the network to the parameter setting where the validation error was at its lowest (the point at which the network started overfitting to the training set) \eqref{GrindEQ__45_}.

\noindent 

\noindent One can also prevent a model from passing that point by specifying early stopping criteria, which will kick the neural network out of training when a defined minimum improvement on the validation error has not occurred for a defined number of epochs \eqref{GrindEQ__45_}.

\noindent 

\noindent Computational efficiency is maintained by checking the abovementioned conditions at specified training intervals, i.e. not checking whether early stopping criteria have been met after each epoch. Storing parameter settings can be made more efficient by saving in a slower form of memory, such as hard disk space to keep available random-access memory or GPU memory space sufficient for model training \eqref{GrindEQ__45_}.

\noindent 

\noindent Once early stopping has been reached, the checkpointed model can be trained further by adding the previously held out validation data to the training data and monitoring the objective function as a guide for when to interrupt training \eqref{GrindEQ__45_}.

\noindent 

\noindent Alternatively, once early stopping criteria are reached, one can retrain a completely new neural network, with the same hyperparameters as the stopped network, for the number of epochs it ran, but this time using the full training + validation data for training \eqref{GrindEQ__45_}.

\noindent 

\noindent Early stopping is often used in conjunction with other regularization techniques, since it is unobtrusive towards the learning dynamics, i.e. it does not change how the neural network arrives at its optimal weights, it simply changes when to stop adjusting them to prevent overfitting \eqref{GrindEQ__45_}.

\noindent 

\noindent 
{\bf Ensembled Models}

\noindent 

\noindent Bagging and other model averaging techniques involve training a multitude of models and allowing each of them to vote towards the outcome, making use of the principle that a number of Deep Learning models which have each been set up differently should not all make the same ``cognitive errors'' when learning useful representations to inform accurate predictions on the test set \eqref{GrindEQ__45_}.

\noindent 

\noindent Bagging, in particular, requires construction of multiple training datasets by sampling with replacement from the full training dataset, resulting in around a third of the full training observations not being present in each of the resampled training sets, and different observations being missing in each \eqref{GrindEQ__45_}.

\noindent 

\noindent Since random weight initialization and random minibatch selection can result in slightly different weight parameterisation, even when the same architecture is trained multiple times on the same dataset, model averaging is a highly reliable way to reduce overfitting \eqref{GrindEQ__45_}.

\noindent 

\noindent Boosting is an alternative approach to ensembled methods, which actually increases the capacity of the ensemble by learning based on the variance of previous neural networks by adding additional neural networks sequentially, or even by incrementally introducing hidden units to a single ANN \eqref{GrindEQ__45_}.

\noindent 

\noindent 
{\bf Dropout}

\noindent 

\noindent The computational cost of training and evaluating an ensemble of more than 10 neural networks can become impractical in terms of memory and runtime constraints. Dropout is a computationally inexpensive alternative regularization method, which achieves a similar outcome \eqref{GrindEQ__45_}.

\noindent 

\noindent Dropout regularization consists of training the entire ensemble of subnetworks which can be achieved by setting the output of a subset of hidden units to zero, thus approximating model averaging methods \eqref{GrindEQ__45_}.

\noindent 

\noindent Practically, dropout is achieved by a combination of mini-batch training and binary mask generation during each minibatch training round. The binary mask is of the same dimensions as the input- and hidden- units and each element in the mask is multiplied by its corresponding neuron, effectively pruning the neural network by setting the output of a random subset of neurons to zero \eqref{GrindEQ__45_}.

\noindent 

\noindent The probability of sampling a 1 at each unit of the mask is a hyperparameter set before training. Each unit in the mask is sampled independently \eqref{GrindEQ__45_}.

\noindent 

\noindent 
\subparagraph{Optimization}

\noindent 

\noindent The essential optimization objective in deep learning is to find the optimal set of hyperparameters $\theta $ to minimize the objective function $J(\theta )$ \eqref{GrindEQ__45_}.

\noindent 

\noindent Adaptive learning rates, utilization of the second derivative of the loss function during training and various parameter initialization- and other advanced strategies can be employed to make the training/ optimization process more effective \eqref{GrindEQ__45_}.

\noindent 

\noindent 
{\bf Batch and Minibatch}

\noindent 

\noindent The process of minimizing the objective function \textbf{\textit{J}}, can be made more efficient by sampling a ``minibatch'' of training examples at each iteration, this process also compensates for redundancy in the training data, where many observations are effectively contributing the same information regarding the gradient of the loss function \eqref{GrindEQ__45_}.

\noindent 

\noindent Using smaller batches can also prevent overfitting; this regularizing feature is optimal when using a batch size of one with a very small learning rate to maintain stability because the gradient will have high variance in this case, but the combination of a slow learning rate and high number of iterations for each epoch can result in very long compute time during training \eqref{GrindEQ__45_}.

\noindent 

\noindent 
{\bf Stochastic Gradient Descent}

\noindent 

\noindent Stochastic gradient descent (SGD) is a commonly used optimization algorithm that makes use of the average gradient of the loss function over a minibatch of training examples as an unbiased estimate of the true gradient; along with a learning rate $\epsilon $, which decreases over time to compensate for noise in the gradient introduced by stochasticity of the process. The learning rate at iteration \textit{i }is denoted as ${\epsilon }_i$. The learning rate is often set to decay linearly until iteration $\tauup$, i.e.

\noindent 
\[{\epsilon }_i\mathrm{=}\left(\mathrm{1-}\alpha \right){\epsilon }_0\mathrm{+}\alpha {\epsilon }_{\tau }\] 
where $\alpha =\frac{i}{\tau }$. After iteration $\tau $, $\epsilon $ is commonly leaved constant \eqref{GrindEQ__45_}.

\noindent 

\noindent 
{\bf Momentum}

\noindent 

\noindent Momentum, in the context of deep learning, is a method which results in accelerated learning compared to SGD, by taking an exponentially decaying moving average of past gradients into account when updating weights during backpropagation. A weighting hyperparameter $\alpha \ \epsilon \ [$$0,1)$ , determines the rate of decay of previous gradients in determining this so-called momentum \eqref{GrindEQ__45_}.

\noindent 

\noindent A simplified representation of the SGD algorithm with momentum looks as follows:

\noindent 

\noindent Given:

\begin{enumerate}
\item  Learning rate $\boldsymbol{\mathrm{\epsilonup }}$

\item  Momentum decay parameter\textbf{ }$\boldsymbol{\mathrm{\alphaup }}$

\item  Initial velocity\textit{ }\textbf{\textit{v}}

\item  Initial parameter to be updated $\boldsymbol{\theta }$
\end{enumerate}

\noindent 

\noindent While stopping criteria is unmet, DO:

\begin{enumerate}
\item  Sample minibatch of size \textit{m} from training data

\item  Compute gradient:
\[g\mathrm{\ }\mathrm{\leftarrow }\mathrm{\ }\frac{\mathrm{1}}{m}{\mathrm{\nabla }}_{\theta }\sum_i{L\left(f\left(x^{\left(i\right)}\mathrm{;}\theta \right),y^{\left(i\right)}\right)}\] 

\item  Update velocity:
\[v\mathrm{\ }\mathrm{\leftarrow }\mathrm{\ }\alpha v\mathrm{-}\epsilon g\] 

\item  Update parameter: 
\[\theta \mathrm{\ }\mathrm{\leftarrow }\mathrm{\ }\theta \mathrm{+}v\] 
\end{enumerate}


\noindent Commonly used values of $\alpha $ are 0.5, 0.9 and 0.99, and similarly to the learning rate, $\alpha $ can also be adapted over time \eqref{GrindEQ__45_}.

\noindent 
{\bf Nesterov Momentum}

\noindent 

\noindent A momentum variant based on the accelerated gradient method proposed by Nesterov, Nesterov momentum updates parameters according to the following rules:
\[v\mathrm{\ }\mathrm{\leftarrow }\mathrm{\ }\alpha v\mathrm{-}\mathrm{\ }\epsilon {\mathrm{\nabla }}_{\theta }\left[\frac{\mathrm{1}}{m}L\mathrm{(}f\left(x^{\left(i\right)}\mathrm{;}\theta \mathrm{+}\alpha v\right)\mathrm{;}y^{\left(i\right)}\mathrm{)}\right]\] 

\[\theta \mathrm{\ }\mathrm{\leftarrow }\mathrm{\ }\theta \mathrm{+}v\] 


\noindent Nesterov momentum differs from standard momentum in that the gradient is evaluated after velocity is applied, whereas in standard momentum, the gradient is evaluated first, before velocity is calculated and applied, as can be seen in the following algorithm for Nesterov momentum, compared to that for standard momentum shown above \eqref{GrindEQ__45_}.

\noindent 

\noindent Given:

\begin{enumerate}
\item  Learning rate $\boldsymbol{\mathrm{\epsilonup }}$

\item  Momentum decay parameter\textbf{ }$\boldsymbol{\mathrm{\alphaup }}$

\item  Initial velocity\textit{ }\textbf{\textit{v}}

\item  Initial parameter to be updated $\boldsymbol{\theta }$
\end{enumerate}

\noindent 

\noindent While stopping criteria is unmet, DO:

\begin{enumerate}
\item  Sample minibatch of size \textit{m} from training data

\item  Apply interim update:
\[\widetilde{\theta \mathrm{\ }}\mathrm{\leftarrow }\theta \mathrm{+}\alpha v\] 

\item  Compute interim gradient:
\[g\mathrm{\ }\mathrm{\leftarrow }\mathrm{\ }\frac{\mathrm{1}}{m}{\mathrm{\nabla }}_{\widetilde{\theta }}\sum_i{L\mathrm{(}f\left(x^{\left(i\right)}\mathrm{;}\widetilde{\theta }\right),y^{\left(i\right)}\mathrm{)}}\] 

\item  Update velocity:
\[v\mathrm{\ }\mathrm{\leftarrow }\alpha v\mathrm{-}\epsilon g\] 

\item  Update parameter:
\[\theta \mathrm{\leftarrow }\theta \mathrm{+}v\] 
\end{enumerate}


\noindent 
{\bf Adaptive Learning Rates}

\noindent 

\noindent Since the learning rate is a very important hyperparameter and difficult to set; a variety of algorithms have been developed by the deep learning community that dynamically modify the learning rate as training progresses.

\noindent 

\noindent 
{\it AdaGrad}

\noindent 

\noindent The AdaGrad algorithm adapts each one of a model's parameters by scaling them inversely proportional to the square root of the summed historical square values of their individual gradients. In doing so, AdaGrad ensures that parameters that have a greater influence on the objective function (i.e. those that contribute a larger partial derivative to the objective function) have a rapidly shrinking learning rate $\alphaup$, whereas those with smaller partial derivatives of the objective function decrease their learning rate much more slowly \eqref{GrindEQ__45_}.

\noindent 

\noindent 
{\it RMSProp}

\noindent 

\noindent RMSProp is an adaptation of AdaGrad that uses an exponentially weighted moving average, therefore not taking into account all historical gradients but in essence forgetting gradients that fall outside of a specified length scale, $\rho $ \eqref{GrindEQ__45_}.

\noindent 

\noindent 
{\it Adam}

\noindent Originating as an acronym for ``adaptive moments'', the Adam algorithm is generally touted as an optimization strategy robust to various settings of hyperparameters. Adam combines features of momentum and RMSProp, by using momentum to estimate the first moment of the gradient and by applying bias corrections to both the first and second order moments of the gradient \eqref{GrindEQ__45_}.


\subsection{ Convolutional Neural Networks}


\paragraph{ The Kernel Concept and Motivation for CNNs}

\noindent 

\noindent Convolutional Neural Networks (CNNs) are an extension of deep learning models, highly successful in processing data with a grid-like topology, e.g. images. At least one linear mathematical operation, called a convolution, is applied in CNNs, usually in addition to the general matrix multiplication performed in traditional feedforward neural networks \eqref{GrindEQ__45_}.

\noindent 

\noindent An example of a simple 2D convolution (multiplying a 3$\times$4 matrix by a 2$\times$2 kernel) is shown below (adapted from \eqref{GrindEQ__45_}).

\noindent 

\noindent 
\[ \begin{array}{cc}
a & b \\ 
e & f \\ 
i & j \end{array}
\mathrm{\ \ \ \ } \begin{array}{cc}
c & d \\ 
g & h \\ 
k & l \end{array}
      *       \begin{array}{cc}
w & x \\ 
y & z \end{array}
\] 


\noindent =

\noindent 
\[ \begin{array}{ccc}
aw\mathrm{+}bx\mathrm{+}ey\mathrm{+}fz & bw\mathrm{+}cx\mathrm{+}fy\mathrm{+}gz & cw\mathrm{+}dx\mathrm{+}gy\mathrm{+}hz \\ 
\mathrm{\ } & \mathrm{\ } & \mathrm{\ } \\ 
\mathrm{\ }ew\mathrm{+}fx\mathrm{+}iy\mathrm{+}jz & \mathrm{\ }fw\mathrm{+}gx\mathrm{+}jy\mathrm{+}kz & gw\mathrm{+}hx\mathrm{+}ky\mathrm{+}lz\mathrm{\ } \end{array}
\] 


\noindent 

\noindent There are three major mechanisms that improve the accuracy of ML algorithms that motivate the implementation of convolutions in a deep learning architecture, namely parameter sharing, equivariant transformations and sparse interactions \eqref{GrindEQ__45_}. These will be discussed below.

\noindent 

\noindent Sparse interactions occur in CNNs because of kernels that are smaller than the input matrix, which means that every input unit does not have a connection to every output unit (as is the case in fully connected traditional ANNs), this sparsity of weights allows for the detection of meaningful small-scale features, such as edges, which are combined downstream (via indirect interactions of neurons in preceding layers) into progressively larger features, such as textures, shapes and actual visual elements, such as faces. Reducing the number of weights in this manner also leads to an increase in the efficiency of the neural network, since fewer operations are required per layer and fewer weights need to be stored and adjusted \eqref{GrindEQ__45_}.

\noindent 

\noindent Parameter sharing allow certain parameters to be used by more than one function in a CNN, unlike traditional neural networks, which use each weight in a neural network in just one operation when the network's output is calculated. In a CNN, each element of the kernel is multiplied by every element of the input matrix (where dimension differences do not allow for this, edges may be padded with zero-valued matrix elements to enable it). The weights of the kernel function are learnt and applied uniformly, i.e. they are not relearned at each position of the input matrix, again this has benefits with regards to computational efficiency \eqref{GrindEQ__45_}.

\noindent 

\noindent Equivariance to translation is a phenomenon which results from parameter sharing and means that the output of a convolutional layer changes in the same way that its input changes, i.e. $f(x)$ is said to be equivariant to a function $g$ if $g\left(f\left(x\right)\right)=f(g\left(x\right))$. In a convolution operation, the function $g$ translates (shifts) the input matrix in some way, but since the convolution operation is equivariant to the function $g$, it does not matter at which (x,y) coordinates a feature occurs in the input matrix, since it will still result in the same output after the convolution operation has been applied \eqref{GrindEQ__45_}.

\noindent 


\paragraph{ Pooling}

\noindent 

\noindent CNN layers are generally composed of three operations:

\noindent 

\begin{enumerate}
\item  The appropriate amount of convolution operations, as introduced above, are applied in parallel over the input matrix

\item  A non-linear activation function is applied to the output of each convolution operation performed in step one

\item  A pooling operation introduces an additional final modification to the layer output
\end{enumerate}

\noindent 

\noindent The pooling function in step 3 above, performs a statistical summary over a window of outputs within a defined range, which could be, for example, the ${\mathrm{L}}_{\mathrm{2}}$-norm, mean or maximum over the series of rectangular ranges thus defined \eqref{GrindEQ__45_}.

\noindent 

\noindent Pooling serves the purpose of insuring invariance to local translation, where the presence of a feature matters more than its location. In some cases, the specific orientation and location of a feature does matter though. Pooling over separate convolutions that are independently parameterized can allow the ANN to learn which translations it should be invariant to which translations it shouldn't be invariant to \eqref{GrindEQ__45_}.

\noindent 

\noindent Pooling with down-sampling is achieved by reducing the number of pooling operations relative to the number of detector units, by introducing a stride greater than one. See Figure 29 for an illustration of the effect of using different stride widths with the same pool-width. It is straightforward to see that down-sampling applied in this manner will lead to fewer inputs to process, reduced memory requirements and improved statistical efficiency \eqref{GrindEQ__45_}.

\noindent 

\noindent \includegraphics*[width=4.77in, height=3.03in, keepaspectratio=false]{image36}

\noindent \textbf{}

\noindent \textbf{Figure 28: An illustration of the concept of max pooling, using pool-width of 3 with a stride of one (top panel) vs a stride of two (bottom panel) \eqref{GrindEQ__48_}.}

\noindent 


\paragraph{ The Convolution Function}

\noindent 

\noindent In practice, data fed to a CNN usually consists of a grid of vectors, effectively adding a depth- (or channel-) dimension to the usual width- and height- dimensions of a grid. In deep learning packages such as Tensorflow, indices of values in such a tensor specify 4 locations, i.e. each value has a row-, column-, channel- and observation index \eqref{GrindEQ__45_}.

\noindent 

\noindent 
\subparagraph{The Convolution Operation}

\noindent 

\noindent Given an element of a 3-D input tensor ${\boldsymbol{\mathrm{V}}}_{i,j,k}$\textbf{ }i.e. a value in the \textit{i${}^{th}$} channel, \textit{j${}^{th}$} row and \textit{k${}^{th}$} column of a single training observation $\boldsymbol{\mathrm{V}}$, which is convolved by a 4-D kernel tensor \textbf{K} to generate an element in an output tensor specified by ${\boldsymbol{\mathrm{Z}}}_{i,j,k}$, then ${\boldsymbol{\mathrm{K}}}_{i,j,k,l}$ represents the strength of the connection between an element in channel \textit{i} of the output tensor (${\boldsymbol{\mathrm{Z}}}_{i,\ \ .,.}$) and an element in channel \textit{j} of the input tensor (${\boldsymbol{\mathrm{V}}}_{j,.,.}$), offset by \textit{k} rows and \textit{l} columns between the input and output element. Thus ${\boldsymbol{\mathrm{Z}}}_{i,j,k}$\textbf{ }is calculated as follows:

\[{\boldsymbol{\mathrm{Z}}}_{i,j,k}\mathrm{=\ }\sum_{l,m,n}{{\boldsymbol{\mathrm{V}}}_{l,j\mathrm{+}m,k\mathrm{+}n\mathrm{-1}}{\boldsymbol{\mathrm{K}}}_{i,l,m,n}}\] 


\noindent where the summation over the indices is for all valid indices in the tensor \eqref{GrindEQ__45_}.

\noindent 

\noindent For computational efficiency, downsampling of the convolution function can be implemented by skipping over some positions in the kernel, specified by a parameter called stride \eqref{GrindEQ__45_}.

\noindent 

\noindent Figure 30 illustrates how implementing a convolution with stride = 2, i.e. only sampling every second pixel for convolution, is mathematically equivalent to performing downampling after a convolution applied to all pixels (i.e. stride = 1), followed by downsampling \eqref{GrindEQ__45_}.

\noindent 

\noindent 

\noindent \includegraphics*[width=4.01in, height=3.91in, keepaspectratio=false]{image37}

\noindent \textbf{Figure 29: Illustration of mathematical equivalence of implementing a convolution with unit stride followed by downsampling to implementing a convolution with stride = 2.}

\noindent Zero-padding is often applied to the input vector in order to prevent it from shrinking by one pixel less than the applied kernel width, i.e. for an input image of width \textit{m} and kernel width \textit{k}, the output of the convolution with no zero-padding will be \textit{m-k+}1, a situation which would enforce smaller networks and smaller subsequent kernels if not accounted for, which in turn would limit the capacity of the network to find useful representations of the data \eqref{GrindEQ__45_}.

\noindent 

\noindent Convolutions applied with no padding of the input image are known as valid convolutions, where pixels in the output of a convolution are a function of the same amount of pixels in the input, and the kernel can only be applied to positions on the image where the kernel is contained by the image \eqref{GrindEQ__45_}.

\noindent 

\noindent When just enough zero-padding is applied to the input image to ensure that the output will be of the same dimensions, the convolution is known as a same convolution \eqref{GrindEQ__45_}. Although same convolutions do not limit the size of the network and allow one to build neural networks of arbitrary depth, they still result in pixels close to the edges of the image having less connections to the output image and therefore that their influence on the network as a whole will be reduced \eqref{GrindEQ__45_}.

\noindent 

\noindent Full convolutions result from applying enough zero-padding to allow each pixel to be visited \textit{k} times in each direction of the convolution operation, and therefore should result in an output with \textit{m+k-}1 pixels. This results in output pixels near the border being influenced by fewer pixels than output pixels near the centre, making the kernel harder to train \eqref{GrindEQ__45_}.

\noindent 

\noindent The ideal amount of padding generally lies between the amount of padding required to achieve valid- and same convolutions \eqref{GrindEQ__45_}.


\subsection{ Recurrent Neural Networks}

\noindent Recurrent neural networks (RNNs) are specifically designed to process sequential values. Parameter sharing is an essential aspect of RNNs and facilitate the detection of patterns that could potentially occur in more than one place in the sequence; this family of ANNs also accounts for sequences of differing length \eqref{GrindEQ__45_}.

\noindent 

\noindent Parameter sharing in RNNs manifest in the form that each element of the output is a function of previous elements of the output within a specified range and is updated using the same rule used to update previous elements of the output \eqref{GrindEQ__45_}.

\noindent 

\noindent The input vector to an RNN will be vectors $x^{(t)}$ with timestep \textit{t} consisting of a range from 1, {\dots}, $\tau $ \eqref{GrindEQ__45_}.

\noindent 

\noindent Recurrent neural networks extend the concept of a computational graph to include cyclical connections, where the present value of a variable is understood to have an influence on its future value \eqref{GrindEQ__45_}.

\noindent 


\paragraph{ Computational Graphs}

\noindent 

\noindent Computational graphs are visual depictions which formalize a set of operations applied to an input vector, for example the computational graph formalizing the ReLU activated output of a hidden unit, i.e. $H=\mathrm{max}\mathrm{}\{0,WX+b\}$, would look as follows:

\noindent 

\noindent \includegraphics*[width=2.61in, height=2.88in, keepaspectratio=false]{image38}

\noindent \textbf{Figure 30: ReLU activated hidden unit in a Neural Network depicted as a computational graph}

\noindent In RNNs, computational graphs manifest as repetitive chains of operations which result in parameter sharing across neural network architectures \eqref{GrindEQ__45_}.

\noindent 

\noindent As an example, a dynamical system is classically expressed as:

\noindent 
\[s^{\mathrm{(}t\mathrm{)}}\mathrm{=}f\mathrm{(}s^{\left(t\mathrm{-1}\right)}\mathrm{;}\theta \mathrm{)}\] 


\noindent Here, the state of the system at time t, $s^{(t)}$, explicitly depends on its state at the previous time step (\textit{t-}1).

\noindent 

\noindent This graph can be unfolded for a finite number of timesteps, $\tau $, by applying the above expression $\tau -1$ times, e.g. if $\tau $=3:

\noindent 
\[s^{\left(\mathrm{3}\right)}\mathrm{=}f\left(s^{\left(\mathrm{2}\right)}\mathrm{;}\theta \right)\] 
\[\mathrm{=}f\left({f\mathrm{(}s}^{\left(\mathrm{1}\right)}\mathrm{;}\theta \right)\mathrm{;}\theta \mathrm{)}\] 


\noindent The above equation can be represented as an acyclic graph, which does not make use of recurrence, as follows:

\noindent 

\noindent \includegraphics*[width=5.84in, height=0.84in, keepaspectratio=false]{image39}

\noindent \textbf{Figure 31: Acyclic computational graph of a dynamical system}

\noindent If we extend this to express the dynamical system's state at any point being informed by all the previous states of the system, the equation becomes:

\noindent 
\[s^{\mathrm{(}t\mathrm{)}}\mathrm{=}f\mathrm{(}s^{\left(t\mathrm{-1}\right)},x^{\left(t\right)}\mathrm{;}\theta \mathrm{)}\] 


\noindent This is the basic formula upon which RNNs are built, where the ``states'' of the system are the neural network's hidden units, i.e.

\noindent 
\[h^{\mathrm{(}t\mathrm{)}}\mathrm{=}f\mathrm{(}h^{\left(t\mathrm{-1}\right)},x^{\left(t\right)}\mathrm{;}\theta \mathrm{)}\] 


\noindent \eqref{GrindEQ__45_}.

\noindent 


\paragraph{ Long Short-Term Memory}

\noindent 

\noindent Long Short-Term Memory (LSTM) recurrent neural networks are a highly successful class of RNN which deals with the problem of exploding or vanishing gradients introduced by other RNN implementations by enforcing constant error flow through the internal states of special units, called memory cells, shown in a computational graph in Figure 33 \eqref{GrindEQ__49_}.

\noindent 

\noindent 

\noindent 

\noindent 

\noindent \includegraphics*[width=4.20in, height=4.65in, keepaspectratio=false]{image40}

\noindent \textbf{Figure 32: Graph-based representation of special LSTM units}

\noindent 

\noindent Multiplicative input and output gates protect other units from irrelevant inputs and currently irrelevant stored memory states, respectively. Each memory cell as shown above consists of a central linear unit with a self-connection which is fixed \eqref{GrindEQ__49_}. In this way, a memory cell can decide whether or not to save information about its current state, based on inputs from other memory cells.

\noindent 


\subsection{ Generative Models}

\noindent Generative models are concerned with modelling potentially high-dimensional distributions. Dependencies between various random variables in the multidimensional distribution can also be captured during this modelling process \eqref{GrindEQ__50_}.

\noindent 

\noindent Generative models are concerned with generating data that is similar to seen data, but not exactly the same, i.e. our training examples $X$ are distributed according to some unknown distribution $P_{gt}(X)$ and we want to model a distribution $P$ which is as similar as possible to $P_{gt}$ and therefore allows us to generate new examples $X$ by sampling from $P$ \eqref{GrindEQ__50_}.

\noindent 

\noindent Neural networks can be utilised as function approximators towards constructing a modelled distribution $P$ as outlined above \eqref{GrindEQ__50_}.

\noindent 


\paragraph{ Background: Latent Variable Models}

\noindent When there are complex dependencies between the dimensions of the data, generative models become very hard to train. Latent variables are samples drawn from specific latent distributions constructed during training, before the generative process commences, i.e. the model first chooses what it is going to simulate before it starts simulating \eqref{GrindEQ__50_}.

\noindent 

\noindent In order to deduce that a generative model is representative, one needs to find that for each datapoint $X$ in $\chi $, there are one or more latent variable settings which result in the model generating something sufficiently similar to $X$ \eqref{GrindEQ__50_}.

\noindent 

\noindent A vector of latent variables $z$, are sampled from a high dimensional latent space $Z$, according to a probability density function (p.d.f.): $P(z)$ defined over $Z$. A group of deterministic functions $f(z;\theta )$ parameterized by a vector $\theta $ in some space $\mathrm{\Theta }$, with $f:Z\times \mathrm{\Theta }\to \chi $. While $f$ is deterministic, $z$ is randomly sampled and $\theta $ is fixed, which makes $f(z;\theta )$ a random variable in the space $\chi $. $\theta $ needs to be optimized so that sampling $z$ from $P(z)$ will result in a high probability of $f(z;\theta )$ outputting data similar to the training data $X$ \eqref{GrindEQ__50_}.

\noindent 

\noindent More formally, we want to maximize the probability of each $X$, according to:
\[P\left(X\right)\mathrm{=}\int{P\left(X\mathrel{\left|\vphantom{X z\mathrm{;}\theta \mathrm{\ }}\right.\kern-\nulldelimiterspace}z\mathrm{;}\theta \mathrm{\ }\right)P\left(z\right)dz}\] 
$f(z;\theta )$ has been changed to a distribution $P\left(X\mathrel{\left|\vphantom{X z;\theta \mathrm{\ }}\right.\kern-\nulldelimiterspace}z;\theta \mathrm{\ }\right)$ in the expression above, in order to show explicitly that $X$ depends on $z$. Maximum Likelihood underpins the notion that if $X$ is likely to be reproduced, generated examples that are highly similar to $X$ are also likely to be produced, and dissimilar examples are unlikely \eqref{GrindEQ__50_}.

\noindent 

\noindent VAEs often model the output distribution as a Gaussian, $P\left(X\mathrel{\left|\vphantom{X z;\theta \mathrm{\ }}\right.\kern-\nulldelimiterspace}z;\theta \mathrm{\ }\right)=N(X|f\left(z;\theta \right),{\sigma }^2*I)$, i.e. the distribution has mean $f(z;\theta )$ and covariance equal to some scalar $\sigma $ multiplied by the identity matrix $I$, with $\sigma $ being a tuneable hyperparameter \eqref{GrindEQ__50_}.

\noindent 

\noindent A VAE will in general not produce examples identical to any $X$, especially not during early training, but under the Gaussian assumption, $P(X)$ can be increased via gradient descent by making $f(z;\theta )$ approach $X$ given some $z$ \eqref{GrindEQ__50_}.

\noindent 


\paragraph{ Variational Autoencoders}

\noindent Variational Autoencoders (VAEs) aim to maximize  $P\left(X\right)=\int{P\left(X\mathrel{\left|\vphantom{X z;\theta \mathrm{\ }}\right.\kern-\nulldelimiterspace}z;\theta \mathrm{\ }\right)P\left(z\right)dz}$ by defining latent variables $z$ and integrating over $z$. Choosing the latent variables $z$ are not trivial, since $z$ is generally not just defined by the label of the example that needs to be generated, but by other features specific to the example \eqref{GrindEQ__50_}, in our case, $z$ would not just be \textit{electron} or \textit{pion}, but additional dimensions such as the particle's momentum, angle, etc. Generally, a researcher would not explicitly specify what the dimensions of $z$ specify, nor how the dimensions of $z$ depend on one another \eqref{GrindEQ__50_}.

\noindent 

\noindent In VAEs, $z$ is drawn from a distribution $N(0,I)$, where I is the identity matrix, since any distribution in $d$ dimensions can be generated by sampling from $d$ normally distributed variables and mapping them through a function with high enough capacity to generate $X$. When $f(z;\theta )$ is a neural network then the initial layers will be involved in generating $z$ while the later layers will be concerned with mapping $z$ to $X$; $P(X)$ will be maximized by finding a computable formula for it, taking its gradient at each epoch and optimizing it using stochastic gradient ascent \eqref{GrindEQ__50_}.

\noindent 

\noindent $P\left(X\right)$ can be computed approximately by sampling $z$ values repeatedly $z=\{z_1,z_2,\dots ,z_n\}$ and computing $P(X)\approx \frac{1}{n}\sum_i{P(X|z_i)}$, in high dimensional spaces, $n$ might have to be very large before $P(X)$ can be accurately approximated \eqref{GrindEQ__50_}.

\noindent 

\noindent For most $z$, $P(X|z)$ will be close to zero, but in order for the VAE to be useful, we need to sample $z$ values that are likely to have resulted in $X$ and sample only from that subset, a new function $Q(z|X)$ is needed to take an existing $X$ value and calculate a distribution of $z$ values that could have realistically resulted in $X$ being generated; this narrows the universe of $z$ values down from the larger universe of all $z's$ likely under the prior $P(z)$ \eqref{GrindEQ__50_}.

\noindent 

\noindent How $E_{Z\sim Q}P(X|z)$ and $P(X)$ are related is one of the basic tenets upon which variational Bayesian methods are built. The Kullback-Leibler divergence ($\mathcal{D}$) between $P(z|X)$ and $Q(z)$ for an arbitrary $Q$ which does not necessarily have to depend on $X$, is given by:

\noindent 
\[\mathcal{D}\mathrm{[}Q\mathrm{(}z\mathrm{)|}\left|P\left(z\mathrel{\left|\vphantom{z X}\right.\kern-\nulldelimiterspace}X\right)\right]\mathrm{=}E_{Z\mathrm{\sim }Q}\mathrm{[}{\mathrm{log} Q\left(z\right)\ }\mathrm{-}{\mathrm{log} P\mathrm{(}z\mathrm{|}X\mathrm{)}\ }\mathrm{]}\] 


\noindent $P(X)$ and $P(X|z)$ can be added to this equation by applying Bayes rule:

\noindent 
\[\mathcal{D}\mathrm{[}Q\mathrm{(}z\mathrm{)|}\left|P\left(z\mathrel{\left|\vphantom{z X}\right.\kern-\nulldelimiterspace}X\right)\right]\mathrm{=}E_{Z\mathrm{\sim }Q}\left[{\mathrm{log} Q\left(z\right)\ }\mathrm{-}{\mathrm{log} P\left(z\mathrel{\left|\vphantom{z X}\right.\kern-\nulldelimiterspace}X\right)\ }\mathrm{-}{\mathrm{log} P\left(z\right)\ }\right]\mathrm{+}{\mathrm{log} P\mathrm{(}X\mathrm{)}\ }\] 


\noindent Since ${\mathrm{log} P(X)\ }$ does not depend on $z$, it appears outside the expectation. Rearrangement of this formula, negation and contraction of part of $E_{z\sim Q}$ into a KL-divergence term gives us:

\noindent 
\[{\mathrm{log} P\left(X\right)\mathrm{-}\ }\mathcal{D}\mathrm{[}Q\mathrm{(}z\mathrm{)|}\left|P\left(z\mathrel{\left|\vphantom{z X}\right.\kern-\nulldelimiterspace}X\right)\right]\mathrm{=}E_{Z\mathrm{\sim }Q}\left[{\mathrm{log} P\left(X\mathrm{|}z\right)\ }\right]\mathrm{-}\mathcal{D}\mathrm{[}Q\mathrm{(}z\mathrm{)||}P\mathrm{(}z\mathrm{)]}\] 


\noindent In the above equation, $X$ is fixed and $Q$ can be any distribution, regardless of whether it accurately maps $X$ to $z's$ that could have produced $X$, but in our case we are interested in accurately inferring $P(X)$ and therefore we want to find a $Q$ which does depend on $X$ and which also keeps $\mathcal{D}[Q(z)|\left|P\left(z\mathrel{\left|\vphantom{z X}\right.\kern-\nulldelimiterspace}X\right)\right]$ as small as possible:

\noindent 
\[{\mathrm{log} P\left(X\right)\mathrm{-}\ }\mathcal{D}\mathrm{[}Q\mathrm{(}z\mathrm{|}X\mathrm{)|}\left|P\left(z\mathrel{\left|\vphantom{z X}\right.\kern-\nulldelimiterspace}X\right)\right]\mathrm{=}E_{Z\mathrm{\sim }Q}\left[{\mathrm{log} P\left(X\mathrm{|}z\right)\ }\right]\mathrm{-}\mathcal{D}\mathrm{[}Q\mathrm{(}z\mathrm{|}X\mathrm{)||}P\mathrm{(}z\mathrm{)]}\] 


\noindent The formula above is the central formula of the VAE, the left hand side is what needs to be maximized: $P(X)$, penalized by $-D[Q(z|X)||P\left(z\mathrel{\left|\vphantom{z X}\right.\kern-\nulldelimiterspace}X\right)]$, which will be minimized if $Q$ is a high capacity distribution which produces $z$ values that are likely to reproduce $X$, the right hand side is differentiable and can therefore be optimized using gradient descent.

\noindent 

\noindent When looking at the above equation, the right hand side takes the form of an autoencoder, where $Q$ encodes $X$ into latent variables $z$ and P decodes these latent variables to reconstruct $X$.

\noindent 

\noindent On the left side of the equation, ${\mathrm{log} P(X)\ }$ is being maximized while $\mathcal{D}[Q(z|X)||P\left(z\mathrel{\left|\vphantom{z X}\right.\kern-\nulldelimiterspace}X\right)]$ is being minimized. While $P(z|X)$ is not analytically solvable and simply describes $z$ values likely to reproduce $X$, the second term in the KL-divergence on the left is forcing $Q(z|X)$ to be as similar as possible to $P(z|X)$, and under a model with sufficient capacity $Q(z|X)$ should be able to be exactly the same as $P(z|X)$, which will result in $\mathcal{D}$ being zero and the direct minimization of ${\mathrm{log} P(X)\ }$, in addition $P(z|X)$ is no longer intractable since $Q(z|X)$ can be used to solve for it.

\noindent 

\noindent In order to minimize the right hand side of the above equation via gradient descent, $Q(z|X)$  will usually take the form:

\noindent 
\[Q\left(z\mathrel{\left|\vphantom{z X}\right.\kern-\nulldelimiterspace}X\right)\mathrm{=}N\mathrm{(}z\mathrm{|}\mu \left(X\mathrm{;}\vartheta \right)\mathrm{,}\mathrm{\Sigma }\left(X\mathrm{;}\vartheta \right)\mathrm{)}\] 

 

\noindent Where $\mu $ and $\mathrm{\Sigma }$ are deterministic functions with learnt parameters $\vartheta $; in practice $\mu $ and $\mathrm{\Sigma }$ are learnt via neural networks and $\mathrm{\Sigma }$ is constrained to a diagonal matrix format. $\mathcal{D}[Q(z|X)||P\left(z\right)]$ therefore becomes a KL-divergence between two multivariate Gaussians, computed in closed form as:

\noindent 
\[\mathcal{D}\mathrm{[}N\mathrm{(}{\mu }_0,{\mathrm{\Sigma }}_0\mathrm{)|}\left|N\left({\mu }_{\mathrm{1}},{\mathrm{\Sigma }}_{\mathrm{1}}\right)\right]\mathrm{=\ }\frac{\mathrm{1}}{\mathrm{2}}\mathrm{(tr}\left({\mathrm{\Sigma }}^{\mathrm{-1}}_{\mathrm{1}}{\mathrm{\Sigma }}_0\right)\mathrm{+}{\left({\mu }_{\mathrm{1}}\mathrm{-}{\mu }_0\right)}^{\mathrm{\intercal }}{\mathrm{\Sigma }}^{\mathrm{-1}}\left({\mu }_{\mathrm{1}}\mathrm{-}{\mu }_0\right)\mathrm{-}k\mathrm{+}{\mathrm{log} \mathrm{(}\frac{\mathrm{det}{\mathrm{\Sigma }}_{\mathrm{1}}}{\mathrm{det}{\mathrm{\Sigma }}_0}\mathrm{)}\ }\] 


\noindent With \textit{k} indicating the number of dimensions of the distribution; this can be simplified to become:

\noindent 
\[\mathcal{D}\mathrm{[}N\mathrm{(}\mu \left(X\right)\mathrm{,}\mathrm{\Sigma }\mathrm{(}X\mathrm{))|}\left|N\left(0,I\right)\right]\mathrm{=}\frac{\mathrm{1}}{\mathrm{2}}\mathrm{(tr}\left(\mathrm{\Sigma }\left(\mathrm{X}\right)\right)\mathrm{+}{\left(\mu \left(X\right)\right)}^{\mathrm{\intercal }}\left(\mu \left(X\right)\right)\mathrm{-}k\mathrm{-}{\mathrm{log} \mathrm{det}\mathrm{}\mathrm{(}\mathrm{\Sigma }\mathrm{(}X\mathrm{))}\ }\mathrm{)}\] 


\noindent The other term on the right hand side of the equation, $E_{Z\sim Q}\left[{\mathrm{log} P\left(X|z\right)\ }\right]$, can be estimated by taking a sample from $z$ and calculating $P(X|z)$ for that single sample to approximate $E_{Z\sim Q}\left[{\mathrm{log} P\left(X|z\right)\ }\right]$.

\noindent 

\noindent Since we are doing stochastic gradient descent over different $X$ values from our dataset $D$, we want to perform gradient descent on the following formula:

\noindent 
\[E_{X\mathrm{\sim }D}\mathrm{[}{\mathrm{log} P\left(X\right)\ }\mathrm{-}\mathcal{D}\left[\mathrm{Q}\left(\mathrm{z}\mathrel{\left|\vphantom{\mathrm{z} \mathrm{X}}\right.\kern-\nulldelimiterspace}\mathrm{X}\right)\mathrel{\left|\vphantom{\mathrm{Q}\left(\mathrm{z}\mathrel{\left|\vphantom{\mathrm{z} \mathrm{X}}\right.\kern-\nulldelimiterspace}\mathrm{X}\right) \left|\mathrm{P}\left(\mathrm{z}\mathrel{\left|\vphantom{\mathrm{z} \mathrm{X}}\right.\kern-\nulldelimiterspace}\mathrm{X}\right)\right]}\right.\kern-\nulldelimiterspace}\left|\mathrm{P}\left(\mathrm{z}\mathrel{\left|\vphantom{\mathrm{z} \mathrm{X}}\right.\kern-\nulldelimiterspace}\mathrm{X}\right)\right]\right]\mathrm{=}E_{X\mathrm{\sim }D}\mathrm{[}E_{Z\mathrm{\sim }Q}\left[{\mathrm{log} P\left(X\mathrel{\left|\vphantom{X z}\right.\kern-\nulldelimiterspace}z\right)\ }\right]\mathrm{-}\mathcal{D}\mathrm{[Q(z|X)||P(z)]]}\] 


\noindent By sampling a single value of $X$ and a single value of $z$, we can compute the gradient of ${\mathrm{log} P\left(X\mathrel{\left|\vphantom{X z}\right.\kern-\nulldelimiterspace}z\right)-\mathcal{D}\mathrm{[Q(z|X)||P(z)]}\ }$, which when averaged over multiple samples, converges to the full equation to be optimized.

\noindent 

\noindent The issue here is that $E_{Z\sim Q}[{\mathrm{log} P(X|z)\ }]$ does not only depend on the parameters of $P$, but also those of $Q$, but this is not accounted for in the above equation. For VAEs to work properly, $Q$ needs to be driven to produce $z's$ from $X$ that are likely to be reliably decoded by $P$.

\noindent 

\noindent Figure 34 illustrates how this proxy formula can be used by averaging over multiple samples to get to the expected outcome, but since there is a sampling procedure embedded within the neural network, gradient descent cannot be performed on it.

\noindent 

\noindent Figure 35, on the other hand, shows how a ``reparameterization trick'' removes he sampling procedure from the neural network proper and treats it as an input layer. Since we have $\mu (X)$ and $\mathrm{\Sigma }(X)$, we can sample $\epsilon $ from $N(0,I)$ and compute $z$ from $\epsilon $ as follows: $z=\mu \left(X\right)+{\mathrm{\Sigma }}^{\frac{1}{2}}\left(X\right)*\epsilon $.

\noindent 

\noindent As a result, the gradient of the following equation will actually be taken:

\noindent 
\[E_{X\mathrm{\sim }D}\left[E_{\epsilon \mathrm{\sim }N\left(0,I\right)}\left[{\mathrm{log} P\left(X\mathrel{\left|\vphantom{X z\mathrm{=}\mu \left(X\right)\mathrm{+}{\mathrm{\Sigma }}^{\frac{\mathrm{1}}{\mathrm{2}}}\left(X\right)\mathrm{*}\epsilon }\right.\kern-\nulldelimiterspace}z\mathrm{=}\mu \left(X\right)\mathrm{+}{\mathrm{\Sigma }}^{\frac{\mathrm{1}}{\mathrm{2}}}\left(X\right)\mathrm{*}\epsilon \right)\ }\right]\mathrm{-}\mathcal{D}\mathrm{[Q(z|X)||P(z)]}\right]\] 


\noindent 

\noindent 

\noindent \includegraphics*[width=3.33in, height=3.33in, keepaspectratio=false]{image41}

\noindent \textbf{Figure 33: Training-time VAE}

\noindent 

\noindent 

\noindent \includegraphics*[width=3.48in, height=3.26in, keepaspectratio=false]{image42}

\noindent \textbf{Figure 34: Training-time VAE with reparameterization trick to enable backpropagation}

\noindent 

\noindent 

\noindent \includegraphics*[width=1.30in, height=2.24in, keepaspectratio=false]{image43}

\noindent \textbf{Figure 35: Testing time VAE}

\noindent 

\noindent 

\noindent Once the model is ready to be tested, values from $z\sim N(0,I)$ are sampled and fed to the decoder; the encoder, along with the attendant reparameterization trick used during training are thrown away. 


\paragraph{ Generative Adversarial Networks}

\noindent 

\noindent Generative Adversarial Networks (GANs) are a deep learning framework which pits two neural networks against each other in an adversarial mini-max game: the generative model $G$ is trained to the point where it accurately captures the distribution of the training data, and the discriminative network $D$ takes the output of $G$ and estimates the probability of whether $G$'s output originated from the actual data distribution or from a model distribution \eqref{GrindEQ__51_}.

\noindent 

\noindent The mini-max game can be expressed mathematically as:

\noindent 
\[{\mathop{\mathrm{min}}_{G} {\mathop{\mathrm{max}}_{D} V\left(D,G\right)\mathrm{=}E_{x\mathrm{\sim }p_{data}\left(x\right)}\left[{\mathrm{log} D\left(x\right)\ }\right]\mathrm{+}E_{z\mathrm{\sim }p_z\mathrm{(}z\mathrm{)}}\mathrm{[}{\mathrm{log} \left(\mathrm{1-}D\left(G\left(z\right)\right)\right)\mathrm{)}\ }\mathrm{]}\ }\ }\] 


\noindent Essentially, the objective is to maximize the probability of $D$ assigning the correct label to samples from $G$, i.e. is a given observation from the ``data''- or ``model'' distribution, while training $G$ to minimize ${\mathrm{log} \left(1-D\left(G\left(z\right)\right)\right)\ }$, i.e. we want $G$ to produce samples that are hard to discriminate from samples from the true data distribution.

\noindent 

\noindent This is done by sampling from a random noise vector $z$, with a defined prior $p_z(z)$ and learning a transformation from the noise vector to a distribution which is highly similar (preferably identical) to the true data distribution; in practice, this transforming function is the generative network $G(z,{\theta }_g)$, with ${\theta }_g$ being the parameters of a deep neural network which maps $z$ to data space.

\noindent 

\noindent In practice, the training algorithm will alternately optimize $D$ for $k$ steps and $G$ for a single step, which allows $D$ to remain close to its optimum if $G$ does not change too rapidly, this also allows for the algorithm to run computationally more efficiently and prevents overfitting. During the early stages of training, it will be quite easy for $D$ to discriminate between data and model samples, since $G$ will still be learning to output more realistic samples, therefore $G$'s objective function ${\mathrm{log} \left(1-D\left(G\left(z\right)\right)\right)\ }$ will saturate, so an alternative objective function ${\mathrm{log} D\left(G\left(z\right)\right)\ }$ is maximized in practice by $G$, which does not change the dynamics of $D$ and $G$ much but allows for gradients that are sufficiently large to perform useful stochastic gradient descent.

\noindent 

\noindent 

\noindent \includegraphics*[width=4.08in, height=1.90in, keepaspectratio=false, trim=0.00in 0.41in 0.00in 0.21in]{image44}

\noindent \textbf{Figure 36: Gan Densities during training, close to convergence, P(x) is shown in black, G(z) in blue and D(G(z)) in red}

\noindent 

\noindent 

\noindent \includegraphics*[width=3.86in, height=1.84in, keepaspectratio=false, trim=0.00in 0.36in 0.00in 0.19in]{image45}

\noindent \textbf{Figure 37: Gan Densities during training, once the Algorithm has converged, G(z) matches P(x) perfectly and D(G(z)) outputs 0.5 everywhere}

\noindent 


\paragraph{ Variations on the GAN concept used towards Event Simulation in this Dissertation}

\noindent 

\noindent 
\subparagraph{Auxiliary Classifier Generative Adversarial Network}

\noindent 

\noindent Auxiliary Classifier Generative Adversarial Networks enforce class label conditioned synthesis models, i.e. by feeding a GAN the class label of an image $c\ \sim p_c$, $G$ is now a function of both a noise vector and the label of the image, i.e. $G(z,c)$, whereas $D$ is not only tasked with classifying whether data is real or simulated, but also the class label of the image. This process has been shown to stabilize training \eqref{GrindEQ__52_}.

\noindent 

\noindent 
\subparagraph{Adversarial Autoencoder}

\noindent 

\noindent Adversarial Autoencoders match the aggregated posterior of the latent space vector from an autoencoder $q\left(z\right)=\int^{\ }_x{q\left(z\mathrel{\left|\vphantom{z x}\right.\kern-\nulldelimiterspace}x\right)p_d\left(x\right)dx}$ with an arbitrary prior distribution $p(z)$, a process which results in meaningful samples being generated from any sample from any part of the prior space. The decoder function learns a function to map from the imposed prior distribution to the data distribution. In this set-up, the generator of the GAN also acts as the encoder function of the autoencoder, a process which assists the generator in fooling the discriminator of the GAN into misclassifying simulated data as real data \eqref{GrindEQ__53_}.

\noindent 

\noindent 
\subparagraph{Bidirectional Generative Adversarial Network}

\noindent 

\noindent Bidirectional Generative Adversarial Networks (BiGANs) make use of an encoder function which maps data $x$ into a latent feature space $z$ for the generative model, i.e.: $E:{\mathrm{\Omega }}_x\longrightarrow {\mathrm{\Omega }}_z$. Here the discriminator has access to both the (simulated or real) $x$, as well as its latent encoding $z$ when classifying samples as real or simulated. In this way, BiGANs not only learn how to map from a latent space to data, but how to perform the inverse mapping, i.e. data to latent space \eqref{GrindEQ__54_}.

\noindent 
\subparagraph{Deep Convolutional Generative Adversarial Network}

\noindent 

\noindent Deep Convolutional Generative Adversarial Networks make use of fully convolutional neural networks for both the generator and discriminator, using strided convolutions to allow these networks to learn their own spatial downsampling in the case of the discriminator and spatial upsampling in the case of the generator. These models also make use of Batch Normalization, which ensures that the input to any hidden unit has zero mean and a variance of 1, a process which compensates for poor initialization strategies, helps with the flow of gradients through complex models and preventing the generator from outputting similar simulated images from any sample of the noise space, a common issue which plagues GANs \eqref{GrindEQ__55_}.

\noindent 
\subparagraph{Least Squares Generative Adversarial Network}

\noindent 

\noindent Whereas regular GANs use the sigmoid cross entropy loss function, which results in vanishing gradients when samples are generated that are classified as real data, but that are still far from looking like real data, Least Squares Generative Adversarial  Networks  (LSGANs) use an adaptation of the least squares loss function for the discriminator network, which has been shown to result in images of higher quality and result in networks that are more stable during training \eqref{GrindEQ__56_}.

\noindent 

\noindent 


\section{ Statistical Tests}

\noindent \eject 


\subsection{ Hypotheses}

\noindent Statistical tests are mathematical constructs designed to enable a researcher to make a measurable statement concerning to what extent observed data agrees with probabilistic predictions made about it in the form of a hypothesis \eqref{GrindEQ__57_}.

\noindent 

\noindent When performing a statistical test, a null hypothesis, denoted as $H_0$, is put forth, as well as one or more alternative hypotheses, ($H_1$,$\ H_2,\dots $).

\noindent 

\noindent Given a dataset of \textit{n} measurements of a random variable $x=\ x_1,\dots ,\ x_n$, a set of hypotheses $H_0,H_1$ are proposed, each specifying a joint probability density function (p.d.f.), i.e. $f\left(x\mathrel{\left|\vphantom{x H_0}\right.\kern-\nulldelimiterspace}H_0\right),\ f\left(x\mathrel{\left|\vphantom{x H_1}\right.\kern-\nulldelimiterspace}H_1\right),\dots $

\noindent 

\noindent In order to assess how well the observed data agrees with any given hypothesis, a test statistic $t(x)$, which is a function of the observed data, is constructed.

\noindent 

\noindent A specific p.d.f. for the test statistic, \textit{t}, is implied by each of the hypotheses, i.e. $g\left(t\mathrel{\left|\vphantom{t H_0}\right.\kern-\nulldelimiterspace}H_0\right),\ g\left(t\mathrel{\left|\vphantom{t H_1}\right.\kern-\nulldelimiterspace}H_1\right),\dots $

\noindent 

\noindent While the test statistic can be a multidimensional vector $t=t_1,t_2,\dots ,\ t_m$ (in principle, even the original vector of observed data points $x=x_1,x_2,\dots ,\ x_n$ can be used), constructing a test statistic of lower dimension (where \textit{m $<$ n}) reduces the amount of data being assessed, without losing discriminative power.

\noindent 

\noindent If a scalar function $t(x)$ is used as the test statistic, a p.d.f. $g\left(t\mathrel{\left|\vphantom{t H_0}\right.\kern-\nulldelimiterspace}H_0\right)$ is given which \textit{t} will conform to when $H_0$ is true, similarly \textit{t} will conform to a different p.d.f. $g\left(t\mathrel{\left|\vphantom{t H_1}\right.\kern-\nulldelimiterspace}H_1\right)$ when $H_1$ is true. Figure 39 illustrates how setting a threshold value for the test statistic, i.e. $t_{cut}$, results in rejection of the null hypothesis when $t>t_{cut}$.\textit{}

\noindent 

\noindent \includegraphics*[width=4.30in, height=2.70in, keepaspectratio=false]{image46}

\noindent \textbf{Figure 38: An illustration of rejection or acceptance of the null hypothesis, under the assumed distributions of ${\boldsymbol{H}}_{\boldsymbol{0}}$ and ${\boldsymbol{H}}_{\boldsymbol{1}}$, when t falls in the critical region $\boldsymbol{t}\boldsymbol{>}{\boldsymbol{t}}_{\boldsymbol{cut}}$}

\noindent The support for various hypotheses under the observed data distribution is framed in terms of acceptance or rejection of the null hypothesis by defining a critical region for the test statistic, beyond which the null hypothesis is rejected; i.e. when the observed value of \textit{t} lies within the critical region, we reject $H_0$. Conversely, when \textit{t} lies within the complement of the critical region, it is said to be within the acceptance region, which will result in the researcher accepting $H_0$.

\noindent 


\subsection{ Significance Level and Power}

\noindent The critical region for rejection of the null hypothesis is defined by a cut-off point, such that the probability of \textit{t} being observed there is defined by a value $\alpha $, called the significance level of the test.

\noindent 

\noindent In the example shown in Figure 39, a critical region is defined by a value: $t_{cut}$, which defines the lower decision boundary for rejecting the null hypothesis.

\noindent 

\noindent The significance level defined as such is given by

\noindent 
\[\alpha \mathrm{=}\int^{\mathrm{\infty }}_{t_{cut}}{g\left(t\mathrel{\left|\vphantom{t H_0}\right.\kern-\nulldelimiterspace}H_0\right)dt}\] 


\noindent $H_0$ would not be rejected when ${t<t}_{cut}$, and there is a probability of $\alpha $ of rejecting $H_0$ when $H_0$ is in fact true (called an error of the first kind), as well as a probability of accepting $H_0$ when $H_1$ was actually true. The probability of making an error of the second kind is given by

\noindent 
\[\beta \mathrm{=}\int^{t_{cut}}_{\mathrm{-}\mathrm{\infty }}{g\left(t\mathrel{\left|\vphantom{t H_{\mathrm{1}}}\right.\kern-\nulldelimiterspace}H_{\mathrm{1}}\right)dt}\] 
$1-\beta $ is called the power of the statistical test to discriminate against $H_1$.

\noindent 


\subsection{ Statistical Tests for Particle Selection}

\noindent 

\noindent In the case of electron-pion particle identification dealt with in this dissertation, we consider the class ``electron'' as signal and ``pion'' as background. As such, we define $H_0=e,{\ \ H}_{\mathrm{1}}=\pi $, and by extension, we treat the output of the final hidden unit in the neural network as a test statistic in its own right, lying either within a p.d.f. ${g(t|H}_0)$ when it is an electron or $g(t|H_{\mathrm{1}})$ when it is a pion. In order to accept or reject $H_0$, we define a critical region $t_{\mathrm{cut}}$. When ${t\ge t}_{\mathrm{cut}}$, we classify the particle as an electron.

\noindent 

\noindent When looking at the probability of classifying a specific particle as a given type, we define the selection efficiencies, i.e. the electron efficiency ${\varepsilon }_{\mathrm{e}}$ and pion efficiency ${\varepsilon }_{\mathrm{\piup }}$ as follows:

\noindent 
\[{\varepsilon }_{\mathrm{e}}\mathrm{=}\int^{t_{cut}}_{\mathrm{-}\mathrm{\infty }}{g\left(t\mathrel{\left|\vphantom{t e}\right.\kern-\nulldelimiterspace}e\right)dt\mathrm{=}\mathrm{1-}\alpha }\] 


\noindent 
\[{\varepsilon }_{\mathrm{\piup }}\mathrm{=}\int^{t_{cut}}_{\mathrm{-}\mathrm{\infty }}{g\left(t\mathrel{\left|\vphantom{t \pi }\right.\kern-\nulldelimiterspace}\pi \right)dt\mathrm{=}\beta }\] 


\noindent This cut-off point can be chosen so as to accept as many electrons as possible, but the price paid for high electron efficiency is a large amount of pion contamination in the electron sample.

\noindent 

\noindent Based on the probability of a particle being an electron obtained from each of the 6 detector layers in the TRD, we use a Bayesian approach outlined in the formula below:

\noindent 
\[P\left(elec\right)\mathrm{=}\frac{\prod^{\mathrm{6}}_{j\mathrm{=1}}{P_j\mathrm{(}elec\mathrm{)}}}{\sum_{k\mathrm{\in }e,\pi }{\prod^{\mathrm{6}}_{j\mathrm{=1}}{P_j\left(k\right)}}}\] 


\noindent Here, $P_j\left(elec\right)$ is the probability of the track being an electron obtained from layer $j$.

\noindent \eject 


\section{ Data}

\noindent \eject 


\subsection{ LHC Runs Used}

\begin{enumerate}
\item   000265377

\item  000265378

\item  000265309

\item  000265332

\item  000265334

\item  000265335

\item  000265336

\item  000265338

\item  000265339

\item  000265342

\item  000265343

\item  000265344

\item  000265381

\item  000265383

\item  000265385

\item  000265388

\item  000265419

\item  000265420

\item  000265425

\item  000265426

\item  000265499
\end{enumerate}

\noindent 


\subsection{ Data Structure}

\noindent 

\noindent A total of 7~735~493 tracklets were extracted from WLCG.

\noindent 

\noindent An example of the data obtained for a single track can be viewed at https://github.com/PsycheShaman/MSc-thesis/blob/master/NEW/example\_pythonDict.txt. This data structure consists of a header section with meta-information about the track, i.e. 

\noindent 

\begin{enumerate}
\item  The run-number and event number which the track was obtained from

\item  The ${\mathrm{V}}_0$ Track ID, identifying the primary vertex from which the track originated

\item  A track number, which is a unique identifier for the track in that event and run number

\item  A PDG code, which is 11 for electrons -11 for positrons, 211 and -211 for positively and negatively charged pions, respectively 

\item  n-$\sigmaup$ electron and n-$\sigmaup$ pion which are the number of standard deviations away from the expected electron- and pion signal, respectively

\item  The transverse momentum of the particle

\item  Information about the angle at which the particle is traveling, i.e. Eta, Theta and Phi

\item  dEdX estimate from the TPC

\item  The detector number, row and column the particle went through in layers 1-6 (indexed from 0-5 because of Python's indexing strategy)

\item  The raw data signal caused by the track as it traversed the six layers of the TRD
\end{enumerate}

\noindent 

\noindent When read into a list data structure in R, the full dataset amounts to 19.7GB.

\noindent 


\subsection{ Graphical Overview of Data}


\paragraph{ Example Images of Tracklet Signals}

\noindent Below are 10 examples of single tracklet signals, each from a single layer of the TRD. In the images below, the signal for 17 pads in the TRD layer were added (along the rows of the image), in order to ensure that the signal for the tracklet will be captured, in case tracking wasn't accurate enough. The columns in the images below represent the charge deposited during a specific time bin within the pad, i.e. moving across the pixels in a certain row of the image, gives an indication of the time-evolution of the signal as charge is deposited.

\noindent 

\noindent \includegraphics*[width=2.49in, height=1.82in, keepaspectratio=false]{image47}\includegraphics*[width=2.49in, height=1.82in, keepaspectratio=false]{image48}

\noindent 

\noindent 

\noindent \includegraphics*[width=2.49in, height=1.82in, keepaspectratio=false]{image49}\includegraphics*[width=2.49in, height=1.82in, keepaspectratio=false]{image50}

\noindent \includegraphics*[width=2.49in, height=1.82in, keepaspectratio=false]{image51}\includegraphics*[width=2.53in, height=1.86in, keepaspectratio=false]{image52}

\noindent \includegraphics*[width=2.53in, height=1.86in, keepaspectratio=false]{image53}\includegraphics*[width=2.49in, height=1.82in, keepaspectratio=false]{image54}

\noindent \includegraphics*[width=2.49in, height=1.82in, keepaspectratio=false]{image55}\includegraphics*[width=2.44in, height=1.79in, keepaspectratio=false]{image56}

\noindent 


\paragraph{ Number of entries per TRD layer number}

\noindent 

\noindent As expected, tracks reach the outermost layers less frequently, as can be seen in Figure 40.

\noindent 

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.14in, keepaspectratio=false, trim=0.00in 0.33in 0.00in 0.28in]{image57}

\noindent \textbf{Figure 39: Number of Entries per Layer Number, for all Runs}


\paragraph{ Missing Data}

\noindent 1~098~636 tracklets returned images with no data (i.e. every pixel = 0), this amounts to 14.5\% of all pions and 12.6\% of all electrons.


\paragraph{ Mean ADC Value per Layer}

\noindent 

\noindent \includegraphics*[width=5.90in, height=3.27in, keepaspectratio=false, trim=0.00in 0.00in 0.00in 0.28in]{image58}

\noindent \textbf{Figure 40: Mean ADC Values per layer, for all runs}


\paragraph{ Principal Component Analysis}

\noindent \includegraphics*[width=5.98in, height=3.62in, keepaspectratio=false]{image59}

\noindent \textbf{Figure 41: 2D PCA}

\noindent 

\noindent \includegraphics*[width=5.92in, height=3.64in, keepaspectratio=false]{image60}

\noindent \textbf{Figure 42: 3D PCA}


\paragraph{ t-SNE}

\noindent \includegraphics*[width=5.95in, height=3.70in, keepaspectratio=false]{image61}


\paragraph{ Momentum bin counts per Particle ID}

\noindent \includegraphics*[width=5.93in, height=3.70in, keepaspectratio=false]{image62}

\noindent \textbf{Figure 43}


\paragraph{ Electron and Pion Counts per Run}

\noindent What follows below is a graphical overview of the number of electrons and pions which were measured in each run, from which it is immediately obvious that there is an overwhelming majority of pions detected.

\noindent 

\noindent \includegraphics*[width=2.99in, height=1.85in, keepaspectratio=false]{image63}\includegraphics*[width=2.84in, height=1.76in, keepaspectratio=false]{image64}

\noindent 

\noindent \includegraphics*[width=2.92in, height=1.80in, keepaspectratio=false]{image65}\includegraphics*[width=2.70in, height=1.67in, keepaspectratio=false]{image66}

\noindent 

\noindent \includegraphics*[width=2.99in, height=1.85in, keepaspectratio=false]{image67}\includegraphics*[width=2.63in, height=1.62in, keepaspectratio=false]{image68}

\noindent 

\noindent \includegraphics*[width=2.70in, height=1.67in, keepaspectratio=false]{image69}

\noindent 


\paragraph{ Bethe Bloch Curve per Run for Electrons and Pions}

\noindent The plots below depict the energy loss as a function of detector material traversed, i.e. the Bethe-Bloch curves for electrons and pions, respectively. It is clear from these plots that electrons deposit proportionately more energy than pions in the lower GeV range.

\noindent 

\noindent \includegraphics*[width=3.06in, height=1.89in, keepaspectratio=false]{image70}\includegraphics*[width=2.77in, height=1.71in, keepaspectratio=false]{image71}

\noindent 

\noindent \includegraphics*[width=2.99in, height=1.85in, keepaspectratio=false]{image72}\includegraphics*[width=2.84in, height=1.76in, keepaspectratio=false]{image73}

\noindent 

\noindent \includegraphics*[width=3.14in, height=1.94in, keepaspectratio=false]{image74}\includegraphics*[width=2.70in, height=1.67in, keepaspectratio=false]{image75}

\noindent 

\noindent 

\noindent 


\paragraph{ n$\sigmaup$ Electron per Run for Electrons and Pions}

\noindent 

\noindent In the plots below, the statistical estimate for electron and pion identification is depicted, it is clear from the plots below that particles with a low n$\sigmaup$ Electron value have been classified as electrons. Pions are centred around an n$\sigmaup$ Electron value of around -5.

\noindent 

\noindent \includegraphics*[width=2.84in, height=1.76in, keepaspectratio=false]{image76}\includegraphics*[width=2.84in, height=1.76in, keepaspectratio=false]{image77}

\noindent 

\noindent \includegraphics*[width=2.99in, height=1.85in, keepaspectratio=false]{image78}\includegraphics*[width=2.84in, height=1.76in, keepaspectratio=false]{image79}

\noindent 

\noindent \includegraphics*[width=2.99in, height=1.85in, keepaspectratio=false]{image80}\includegraphics*[width=2.84in, height=1.76in, keepaspectratio=false]{image81}

\noindent 

\noindent \includegraphics*[width=3.06in, height=1.89in, keepaspectratio=false]{image82}

\noindent 


\section{ Methods}

\noindent \eject 


\subsection{ Data Extraction }

\noindent Using the AliPhysics installation on the Hep01 cluster in the Physics Department at UCT, an AliRoot macro ana.C (https://github.com/PsycheShaman/trdML-gerhard/blob/master/ana.C), modified from a version developed by other collaborators in the SA-ALICE group, was used to extract data from WLCG.

\noindent 

\noindent This script interfaces with https://github.com/PsycheShaman/trdML-gerhard/blob/master/AliTRDdigitsExtract.cxx, also modified from a previously developed C++ file, TRD digits were extracted and filtered for the runs specified in Chapter 5. By redirecting the C++ standard out to a text file, the relevant data was saved into Python dictionaries for each run.

\noindent 

\noindent Jobs were submitted onto the WLCG and monitored using http://alimonitor.cern.ch/. Upon completion, data was extracted back onto Hep01 using the aliensh environment and from there was transferred to a local machine using the rsync bash command.

\noindent 

\noindent Data was backed up in a semi-private GitLab repository, accessible by CERN members, at https://gitlab.cern.ch/cviljoen/msc-thesis-data.

\noindent 

\noindent For work done in Python, both raw digits and particle IDs were extracted into numpy arrays, using https://github.com/PsycheShaman/trdpid/blob/master/py\_datatools/extract/dataset\_generator.py a script developed by an SA-ALICE collaborator, Jeremy Wilkinson.

\noindent 

\noindent For work done in R, python dictionaries were transformed into JSON files, which can be read in by R, using https://github.com/PsycheShaman/MSc-thesis/blob/master/NEW/data\_preprocessing/cat\_files.py.

\noindent 


\subsection{ Deep Learning for Particle Identification}


\paragraph{ 6-tracklet Case with Downsampling, Across Entire Momentum Spectrum}

\noindent Various deep neural network architectures were developed towards particle identification, using Keras with a Tensorflow back-end.

\noindent 

\noindent The following repositories host the code used to build and train feedforward-, convolutional- and LSTM neural networks towards particle identification:

\noindent 

\noindent https://github.com/PsycheShaman/msc-hpc

\noindent 

\noindent https://github.com/PsycheShaman/hpc-mini

\noindent 

\noindent https://github.com/PsycheShaman/MSc-thesis

\noindent 

\noindent A summary of the input tensors used for the various types of deep learning architectures mentioned above is as follows:

\noindent 

\begin{enumerate}
\item  Feed-forward neural networks were fed with summarised data, i.e. the $17\times 24$ matrices were collapsed down to 24 row-sums and 17 column-sums.

\item  2D convolutional neural networks were fed data with one image channel added to the 17$\ \times \ $24 matrices, i.e. $17\times 24\times 1$

\item  1D convolutional neural networks were fed data with one channel added to the 24 column-sums, i.e. $24\times 1$

\item  LSTM neural networks were fed data with the 17$\ \times \ $24 matrices transposed to become 24$\ \times $ 17, i.e. 24 time-bins with 17 features
\end{enumerate}

\noindent 

\noindent Class imbalances were accounted for by downsampling the pion sample to be equal in size to the electron sample.

\noindent 

\noindent Data was normalized as follows:

\noindent 
\[x\mathrm{=}\frac{x\mathrm{-}\mathrm{max}\mathrm{}\mathrm{(}x\mathrm{)}}{\mathrm{max}\mathrm{}\mathrm{(}x\mathrm{)}}\] 


\noindent The SLURM-managed High-Performance Computing Cluster at UCT was utilized extensively to test the performance of various deep learning architectures, enabling one to train various deep learning models in parallel.

\noindent 


\paragraph{ Single Tracklet Case, No Downsampling, Separate Analysis per Momentum Bin}

\noindent 

\noindent Tracklets with no signal was removed.

\noindent 

\noindent Data was split into the following momentum bins:

\noindent 
\[P\le 2\ GeV,\] 
\[2\ GeV<P\le 3\ GeV,\] 
\[3\ GeV<P\le 4\ GeV,\] 
\[4\ GeV<P\le 5\ GeV,\] 
\[5\ GeV<P\le 6\ GeV\] 


\noindent 

\noindent 
\subparagraph{Accounting for Class Imbalance without resorting to down-sampling: Custom Focal Loss Function}

\noindent 

\noindent Focal Loss was proposed by \eqref{GrindEQ__58_} as a loss function which down-weights the importance of well-classified examples, effectively making training examples that are more difficult to classify contribute more to the overall loss. This is useful in cases, such as this project, where one class dominates the class distribution and therefore becomes favoured during prediction, since doing so results in a low overall loss. Focal loss is not a default loss function in Keras, but has been implemented by \eqref{GrindEQ__59_} for Python. The author adapted this for use in Keras with R \eqref{GrindEQ__60_}, since no equivalent custom focal loss implementation could be found for R.

\noindent 

\noindent \includegraphics*[width=5.91in, height=3.62in, keepaspectratio=false]{image83}

\noindent \textbf{Figure 44}

\noindent 

\noindent 
\subparagraph{Resume training of model}

\noindent 

\noindent Keras allows one to resume training of a model, by increasing the number of epochs in the keras::fit() function to the total number of desired epochs, i.e. number of epochs (previous training round) + additional epochs to be run, and specifying an initial\_epochs argument, which should be the final epoch number from the previous training round.


\subsection{ Deep Learning for Distinguishing Geant4 data from real data}

\noindent 

\noindent Geant4 based simulations were configured using https://github.com/PsycheShaman/trdpid/blob/master/sim/Config.C , simulations were run as per the following shell script: https://github.com/PsycheShaman/trdpid/blob/master/sim/runtest.sh which calls upon the simulation script https://github.com/PsycheShaman/trdpid/blob/master/sim/sim.C the reconstruction script https://github.com/PsycheShaman/trdpid/blob/master/sim/rec.C and the analysis script https://github.com/PsycheShaman/trdpid/blob/master/sim/ana.C in sequence in order to create Monte Carlo simulations in a similar format to raw data analysed during particle identification.

\noindent 

\noindent Before commencing deep learning, the dataset was sanitized to ensure that similar ranges for key variables were covered by both real and simulated data, Eta distributions for real and simulated data were similar.

\noindent 

\noindent The following cut on Eta was applied to both datasets:

\noindent 
\[\left|\mathrm{\etaup }\right|\mathrm{\le }\mathrm{0.9}\] 


\noindent 

\noindent \includegraphics*[width=3.51in, height=2.16in, keepaspectratio=false]{image84}

\noindent \textbf{Figure 45: Eta distributions for real and Geant simulated data}

\noindent n$\sigmaup$-pion estimates from the Time Projection Chamber (TPC) were dissimilar for real and simulated data (n$\sigmaup$ = -999 is an error flag).

\noindent 

\noindent The following cut on n$\sigmaup$ was applied to real and simulated data:

\noindent 
\[{\mathrm{|n}\mathrm{\sigmaup }}_{\pi }\mathrm{|}\mathrm{\le }\mathrm{3}\] 


\noindent \includegraphics*[width=3.56in, height=2.20in, keepaspectratio=false]{image85}

\noindent 

\noindent \textbf{Figure 46: nsigma pion estimate (TPC) distributions for real and Geant simulated data, before cut}

\noindent \includegraphics*[width=3.43in, height=2.12in, keepaspectratio=false]{image86}

\noindent \textbf{Figure 47: nsigma pion distributions after applying cut}

\noindent 

\noindent Momentum ranges for real and simulated data were also quite dissimilar, the following cut on momentum was applied to both real and simulated data:

\noindent 
\[\mathrm{1.5}\mathrm{\le }\left|P\right|\mathrm{\le }\mathrm{20}\] 


\noindent \includegraphics*[width=3.45in, height=2.13in, keepaspectratio=false]{image87}

\noindent \textbf{Figure 48: Momentum distributions for both real and Geant simulated data, before cut}

\noindent \includegraphics*[width=3.45in, height=2.13in, keepaspectratio=false]{image88}

\noindent \textbf{Figure 49: Momentum distributions after cut}

\noindent 

\noindent 

\noindent The task of distinguishing simulated from real data was performed using convolutional neural networks.

\noindent 


\subsection{ Deep Generative Models Towards Event Simulation}

\noindent 

\noindent Towards developing a prototype for event simulation, the following models were built:

\noindent 

\begin{enumerate}
\item  Autoencoders

\item  Variational Autoencoders

\item  Various types of Generative Adversarial Networks
\end{enumerate}

\noindent 

\noindent The following repositories contain code used to build and run Deep Generative Models for this project:

\noindent 

\noindent https://github.com/PsycheShaman/deep-gen

\noindent 

\noindent https://github.com/PsycheShaman/Keras-GAN (forked from https://github.com/eriklindernoren/Keras-GAN and adapted to be able to work with data from this project).


\section{ Results}

\noindent \eject 


\subsection{ Deep Learning for Particle Identification}


\paragraph{ 2D Convolutional Neural Networks}

\noindent 

\noindent After testing a wide variety of architecture possibilities, with various levels of hyperparameter settings, i.e. learning rate, activation functions, optimizers, batch sizes, epochs, number of convolutional/ recurrent/ dense layers; the following 2D convolutional neural network achieved the lowest pion efficiency ${\varepsilon }_{\pi }=2.2\%$ at electron efficiency ${\varepsilon }_{e^-}=90\%$:

\noindent 

\noindent \includegraphics*[width=1.68in, height=5.96in, keepaspectratio=false]{image89}

\noindent \textbf{Figure 50: Particle Identification Model Architecture}

\noindent 

\noindent Using the Adam optimizer with learning rate = 0.00001, trained for 100 epochs with a batch size of 32, using binary cross-entropy as the loss function to be optimized.

\noindent 

\noindent The training and validation accuracy and loss graphs for this model are depicted below.

\noindent 

\noindent \includegraphics*[width=3.88in, height=3.05in, keepaspectratio=false]{image90}

\noindent \textbf{Figure 51: Training vs Validation Accuracy}

\noindent 

\noindent \includegraphics*[width=3.88in, height=3.00in, keepaspectratio=false]{image91}

\noindent \textbf{Figure 52: Training vs Validation Loss}

\noindent Pion efficiency at 90\% electron efficiency was calculated by writing a function which, when minimized, finds the cut-off point (critical region) in the distribution of the single node output layer which results in 90\% of true electrons being classified as electrons, on a per-tracklet basis. This was done for tracks which left signal in all 6 layers of the TRD, allowing one to construct a test statistic taking all 6 estimates for the track into account, as outlined in 5.3 Statistical Tests for Particle Selection.

\noindent 

\noindent The result of the minimization process is shown in Figure 48: any track which receives a combined probability above $t_{cut}$ (shown as a vertical red line) was classified as an electron, otherwise it was classified as a pion.

\noindent 

\noindent \includegraphics*[width=3.90in, height=2.24in, keepaspectratio=false]{image92}

\noindent \textbf{Figure 53: t-statistic selection allowing for 90\% electron efficiency}

\noindent After applying the classification based on $t_{cut}$, the histograms for the 6-tracklet estimate, for both electrons and pions is shown below:

\noindent 

\noindent \includegraphics*[width=2.88in, height=3.20in, keepaspectratio=false]{image93}

\noindent \textbf{Figure 54: Combined output probabilities for true electrons (1, blue) and true pions (0, red)}

\noindent Table 2 shows the obtained confusion matrix, with predicted labels along the rows and actual labels along the columns, here the diagonal of the confusion matrix are particles that were classified correctly:

\noindent 

\noindent \textbf{Table 2: Confusion Matrix for Particle Identification}

\begin{tabular}{|p{1.3in}|p{1.3in}|p{1.3in}|} \hline 
\textbf{Prediction/Actual} & \textbf{0} & \textbf{1} \\ \hline 
0 & 47 833 & 4 893 \\ \hline 
1 & 1 088 & 44 028 \\ \hline 
\end{tabular}



\noindent Whilst they may not be very interpretable, the weights for the four convolutional layers of this model are plotted in Figure 50, Figure 51, Figure 52 and Figure 53 below. 

\noindent \includegraphics*[width=4.31in, height=4.35in, keepaspectratio=false]{image94}

\noindent \textbf{Figure 55: Weights of first convolutional layer}

\noindent 

\noindent \includegraphics*[width=4.15in, height=4.08in, keepaspectratio=false]{image95}

\noindent \textbf{Figure 56: Weights of second convolutional layer}

\noindent \includegraphics*[width=3.89in, height=3.74in, keepaspectratio=false]{image96}

\noindent \textbf{Figure 57: Weights of third convolutional layer}

\noindent 

\noindent \includegraphics*[width=3.90in, height=3.77in, keepaspectratio=false]{image97}

\noindent \textbf{Figure 58: Weights of fourth convolutional layer}


\paragraph{ 1D Convolutional Neural Networks}

\noindent ${\varepsilon }_{\pi }=6.5\%$ at electron efficiency ${\varepsilon }_{e^-}=90\%$

\noindent 

\noindent 


\paragraph{ Fully Connected Feedforward Neural Networks}

\noindent ${\varepsilon }_{\pi }=4.45\%$ at electron efficiency ${\varepsilon }_{e^-}=89.99\%$


\paragraph{ LSTM Neural Networks}

\noindent ${\varepsilon }_{\pi }=8.5\%$ at electron efficiency ${\varepsilon }_{e^-}=90\%$


\paragraph{ Random Forests}

\noindent ${\varepsilon }_{\pi }=5.8\%$ at electron efficiency ${\varepsilon }_{e^-}=90\%$


\paragraph{ Gradient Boosting Machines}

\noindent ${\varepsilon }_{\pi }=6.59\%$ at electron efficiency ${\varepsilon }_{e^-}=89.99\%$

\noindent 


\paragraph{ VAE for Classification}

\noindent 

\noindent \includegraphics*[width=5.91in, height=3.53in, keepaspectratio=false]{image98}

\noindent \textbf{Figure 59: The three Principal Components derived from a VAE's (256:128:3:128:256) latent variables on training set = 5~587~819 observations}

\noindent 

\noindent \includegraphics*[width=5.91in, height=3.64in, keepaspectratio=false]{image99}


\subsection{ Distinguishing Geant4 Simulations from Real Data}

\noindent 

\noindent Distinguishing Geant4 simulations from real data proved to be a much easier task than distinguishing real electrons from real pions, as depicted in the training graphs in Figure 59.

\noindent 

\noindent \includegraphics*[width=3.25in, height=3.25in, keepaspectratio=false]{image100}

\noindent \textbf{Figure 60: Training loss and accuracy curves for training and validation data}

\noindent Table 3 shows the obtained confusion matrix for the following model architecture:

\noindent \includegraphics*[width=2.17in, height=5.41in, keepaspectratio=false]{image101}

\noindent \textbf{Figure 61: Model architecture for distinguishing real from Geant simulated data}

\noindent 

\noindent \textbf{Table 3: Confusion Matrix for distinguishing between Geant vs Real Data }

\begin{tabular}{|p{1.3in}|p{1.3in}|p{1.3in}|} \hline 
\textbf{Prediction/Actual} & \textbf{0} & \textbf{1} \\ \hline 
0 & 42 553 & 681 \\ \hline 
1 & 7 069 & 24 058 \\ \hline 
\end{tabular}




\subsection{ Deep Generative Models Towards Event Simulation}


\paragraph{ Variational Autoencoders}

\noindent 

\noindent Although various VAEs were prototyped, the following architecture produced the best results:

\noindent 

\noindent \includegraphics*[width=1.39in, height=2.67in, keepaspectratio=false]{image102}

\noindent \textbf{Figure 62: Encoder}

\noindent Encoder returns the $\mu $ for each of the 100 latent dimensions, $\sum{\ }$, which is calculated as $\mu \times $0.5; and $z$, which is the result of multiplying a random normal vector $\varepsilon $ with $e^{\sum{\ }}$ and adding $\mu $, i.e.
\[z\mathrm{=}\left(\varepsilon \mathrm{\times }e^{\sum{\mathrm{\ }}}\right)\mathrm{+}\mu \] 


\noindent The input to the decoder is a sampled z vector as defined above.

\noindent 

\noindent \includegraphics*[width=1.67in, height=4.34in, keepaspectratio=false]{image103}

\noindent \textbf{Figure 63: Decoder}

\noindent Below are four examples of simulated tracklet image data, produced by the VAE as explained above.

\noindent 

\noindent \includegraphics*[width=2.44in, height=1.79in, keepaspectratio=false]{image104}\includegraphics*[width=2.49in, height=1.82in, keepaspectratio=false]{image105}

\noindent 

\noindent \includegraphics*[width=2.53in, height=1.86in, keepaspectratio=false]{image106}\includegraphics*[width=2.44in, height=1.79in, keepaspectratio=false]{image107}

\noindent \textbf{Figure 64: Four examples of simulated data created using a Variational Autoencoder}

\noindent 
{\bf Deep Learning Towards Distinguishing Variational Autoencoder Data from Real Data}

\noindent 

\noindent While these results look quite believable at first glance, it was quite easy to distinguish 100~000 real data samples vs 100~000 samples simulated with VAE using a CNN to 100\% accuracy, as can be seen in Figure 59.

\noindent 

\noindent \includegraphics*[width=2.54in, height=1.82in, keepaspectratio=false]{image108}\includegraphics*[width=2.54in, height=1.82in, keepaspectratio=false]{image109}

\noindent \textbf{Figure 65: Training accuracy and loss curves for training vs validation data}

\noindent 


\paragraph{ Generative Adversarial Networks}

\noindent 

\noindent Below is a quick summary of the various Deep Generative Models used under the broader GAN category; which also shows how adjusting certain parameters lead to different results, illustrated by the accompanying images.

\noindent 
\subparagraph{Adversarial Autoencoder}

\noindent 
{\bf Version 1}

\noindent 

\noindent 10 Latent dimensions

\noindent Adam optimizer with learning rate = 0.002 and beta1 = 0.5

\noindent Batch size = 32

\noindent Encoder with 2 hidden layers with 512 nodes each, using leaky ReLU activation

\noindent Decoder with 2 hidden layers with 512 nodes each, using leaky ReLU activation and an output layer with tanh activation

\noindent Discriminator with two hidden layers, with 512 and 256 nodes respectively and sigmoid activation in the single-node output layer

\noindent 

\noindent Sample result after 19800 epochs:

\noindent \includegraphics*[width=4.86in, height=3.24in, keepaspectratio=false]{image110}

\noindent 

\noindent 
{\bf Version 2}

\noindent 

\noindent 100 Latent dimensions

\noindent Adam optimizer with learning rate = 0.00002 and beta1 = 0.5

\noindent Batch size = 32

\noindent Encoder with 3 hidden layers with 512 nodes each, using leaky ReLU activation

\noindent Decoder with 3 hidden layers with 512 nodes each, using leaky ReLU activation and an output layer with tanh activation

\noindent Discriminator with two hidden layers, with 512 and 256 nodes respectively and sigmoid activation in the single-node output layer

\noindent 

\noindent Sample result after 30800 epochs:

\noindent 

\noindent \includegraphics*[width=5.04in, height=3.36in, keepaspectratio=false]{image111}

\noindent 
{\bf Version 3}

\noindent 

\noindent 8 Latent dimensions

\noindent Adam optimizer with learning rate = 0.000002 and beta1 = 0.5

\noindent Batch size = 32

\noindent Encoder with 7 hidden layers with 1024, 512, 256, 128, 64, 32 and 16 nodes respectively, using leaky ReLU activation

\noindent Decoder with 4 hidden layers with 128, 256, 512 and 1024 nodes respectively, using leaky ReLU activation and an output layer with tanh activation

\noindent Discriminator with 4 hidden layers, with 1024, 512, 256 and 128 nodes respectively and sigmoid activation in the single-node output layer

\noindent 

\noindent Sample result after 23600 epochs:

\noindent 

\noindent \includegraphics*[width=4.92in, height=3.28in, keepaspectratio=false]{image112}

\noindent 

\noindent 
{\bf Version 4}

\noindent 

\noindent 12 Latent dimensions

\noindent Adam optimizer with learning rate = 0.000002 and beta1 = 0.5

\noindent Batch size = 32

\noindent Encoder with 5 hidden layers with 1024, 512, 512, 128 and 128 nodes respectively, using leaky ReLU activation

\noindent Decoder with 4 hidden layers with 256, 256, 512 and 1024 nodes respectively, using leaky ReLU activation and an output layer with tanh activation

\noindent Discriminator with 8 hidden layers, with 512 nodes each and sigmoid activation in the single-node output layer

\noindent 

\noindent Sample result after 73200 epochs:

\noindent 

\noindent \includegraphics*[width=4.98in, height=3.32in, keepaspectratio=false]{image113}

\noindent 

\noindent 

\noindent 
\subparagraph{Bidirectional Generative Adversarial Network}

\noindent 

\noindent 100 Latent dimensions

\noindent Encoder with 2 hidden layers with 512 units each, using Leaky ReLU activation and employing Batch Normalization with momentum = 0.8 after each

\noindent Generator with the same architecture as the encoder, with an additional dense layer of the same dimensions as the number of pixels in an image, with tanh activation

\noindent Discriminator with 3 hidden layers with 1024 nodes each using leaky relu activation and a single node output layer with sigmoid activation function

\noindent Trained using Adam Optimizer with Learning Rate = 0.0002 and Beta1=0.9 and a batch size of 32

\noindent 

\noindent Example output after 39600 epochs:

\noindent \includegraphics*[width=5.10in, height=3.40in, keepaspectratio=false]{image114}

\noindent 

\noindent \textit{Deep Convolutional Generative Adversarial Network}

\noindent 

\noindent 100 Latent dimensions

\noindent Generator with Dense layer with 3072 nodes, reshaped to $\mathrm{4\times 6\times 128}$ with 2D Upsampling, followed by 3 2D convolutional layers, with the first two followed by Batch normalization and 2D Upsampling, using ReLU activation in the first 4 layers and tanh in the final output layer

\noindent Discriminator with 4 2D convolutional layers, with Batch Normalization applied after the second, third and fourth layer and zero-padding after the second layer, using leaky ReLU activation in the hidden layer and sigmoid in the output node.

\noindent 

\noindent Example output after 57500 epochs:

\noindent \includegraphics*[width=4.92in, height=3.28in, keepaspectratio=false]{image115}

\noindent \textit{}

\noindent \textit{Generative Adversarial Network}

\noindent \textit{}

\noindent 100 Latent dimensions

\noindent Two separate Adam Optimizers used for Generator and Discriminator, i.e. with learning rate = 0.00002 and beta1=0.5 for Discriminator and learning rate = 0.00001 and beta1=0.5 for Generator

\noindent Generator with 12 hidden layer with 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100,1200 hidden units, using leaky ReLU activation and a dense output layer of the desired output image dimensions with tanh activation, including Batch Normalization with momentum = 0.8 after each hidden layer

\noindent Discriminator with 7 hidden layers with 1024, 1024, 512, 512, 256, 256 and 128 units, respectively, using leaky ReLU activation and a single node output layer using sigmoid activation.

\noindent 

\noindent Example Output after 66400 epochs:

\noindent \includegraphics*[width=4.92in, height=3.28in, keepaspectratio=false]{image116}

\noindent 

\noindent 

\noindent \textit{Least Squares Generative Adversarial Network}

\noindent \textit{}

\noindent 100 Latent dimensions

\noindent Adam Optimizer with learning rate = 0.00002 and beta1 = 0.5 using batch size = 32

\noindent Generator with 6 hidden dense layers with 256, 256, 512, 512, 1024 and 1024 hidden layers using leaky ReLU activation and an output layer with tanh activation, using Batch Normalization with momentum = 0.8 after each hidden layer

\noindent Discriminator with 8 hidden layers with 1024, 1024, 512, 512, 256, 256, 128 and 128 hidden layer and a single node output layer with no activation function

\noindent 

\noindent \textit{}

\noindent Example output after 64600 epochs:

\noindent 

\noindent \includegraphics*[width=4.74in, height=3.16in, keepaspectratio=false]{image117}

\noindent \eject 


\section{ Discussion and Conclusions}

\noindent \eject 

\noindent 


\subsection{ Discussion}


\paragraph{ Accuracy Paradox}

\noindent When class imbalances are not accounted for, misleadingly good results appear to occur, but this level of accuracy just reflects the ratio of the classes in the dataset we're working with (the model learns that it gets the lowest loss when it favours the prediction of pion, since there are so many more pions in our dataset, compared to electrons).

\noindent 

\noindent \includegraphics*[width=5.58in, height=3.86in, keepaspectratio=false]{image118}

\noindent \textbf{Figure 66: Accuracy Paradox}


\paragraph{ Particle Identification Using Deep Learning}

\noindent 
\subparagraph{Inherent Limit on the Amount of Information Contained about the Class Label}

\noindent 

\noindent One of the most salient features of the data used in this project is the inherent limit of the amount of information that the input features $x$ contain about the target feature $y$.

\noindent 

\noindent There seems to be an absolute limit at around 75\% training accuracy, regardless of: 

\noindent 

\begin{enumerate}
\item  which architecture was used (Figure 60, Figure 61 and Figure 62 show how this problem is potentially solvable by 2D Convolutional, LSTM and 1D Convolutional Neural Networks)
\end{enumerate}

\noindent 

\noindent 

\noindent \includegraphics*[width=2.55in, height=2.00in, keepaspectratio=false]{image119}

\noindent \textbf{Figure 67: Example of a 2D-Convolutional Network training to high validation accuracy}

\noindent \includegraphics*[width=2.47in, height=1.91in, keepaspectratio=false]{image120}

\noindent \textbf{Figure 68: Example of an LSTM Network training to high validation accuracy}

\noindent \includegraphics*[width=2.53in, height=1.96in, keepaspectratio=false]{image121}

\noindent \textbf{Figure 69: Example of a 1D-Convolutional Neural Network training to high validation accuracy}

\noindent 

\noindent 

\begin{enumerate}
\item  the amount of epochs used for training (Figure 63 shows how training a highly successful model for twice the number of epochs only results in eventual overfitting)
\end{enumerate}

\noindent 

\noindent 

\noindent \includegraphics*[width=2.61in, height=2.05in, keepaspectratio=false]{image122}

\noindent \textbf{Figure 70: Running a successful model for twice the number of epochs results in minimal gains and eventually, results in overfitting}

\noindent 

\noindent 
\subparagraph{Convolutional Neural Networks}

\noindent 

\noindent Regardless of the limitations outlined above, 2D convolutional neural networks resulted in the highest performance in general, but interestingly, even a very simplistic convolutional neural network (one convolutional layer, using 16 convolutional filters with kernel size equal to image dimensions, i.e. $17\times 24$ and a single dense layer with 12 nodes) still gave comparable results, i.e. ${\varepsilon }_{\pi }=6.2\%$.

\noindent 

\noindent Adding capacity to this simplistic architecture, by increasing from 1 to 4 dense hidden layers increased performance slightly, i.e. to ${\varepsilon }_{\pi }=5.3\%$.

\noindent 

\noindent Using a single convolutional layer with smaller kernel size i.e. $3\times 3,\ \ $and one dense layer did not give comparable results however and resulted in a much higher pion efficiency of ${\varepsilon }_{\pi }=11.2\%$. Again, adding an additional dense layer to this model improved results up to ${\varepsilon }_{\pi }=8.5\%$, but when 3 hidden layers were used with Max Pooling after the second convolutional layer, performance dropped back down to ${\varepsilon }_{\pi }=10\%$.

\noindent 

\noindent Conversely, using 3 convolutional layers with smaller kernel sizes, i.e. $3\times 3,\ \ 3\times 3\ and\ 2\times 2$, and doubling the number of convolutional filters in each layer, i.e. 16, 32 and 64 filters in layer 1, 2 and 3 respectively, with Max Pooling layers after the first and second layers resulted in a pion efficiency of ${\varepsilon }_{\pi }=5.5\%$.

\noindent 

\noindent Using a 2D Transpose Convolutional Layer with 16 filters having kernel size =  $3\times 5$ as the first layer, instead of a normal 2D Convolutional layer with the same configuration increased the pion efficiency of one model from ${\varepsilon }_{\pi }=4.5\%$ to ${\varepsilon }_{\pi }=3.8\%$, but no further gains were found when adding capacity by increasing the number of hidden layers, or when changing the kernel size of the 2D Transpose Convolutional Layer.

\noindent 

\noindent 
\subparagraph{1D Convolutional Neural Networks}

\noindent 

\noindent While 2D Convolutional Neural Networks were more successful than 1D Convolutional Neural Networks, 1D CNNs resulted in similar performance, even though they were fed data with lower dimensions, i.e. a compressed version of the data 2D CNNs were trained on, in the form of column sums of the original 2D image data. Using 4 1D Convolutional Layers (the first 3 of which were followed by Max Pooling operations), and 8 dense layers resulted in a pion efficiency of ${\varepsilon }_{\pi }=6.5\%$. It should be noted that many more 2D CNNs were built during this project, but the fact that 1D CNNs gave comparable accuracy provides additional support to the hypothesis that there is a limit to the amount of information contained in the 2D images from the TRD about the Particle ID.

\noindent 

\noindent 
\subparagraph{LSTM Networks}

\noindent 

\noindent The nature of this dataset is such that it can be framed as an image for Convolutional Neural Networks, but it is essentially a timeseries as well, with columns going across indicating the ADC signal at sequential time intervals and rows indicating where the charge was deposited (in which pad in a specific TRD chamber, row and column).

\noindent 

\noindent The lowest pion efficiency using LSTM networks was ${\varepsilon }_{\pi }=8.5\%$, which is much higher than that achieved by the best 2D Convolutional Neural Networks. This was achieved by having 4 LSTM layers return sequences sequentially, followed by a final LSTM network which fed into a dense architecture of 5 fully connected layers of 128 nodes each; although similar performance (${\varepsilon }_{\pi }=8.9\%$) was obtained using only two LSTM layers, with the second layer going backwards, followed by four dense layers of 256 nodes each. Using 6 LSTM layers, alternating between going backwards and forwards, with four dense layers of 256 nodes each made pion efficiency drop back down to ${\varepsilon }_{\pi }=9.2\%$.

\noindent 

\noindent 
\subparagraph{The Effect of Regularization}

\noindent 

\noindent Figure 64 shows the effect of applying too much Dropout at training time. The left side of the figure shows a model which overfit during training and the right hand side shows the effect of applying a Dropout rate of 0.5 to each layer of the same network.

\noindent 

\noindent 

\noindent \includegraphics*[width=2.61in, height=2.05in, keepaspectratio=false]{image123}\includegraphics*[width=2.53in, height=1.96in, keepaspectratio=false]{image124}

\noindent \textbf{Figure 71: No Dropout vs Same Model with too much Dropout}

\noindent 

\noindent In general, using a Dropout rate of 0.2 on the fully connected layers of the network proved to be a decent strategy, bearing in mind that other hyperparameters also play a role, e.g. a model employing the abovementioned Dropout rate which resulted in a pion efficiency of ${\varepsilon }_{\pi }=6.5\%$ dropped down to a pion efficiency of ${\varepsilon }_{\pi }=24.1\%$ when the learning rate (using the Adam optimizer) was reduced from $\epsilon ={10}^{-4}$ to $\epsilon ={10}^{-6}$.

\noindent 

\noindent The use of a Gaussian Noise layer as the first layer of various convolutional architectures was employed, but was unsuccessful, as depicted in Figure 65. All models that incorporated Gaussian noise gave pion efficiencies of ${\varepsilon }_{\pi }>45\%$. While there was not much experimentation with different standard deviations of the Gaussian noise layer, this method seems more applicable to  image data which is less sparse, since it is only active during training time, and since most of the rows in our input data naturally has a value of 0, adding Gaussian noise will only serve to confuse the model when evaluated at test time.

\noindent 

\noindent \includegraphics*[width=2.59in, height=2.00in, keepaspectratio=false]{image125}

\noindent \textbf{Figure 72: Gaussian Noise with $\boldsymbol{\sigmaup}$=0.2}

\noindent 


\paragraph{ Distinguishing Simulated from Real Data}

\noindent Distinguishing Geant4 simulated data from real data proved to be trivial compared to distinguishing real pions from real electrons. What follows is an analysis of features between the two datasets to investigate what differentiates them.

\noindent 

\noindent Firstly, the mean and standard deviation of ADC values for real and simulated datasets are not the same, i.e. ${\mu }_{sim}=2.178$ $;\ {\mu }_{real}=4.527$ and ${\sigma }_{sim}=11.989$ $;\ {\sigma }_{real}=17.995$. The distribution of simulated data's ADC values is slightly more skewed to the right than that of real data, i.e.  ${\gamma }_{1_{sim}}=19.170$ $;\ {\gamma }_{1_{real}}=17.391$. However, the maximum ADC value for both datasets is the same, i.e.  ${max}_{sim}={max}_{real}=1023$. 

\noindent 

\noindent Figure 66 and Figure 67 show the distribution of ADC values for simulated and real data, respectively.

\noindent 

\noindent \includegraphics*[width=3.67in, height=2.13in, keepaspectratio=false]{image126}

\noindent \textbf{Figure 73: Distribution of ADC values for simulated data}

\noindent \includegraphics*[width=3.89in, height=2.29in, keepaspectratio=false]{image127}

\noindent \textbf{Figure 74: Distribution of ADC values for real data}

\noindent Looking at the number of pads that don't contain any data (i.e. the number of rowsums of the image that are equal to 0), one sees the following distributions for simulated and real data:

\noindent \includegraphics*[width=3.97in, height=2.08in, keepaspectratio=false]{image128}

\noindent \textbf{Figure 75: Distribution of the number of pads with no data per image for simulated data}

\noindent 

\noindent \includegraphics*[width=3.68in, height=1.87in, keepaspectratio=false]{image129}

\noindent \textbf{Figure 76: Distribution of the number of pads with no data per image for real data}

\noindent While these distributions are quite similar, one can see that real data is slightly more skewed towards the left and has far fewer instances of images which are completely devoid of signal, i.e. 17 pads with no signal.

\noindent 

\noindent Finally, there are also differences in the distribution of mean ADC values per pad for real and simulated data (see Figure 70 and Figure 71).

\noindent 

\noindent 

\noindent \includegraphics*[width=3.86in, height=2.03in, keepaspectratio=false]{image130}

\noindent \textbf{Figure 77: Distribution of mean ADC value per image for simulated data}

\noindent 

\noindent \includegraphics*[width=3.58in, height=1.95in, keepaspectratio=false]{image131}

\noindent \textbf{Figure 78: Distribution of mean ADC value per image for real data}


\paragraph{ Deep Generative Models towards High Energy Physics Event Simulations}

\noindent 

\noindent While various deep generative architectures can give results that appear vastly different in terms of ``style'', they all fail to capture the underlying distribution of the training data sufficiently.

\noindent 

\noindent A possible reason for this is that the images used for training are quite small and have relatively little information compared to, for instance, images of human faces, where GANs have proven to be quite successful.

\noindent 

\noindent While Variational Autoencoders give images that appear less spread-out along the pad dimension than any of the GAN architectures that were used, with less overall noise, they still fail to capture some of the volatility along the time dimension and appear somewhat ``smeared out'' compared to real data.

\noindent 

\noindent \includegraphics*[width=2.72in, height=2.00in, keepaspectratio=false]{image132}\includegraphics*[width=2.72in, height=2.00in, keepaspectratio=false]{image133}

\noindent \textbf{Figure 79: Real images}

\noindent \includegraphics*[width=2.68in, height=1.96in, keepaspectratio=false]{image134}\includegraphics*[width=2.72in, height=2.00in, keepaspectratio=false]{image135}

\noindent \textbf{Figure 80: Autoencoder outputs}

\noindent 

\noindent While Adversarial Autoencoders gave results that were slightly better than the other GAN architectures experimented with, all of the GAN architectures provided disappointing results, possibly due to the fact that GANs are quite hard to train, since there are two neural networks pitted against each other in such a setup and if either of them become dominant during the training process, they can force the other into a parameter space which is not conducive to training either network properly.

\noindent 

\noindent Examples of this can be seen in an Adversarial Autoencoder setup with very deep architecture in both the Generator and Discriminator (Figure 74), where after 21400 epochs, the Generator still outputs images that are seemingly random, but nonetheless have similar features across images (for some reason this resulted in low error and therefore became prevalent).

\noindent 

\noindent \includegraphics*[width=5.88in, height=3.92in, keepaspectratio=false]{image136}

\noindent \textbf{Figure 81: An illustration of how the Generative network of a GAN can begin producing images with similar features that, even though they are not correct, result in low loss, because of a Discriminator which is not conducive to Generator training.}

\noindent 

\noindent Batch Normalization is a strategy that ensures that a layer's outputs are normally distributed and therefore prevents the Generative component of the GAN to output images that look very similar, in Figure 75, one can see that the output images of the Bidirectional GAN (which employs this strategy) are highly dissimilar, but there is still a lot of noise around the main signal.

\noindent 

\noindent \includegraphics*[width=5.88in, height=3.92in, keepaspectratio=false]{image137}

\noindent \textbf{Figure 82: Illustrating the effect of Batch normalization to prevent output images from looking highly similar}

\noindent 

\noindent It is interesting to note that while one might expect GANs that employ convolutional layers to give better results than fully connected dense layers, the Deep Convolutional GANs gave some of the most unrealistic simulated images (Figure 76). This could be due to the fact that convolutional layers introduce a prior that features are translationally invariant, i.e. they can appear anywhere in the image. While this might be a useful assumption during particle identification, it does not seem to hold for when simulating the data used in this project.

\noindent 

\noindent \includegraphics*[width=5.34in, height=3.56in, keepaspectratio=false]{image138}

\noindent \textbf{Figure 83: Illustrating how convolutional architectures in a GAN setup results in features that might exist in the training distribution, but do not appear in the right place}

\noindent 

\noindent Training after a certain number of epochs does not necessarily result in additional gains in performance, as can be seen in Figure 77, Figure 78 and Figure 79, there is not much change in the output of a Least Squares GAN when training for an additional 293~000 epochs after 94 600 epochs and training for an additional 61~400 epochs after that eventually results in images that look less realistic than those produced during earlier epochs.

\noindent 

\noindent \includegraphics*[width=4.56in, height=3.04in, keepaspectratio=false]{image139}

\noindent \textbf{Figure 84: Least squares GAN after 94600 epochs}

\noindent 

\noindent \includegraphics*[width=4.08in, height=2.72in, keepaspectratio=false]{image140}

\noindent \textbf{Figure 85: Least squares GAN after 387600 epochs}

\noindent 

\noindent \includegraphics*[width=3.90in, height=2.60in, keepaspectratio=false]{image141}

\noindent \textbf{Figure 86: Least squares GAN after 449000 epochs}


\subsection{ Conclusions}

\noindent 


\paragraph{ Particle Identification}

\noindent 

\noindent While neural networks are very good at coming up with their own feature sets to find nested functions that can solve classification problems, what's more important in a deep learning project is making sure that the input data contains sufficient information about the class label.

\noindent 

\noindent While the work done in this project did not achieve comparable results to work done before, pad-per-pad calibration was performed on data in the work done before, which proved to be invaluable in normalizing the data for environmental and electronic variations which occur during data taking and affect how the signal manifests, a process which cannot be solvable by deep learning techniques.


\paragraph{ Simulations}

\noindent 

\noindent Probably the most important result of this dissertation is the fact that Geant4 simulations are easily distinguishable from real data. Whilst deep generative techniques do not appear to be able to give similar performance compared to Geant4 for this particular problem, there could be interesting ways to combine the two methods, e.g. by transforming the output given by Geant4 with deep generative techniques to make it appear more like real data.


\subsection{ Outlook and Future Work}

\noindent 


\paragraph{ Particle Identification}

\noindent 

\noindent Future work should focus on applying additional data pre-processing steps before particle identification is applied. The data used for particle identification in this thesis was raw data from the TRD, whereas previous work used data which was calibrated pad-by-pad per run; however there does not seem to be much promise for arriving at increased accuracy using advanced deep learning methods compared to work that was done before

\noindent 


\paragraph{ Simulations}

\noindent 

\noindent As mentioned in the Conclusions section above, there should be scope to use the output of Geant4 simulations as a latent space to produce more realistic simulations, alternatively, a more realistic approach would entail tuning parameters used during simulation in the following script https://github.com/alisw/AliRoot/blob/master/TRD/TRDbase/AliTRDSimParam.cxx, a lot of hard-coded parameters could be adjusted recursively by using the loss function of a neural network which is used to distinguish between the two data sets as a parameter to be maximized, i.e. to produce simulated data that becomes harder to distinguish from real data as the multidimensional input parameters maximize the loss function of the discriminating neural network more, a setup similar to what occurs in GANs.

\noindent 


\paragraph{ Outlook}

\noindent 

\noindent Tensorflow and the Keras library are immensely useful for rapid prototyping of various deep learning architectures, whether they be for classification or regression problems. Whilst there exists a toolkit for machine learning within the AliRoot infrastructure, there should be benefits to exploring how using more modern deep learning libraries within the environment of AliRoot could extend its capabilities as Machine Learning becomes a more mature and effective field.

\noindent 


\section{ Bibliography}

\noindent 1. Modern Particle Physics. Thomson, Mark. Cambridge, UK~: Cambridge University Press, 2013. ISBN 978-1-107-03426-6.2. Wikimedia Commons. [Online] [Cited: 2 March 2019.] https://commons.wikimedia.org/wiki/File:Standard\_Model\_Feynman\_Diagram\_Vertices.png.3. Particle Data Group. The Review of Particle Physics. 2018.4. ALICE Collaboration. The ALICE Transition Radiation Detector: construction, operation, and performance. s.l.~: CERN, 2017. arXiv:1709.02743v2.5. Connecting QGP-Heavy Ion Physics to the Early Universe. Rafelski, Johann. 2013. Nuclear Physics B Proceedings Supplement.6. The Quark-Gluon Plasma\ast  A Short Introduction. Satz, Helmut. 2011. 6th International Conference on Physics and Astrophysics of Quark Gluon Plasma. https://arxiv.org/abs/1101.3937v1.7. Viljoen, Christiaan Gerhardus. Draw.io. [Online] https://www.draw.io/?lightbox=1\&highlight=0000ff\&edit=\_blank\&layers=1\&nav=1\#G1X-ZGzxO\_b4zo\_74rY9Z9zaikz-Il6R8o.8. QCD Phase Diagram SVG. Wikimedia Commons. [Online] [Cited: 18 2 2019.] https://commons.wikimedia.org/wiki/File:QCDphasediagram.svg.9. Week 3: Thermal History of the Universe. Caltech University. [Online] [Cited: 20 February 2019.] www.astro.caltech.edu/$\mathrm{\sim}$george/ay127/kamionkowski-earlyuniverse-notes.pdf.10. The Steven Hawking Center for Theoretical Cosmology. [Online] http://www.ctc.cam.ac.uk/images/contentpics/outreach/cp\_universe\_chronology\_large.jpg.11. CERN. About CERN: Who we are: Our History. CERN. [Online] CERN. [Cited: 26 January 2019.] https://home.cern/about/who-we-are/our-history.12. ---. CERN: Who We Are: Our Governance: Member States. CERN. [Online] [Cited: 26 January 2019.] https://home.cern/about/who-we-are/our-governance/member-states.13. ---. CERN: About: Who We Are: Our Mission. CERN. [Online] [Cited: 26 January 2019.] https://home.cern/about/who-we-are/our-mission.14. ---. Grafana IT Overview. CERN. [Online] [Cited: 26 January 2019.] http://monit-grafana-open.cern.ch/d/000000884/it-overview?orgId=16.15. Chardley, Sarah. LHC Does a Dry-Run. Symmetry Magazine. [Online] 20 March 2015. [Cited: 26 January 2019.] https://www.symmetrymagazine.org/article/march-2015/the-lhc-does-a-dry-run.16. CERN. CERN Resources: FAQs: Facts and Figures About the LHC. CERN. [Online] [Cited: 06 01 2019.] https://home.cern/resources/faqs/facts-and-figures-about-lhc.17. University of California Davis. RHIC. UC Davis Nuclear. [Online] [Cited: 27 01 2019.] http://nuclear.ucdavis.edu/$\mathrm{\sim}$rpicha/rhic.html.18. Encyclopedia Britannica. Tevatron Particle Accelerator. Encyclopedia Britannica. [Online] [Cited: 27 January 2019.] https://www.britannica.com/technology/Tevatron.19. CIRCUMFERENCE VARIATIONS OBSERVED AT KEKB. Masuzawa, M, et al. 2002. Proceedings of the 7th International Workshop on Accelerator Alignment. Vols. Spring-8.20. Kurokawa, Shin-Ichi and Olsen, Stephen L. The KEK B-Factory Experiment. Stanford University. [Online] [Cited: 27 January 2019.] www.slac.stanford.edu/pubs/beamline/29/2/29-2-kurokawa.pdf.21. Taking a Closer Look at the LHC. The LHC Proton Source. LHC Closer. [Online] [Cited: 27 January 2019.] https://www.lhc-closer.es/taking\_a\_closer\_look\_at\_lhc/0.proton\_source.22. CERN. LHC: The Guide. CERN. [Online] [Cited: 2017 January 2019.] https://home.cern/resources/brochure/cern/lhc-guide.23. Field, Rick. PHY2061 - Enriched Physics 2 - Relativity 4. University of Florida Physics. [Online] [Cited: 27 January 2019.] http://www.phys.ufl.edu/$\mathrm{\sim}$acosta/phy2061/lectures/Relativity4.pdf.24. CERN. The CERN Accelerator Complex. CERN Document Server. [Online] [Cited: 26 January 2019.] https://cds.cern.ch/record/2636343/files/CCC-v2018-print-v2.jpg?subformat=icon-1440.25. ---. LHC Experiments. CERN. [Online] [Cited: 21 February 2019.] https://home.cern/science/experiments.26. ---. ATLAS Experiment. CERN. [Online] [Cited: 21 February 2019.] https://home.cern/science/experiments/atlas.27. ---. ALICE Experiment. CERN. [Online] [Cited: 21 Fenruary 2019.] https://home.cern/science/experiments/alice.28. ---. LHCb Experiment. CERN. [Online] [Cited: 21 February 2019.] https://home.cern/science/experiments/lhcb.29. ---. LHCf Experiment. CERN. [Online] [Cited: 21 Febraury 2019.] https://home.cern/science/experiments/lhcf.30. ---. MoEDAL Experiment. CERN. [Online] [Cited: 21 Febrary 2019.] https://home.cern/science/experiments/moedal.31. ---. ROOT Data Analysis Framework: User's Guide. CERN. [Online] May 2018. https://root.cern.ch/root/htmldoc/guides/users-guide/ROOTUsersGuideA4.pdf .32. [Online] https://root.cern.ch/.33. ROOT 5 Reference Guide. [Online] https://root.cern/root/html534/ClassIndex.html.34. ROOT 6 Reference Guide. [Online] https://root.cern/doc/v616/.35. ALICE Collaboration (CERN). ALICE Analysis Tutorial. [Online] [Cited: 18 2 2019.] https://alice-doc.github.io/alice-analysis-tutorial.36. CERN. High Energy Physics Simulations. LHC @ home. [Online] [Cited: 26 July 2019.] http://lhcathome.web.cern.ch/projects/test4theory/high-energy-physics-simulations.37. Geant4 - a simulation toolkit. Agostinelli, Stefano, et al. 2003, Nuclear Instruments and Methods in Physics Research, Vol. A 506, pp. 250-303.38. ALICE. ALICE Homepage. ALICE. [Online] [Cited: 21 February 2019.] http://alice.web.cern.ch/.39. CERN. CERN Document Server. [Online] [Cited: 21 February 2019.] https://cds.cern.ch/record/2302924.40. The ALICE Collaboration. The ALICE Experiment at the CERN LHC. s.l.~: INSTITUTE OF PHYSICS PUBLISHING AND SISSA, 2008. 2008 JINST 3 S08002.41. ---. The Technical Design Report of the Transition Radiation Detector. Geneva~: CERN, 2001. CERN/LHCC 2001-021 ALICE TDR 9.42. CERN. Physics Vectors. ROOT User's Guide. [Online] [Cited: 23 February 2019.] https://root.cern.ch/root/htmldoc/guides/users-guide/PhysicsVectors.html.43. Spherical Coordinates. Maths Insight. [Online] [Cited: 23 February 2019.] https://mathinsight.org/spherical\_coordinates.44. Particle Identification with the ALICE Transition Radiation Detector. Pachmayer, Yvonne. 2014. arXiv:1402.3508v1 [physics.ins-det].45. Goodfellow, Ian, Bengio, Yoshua and Courville, Aaron. Deep Learning. Cambridge, Masachusetts~: The MIT Press, 2016. ISBN 9780262035613.46. The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. Rosenblatt, F. 6, 1958, Psychological Review, Vol. 65.47. keras.io. Available Activations. Keras. [Online] [Cited: 19 July 2019.] https://keras.io/activations/\#available-activations.48. Viljoen, CG. Draw.io. [Online] https://www.draw.io/?lightbox=1\&highlight=0000ff\&edit=\_blank\&layers=1\&nav=1\#G1Zbad00aGA6OXlYpDzl8ggBK2kRM08xzO.49. Long Short-Term Memory. Hochreiter, Sepp and Schmidhuber, Jurgen. 8, 1997, Neural Computation, Vol. 9, pp. 1735-1780.50. Doersch, Carl. Tutorial on Variational Autoencoders. Carnegie Mellon University. s.l.~: ResearchGate, 2016.51. Generative Adversarial Nets. Goodfellow, Ian, et al. s.l.~: Curran Associates, Inc., 2014.52. Conditional Image Synthesis with Auxiliary Classifier GANs. Odena, Augustus, Olah, Christopher and Shlens, Jonathon. 2017. Proceedings of the 34th International Conference on Machine Learning.53. Adversarial Autoencoders. Makhzani, Alireza, et al. 2016. arXiv:1511.05644v2.54. Adversarial Feature Learning. Donahue, Jeff, Darrell, Trevor and Krahenbuhl, Phillip. 2017. arXiv:1605.09782v7.55. Unsupervised Representation Learning with Deep Convolutional Genarative Adversarial Networks. Metz, Luke, Radford, Alec and Chintala, Soumith. 2016. ICLR 2016. arXiv:1511.06434v2.56. Least Squares Generative Adversarial Networks. Mao, Xudong, et al. 2017. arXiv:1611.04076v3.57. Cowan, Glen. Statistical Data Analysis. Oxford~: Oxford University Press, 1998. ISBN0198501560.58. Weinberg, Steven. The First Three Minutes. Cambridge, Masachusettes~: Fontana Paperbacks, 1976.59. Robinson, Donald. [Online] [Cited: 23 February 2019.] http://donrmath.net/CalcIII/unit1/lesson7/u1l7.html.60. NN SVG. alexlenail. [Online] [Cited: 21 April 2019.] http://alexlenail.me/NN-SVG/AlexNet.html.


\end{document}

